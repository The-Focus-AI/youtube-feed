---
video_id: mq_V6zAjsSI
title: "Why the US need Open Models | Nathan Lambert on what matters in the AI and science world"
channel: Turing Post
duration: 2842
duration_formatted: "47:22"
view_count: 757
upload_date: 2026-02-06
url: https://www.youtube.com/watch?v=mq_V6zAjsSI
thumbnail: https://i.ytimg.com/vi_webp/mq_V6zAjsSI/maxresdefault.webp
tags:
  - OpenSourceAI
  - AIResearch
  - NathanLambert
  - AllenInstitute
  - OLMo
  - DeepSeek
  - Qwen
  - RLHF
  - CodingAgents
  - AIPolicy
  - SovereignAI
  - HybridModels
  - PostTraining
---

# Why the US need Open Models | Nathan Lambert on what matters in the AI and science world

## Summary

Nathan Lambert, research scientist at the Allen Institute for AI (AI2), author of the RLHF Book, and writer of the Interconnects newsletter, joins Turing Post for a wide-ranging conversation about the state of open models, their geopolitical importance, and the future of AI research. Lambert argues that open models are essential not because they will surpass closed models -- he believes they will perpetually lag 6-9 months behind -- but because they serve as the engine for the next decade of AI research, enabling academia to remain relevant and ensuring the US maintains its position in AI innovation. He offers a remarkably candid assessment of China's open model ecosystem, noting that DeepSeek created an industry standard that made Chinese companies default to open releases with permissive licenses, while US companies often attach restrictive terms that enterprise lawyers dislike.

The conversation dives deep into the technical landscape, covering why post-training with reinforcement learning has become increasingly complex, how hybrid architectures mixing linear and traditional attention are emerging across multiple labs simultaneously, and why coding agents represent the next frontier where closed models may pull even further ahead. Lambert shares that even at AI2, where they build open models, most researchers still primarily use closed models -- highlighting the honest gap between open and closed model quality. He discusses the economics of open source AI, noting that "free" open models are far from cheap when you factor in compute for serving, training teams, and data acquisition costs that can run into millions of dollars.

Looking ahead to 2026-2027, Lambert identifies agentic systems as the dominant trend, sovereign AI as a continuing geopolitical theme, and data center construction as a growing societal friction point. He offers a nuanced view on AGI, suggesting that what we already have could be considered AGI by some definitions, and that Claude Code in its latest form is "pretty close" to the colloquial SF definition of AGI as a drop-in remote worker replacement. The interview closes with Lambert recommending "The Season of the Witch," a history of San Francisco that he believes tech workers should read to better understand the community they inhabit.

## Highlights

### "They're going to be the engine for the next 10 years of AI research"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=222s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*3:42-5:02" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-3m42s.mp4"
```
</details>

> "They're going to be the engine for the next 10 years of AI research because academia is just in a lull of influence in terms of the evolution of the science... open models are the platform by which that innovation is happening. And for a country like the US that has had such an excellent history of scientific projects and institutions, if they want to be the institution of AI research they should consider it useful and imperative to have that open model investment be intentional and understood."
> -- Nathan Lambert, [3:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=222s)

### "Our only other option is open weights"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=342s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*5:42-7:00" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-5m42s.mp4"
```
</details>

> "They know that tech companies and potential customers in the US won't sign up for an API where data is sent to China and they pay. They've told me this. They're like, well, our only other option is open weights because then they could still most likely use it."
> -- Nathan Lambert, [5:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=342s)

### "Cloud Code with Opus 4.5 is when I finally started using coding agents"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=962s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*16:02-17:02" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-16m02s.mp4"
```
</details>

> "I've talked to people at these companies and they're like, 'Wow, yeah, Cloud Code with Opus 4.5 is when I finally started using coding agents a couple months ago.' And if they're like still just getting obsessed with them, then they will decide, oh, we need to train the model that's really really good at this."
> -- Nathan Lambert, [16:02](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=962s)

### "Musk Industries, for better or worse, has a bit of villain vibes"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1216s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*20:16-21:20" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-20m16s.mp4"
```
</details>

> "Musk Industries, for better or worse, has a bit of villain vibes, but in a sci-fi movie... for whatever opinion on Elon you have, especially on the political side that I think he should spend less time in, he has such a track record in building businesses that I have to think that there is a plan there."
> -- Nathan Lambert, [20:16](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1216s)

### "The model's outputs are so literally the product"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1925s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*32:05-33:20" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-32m05s.mp4"
```
</details>

> "The model's language is the interface by which the user sees the information. And it's like therefore the model's outputs are so literally the product. If the product is what keeps the person engaged and uses certain triggers to get people to come back to it... that is so deeply entwined in how you would approach what is the user experience."
> -- Nathan Lambert, [32:05](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1925s)

### "One is what we have is an AGI"

[![Clip](https://img.youtube.com/vi/mq_V6zAjsSI/hqdefault.jpg)](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2617s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*43:37-44:50" "https://www.youtube.com/watch?v=mq_V6zAjsSI" --force-keyframes-at-cuts --merge-output-format mp4 -o "mq_V6zAjsSI-43m37s.mp4"
```
</details>

> "I have two views on AGI. One is what we have is an AGI. And two, I understand the colloquial SF lingo for AGI, which is like a drop-in replacement for a remote worker, which I see... I would push them again on like Claude Code's latest form being pretty close to their definition of AGI if you really are flexible and work with it."
> -- Nathan Lambert, [43:37](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2617s)

## Key Points

- **Open Models as Research Engine** ([3:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=222s)) - Open models will be the engine for the next 10 years of AI research, providing a platform for academic innovation that companies cannot nurture
- **AI Policy Impact** ([2:10](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=130s)) - Lambert has become more focused on AI policy, seeing open model building as most impactful on the policy side due to its geopolitical nature
- **DeepSeek's Ideological Origin** ([5:17](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=317s)) - DeepSeek was ideologically motivated in the purest scientific way, creating an industry standard that made Chinese companies default to open models
- **China's Open Weights Strategy** ([5:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=342s)) - Chinese labs release open weights because US companies won't use APIs that send data to China; open weights are their only viable path to US adoption
- **Nvidia's Nematron Efforts** ([7:56](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=476s)) - Nvidia's open model efforts are the closest US equivalent to Chinese open model labs, with a clear business model of selling more GPUs
- **Post-Training Complexity** ([11:17](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=677s)) - Pre-training data is the hardest legal part to open; post-training is complex with lots of sequencing and hard decision-making; RL infrastructure is still fragmented
- **6-9 Month Open Model Lag** ([13:49](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=829s)) - The best open models will remain 6-9 months behind closed models; this dynamic is unlikely to change because resources and talent determine outcomes
- **AI2 Spent Millions on Synthetic Data** ([14:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=882s)) - OLMo 3 used millions of dollars of synthetic data, much from a federal government grant for a frontier supercomputer
- **Coding Agents Still Early** ([15:53](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=953s)) - Closed model researchers are only just becoming obsessed with coding agents, meaning significant improvement is still ahead
- **Low-Hanging Fruit Everywhere** ([17:02](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1022s)) - Improvements come from data scaling, better filtering, GPU kernel optimization, and incremental compounding 10% gains across the training pipeline
- **Hybrid Architecture Convergence** ([18:21](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1101s)) - Qwen 3, Kimi, AI2, and Nvidia are all building hybrid linear/traditional attention models simultaneously; a collective readiness took 2.5 years after Mamba hype
- **Robotics Timeline Too Soon** ([20:56](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1256s)) - Lambert believes Tesla's robotics timeline seems too early, but acknowledges Elon Musk's track record means there may be information he doesn't have
- **Open Source Definition Settled** ([24:39](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1479s)) - The community definition of open source AI has settled on "weights released openly"; Chinese labs have done better on permissive licensing than US companies
- **Free Models Are Not Cheap** ([28:04](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1684s)) - Running open models requires committed compute, minimum hosting costs, and rarely enough inference load to justify the spend
- **AI2 Must Dogfood Its Models** ([28:36](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1716s)) - AI2 needs to build development loops between using and improving its own models to make them more than a "toy project"
- **Lambert Uses Closed Models** ([29:04](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1744s)) - Despite being an open model advocate, Lambert mostly uses closed models because they are not yet good or cheap enough to switch habits
- **Anthropic's Product-Led Strategy** ([34:14](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2054s)) - Anthropic's muted benchmark releases fit their bet on code and agentic experiences, while Gemini 3's benchmark hype hasn't translated to cutting-edge impact
- **Sovereign AI Continues** ([36:09](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2169s)) - International investment in compute and models will continue; France benefited by finding a company willing to accept the sovereign AI narrative early
- **Data Center Construction Friction** ([36:50](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2210s)) - The buildout of data centers is a major societal friction point, with local communities not receiving benefits proportionate to the disruption
- **AGI Is Already Here** ([43:37](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2617s)) - Lambert holds two views: what we have is already AGI, and Claude Code's latest form is close to the colloquial definition of AGI as a remote worker replacement

## Mentions

### Companies
- **Allen Institute for AI (AI2)** ([0:29](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=29s)) - Nathan Lambert's employer; building OLMo open models and working on agentic tool use
- **Anthropic** ([7:40](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=460s)) - Mentioned alongside OpenAI as having talent density comparable to Chinese labs; praised for product-led strategy
- **OpenAI** ([7:40](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=460s)) - Referenced for frontier model capabilities; rumored IPO by end of year; Codex coding agent efforts
- **DeepSeek** ([5:17](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=317s)) - Described as ideologically motivated; created industry standard for open models in China
- **Nvidia** ([7:56](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=476s)) - Nematron open model efforts; clear business model of selling more GPUs; also contributing Megatron LM for pre-training infrastructure
- **Meta/Llama** ([7:25](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=445s)) - Zuckerberg "backed off" with Llama; mentioned as lacking a clear business model for open models
- **Google/Gemini** ([34:41](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2081s)) - Gemini 3 hyped but has "almost zero really cutting edge impact"; lagging on agentic experience
- **Tesla** ([20:49](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1249s)) - Stopping Model S and Model X production seen as shocking; robotics timeline questioned
- **SpaceX** ([20:11](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1211s)) - Referenced in context of Elon Musk's business empire
- **Minimax** ([7:01](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=421s)) - Chinese AI lab shipping new versions monthly with research teams working in shifts
- **Mistral** ([36:31](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2191s)) - Implied as the French company that accepted the sovereign AI narrative early
- **Prime Intellect** ([9:45](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=585s)) - Their verifiers mentioned as an example of early open ecosystem training infrastructure
- **Hugging Face** ([26:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1567s)) - Listed among groups doing fully open AI work

### Products & Technologies
- **OLMo 3** ([12:37](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=757s)) - AI2's open model released in fall; currently transitioning to be more agentic and tool-use capable
- **Claude Code** ([16:02](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=962s)) - Anthropic's coding agent; seen as a pivotal moment for the coding agent paradigm
- **Opus 4.5** ([16:12](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=972s)) - When researchers at frontier labs "finally started using coding agents"
- **Codex** ([17:58](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1078s)) - OpenAI's coding agent; explained as their effort to catch up with Anthropic's agentic experience
- **Qwen 3** ([18:45](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1125s)) - Has a linear model variant; part of hybrid architecture convergence
- **Kimi** ([18:47](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1127s)) - Also building hybrid linear attention models
- **Mamba** ([19:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1147s)) - State space model that generated high hype 2.5 years ago; needed a few architectural changes before becoming practical in hybrid form
- **Megatron LM** ([11:53](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=713s)) - Nvidia's open-source pre-training infrastructure; described as "actually very strong"
- **Nematron** ([7:58](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=478s)) - Nvidia's open model effort; led by three VPs
- **GPT-5.2** ([6:36](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=396s)) - Acknowledged as clearly better than best open models alongside Opus
- **Parakeet** ([30:03](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1803s)) - Nvidia's speech-to-text model used in a creative open model workflow
- **RLVR** ([11:41](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=701s)) - Mentioned as part of the scaling reinforcement learning revolution that increased post-training infrastructure complexity

### People
- **Nathan Lambert** ([0:27](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=27s)) - Research scientist at AI2, Interconnects newsletter author, RLHF Book author
- **Elon Musk** ([20:16](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1216s)) - Discussed in context of Tesla robotics bets and "Musk Industries" having "villain vibes"
- **Sam Altman** ([19:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1194s)) - Referenced regarding OpenAI's fundraising engine
- **Mark Zuckerberg** ([7:25](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=445s)) - "Backed off" with Llama open model efforts
- **Percy Liang** ([7:51](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=471s)) - Stanford efforts linked to open model research ecosystem
- **Lex Fridman** ([0:59](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=59s)) - Lambert appeared on his podcast; referenced multiple times in the conversation
- **Olive Song** ([7:03](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=423s)) - Minimax researcher interviewed by the host about Chinese lab work culture
- **Florian** ([29:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1794s)) - Lambert's colleague at Interconnects who uses a creative Parakeet + Qwen + Claude workflow

## Surprising Quotes

> "I mostly use closed models. They're not good enough or cheap enough where it's obvious that I want to switch my habits."
> -- Nathan Lambert (open model advocate at AI2), [29:09](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1749s)

> "If even AI2 is spending millions of dollars on effective compute for synthetic data and training, you're going to guess that the compute expenditure there is almost like billions at these frontier labs."
> -- Nathan Lambert, [14:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=894s)

> "You have to dogfood the models and build development loops between using the model and improving the model and getting real feedback, because until then it's somewhat of a toy project."
> -- Nathan Lambert on AI2's own models, [28:40](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1720s)

> "Gemini 3 which came out late in the year and was so hyped as like 'Google is back' but has almost zero like really cutting edge impact right now."
> -- Nathan Lambert, [35:05](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2105s)

> "I would push them again on like Claude Code's latest form being pretty close to their definition of AGI if you really are flexible and work with it."
> -- Nathan Lambert, [43:50](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2630s)

## Transcript

[0:00](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=0s) Is what we have is an AGI. They're going to be the engine for the next 10 years of AI research. I think pre-training data is the hardest legal part to get open. The timeline on robotics seems too soon. Musk Industries, for better or worse...

[0:22](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=22s) Hello everyone. I'm very happy to be hosting Nathan Lambert, research scientist at Allen Institute for AI, one of the best specialists, open model advocate and very well articulated educator and I think I was one of your first paying subscribers back in 2022. So, you can tell I'm a big fan. Yeah, it's been fun. We've been crossing paths for years now on the web and in real life. So, it's fun to join the pod.

[0:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=54s) When you were starting your career, could you imagine -- because I was just watching your podcast with Lex Fridman -- could you imagine that you would become a celebrity? No. And I talk about this with my partner and family regularly where it's just like largely due to the dynamics of the AI industry, things have evolved so fast where there are just so many people who are very articulate and good educators that went to these labs to understandably go all in on building AI and part of that with the stakes involved like people at these labs don't tweet publicly that much. Open AI is a whole special thing but a lot of these communicators kind of can't talk so that's this void that I have been launched into.

[1:41](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=101s) And I think there is no way to actually prepare your life or approach where you go through levels of influence so quickly and now it's just like I have this ability and how does it change the relationship to what I work on and the problems that I approach to still have impact? It's like I get invited to all sorts of fancy things and it's just like most of them serve no purpose and it's easy. You just have to say no to them. But I have not fully grappled with what it means to have that ability.

[2:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=127s) And I think in the last year I've definitely become more conscious of trying to help AI policy go well. I think a lot of what we do building open models that are so well documented with care and explaining all the sides probably is most impactful on the policy side of things because open models is so geopolitical and how AI will diffuse through the world which is a very different track than what is so obviously in vogue right now which is that the coding agents are becoming so good and so impactful in this acceleration that comes with it.

[2:43](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=163s) There's attempts to make it seem like these are linked topics where yes, the Chinese model players are releasing agents as well, but I really think that they're not as linked as people think. And a lot of the open models are overhyped in their abilities. And it's kind of a recurring thing where yes, it would be cool if the open models are better, but I just think that it's very different to sit down and use Claude Code for Opus 4.5 or Codex than it is to play with an open model. That's not to say that they don't matter.

[3:13](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=193s) I think it's mostly -- I don't like working in geopolitics, but I would say maybe open models are such a case study in emerging technology and understanding how emerging technology influences the world and creates new pockets of influence is interesting. But that's so new to me. I'm not trained in that at all. So I'm trying to learn.

[3:31](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=211s) That's exactly my question. If open models are not that good, why are you so passionate about open models? Why there are so many conversations about open models for the US? What's the reason? They're going to be the engine for the next 10 years of AI research because the academia has been necessarily -- I was going to say beat down which is not the right way to say it but academia is just in a lull of influence in terms of the evolution of the science to the point where people will say academic AI research doesn't matter right now which I think is a bit shortsighted because it's going to be an engine for exploration in a way that companies can't really nurture.

[4:10](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=250s) But open models are the platform by which that innovation is happening. And for a country like the US that has had such an excellent history of scientific projects and institutions, if they want to be the institution of AI research they should consider it useful and imperative to have that open model investment be intentional and understood and just kind of something that they are in control of. Right now that influence is shifting to China where I think in both models and where research is done and shared it's just kind of something that we cannot know what the future will hold.

[4:44](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=284s) But I would guess -- is this something you want to be in the position to reap the upsides of? China now has a very interesting ecosystem of open models and research and we don't know what will fall out of it. It's just an unknown in terms of technological progression. The US has the resources to own this and also such great academic institutions that want to be more activated and more involved.

[5:04](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=304s) So when DeepSeek happened, when Chinese models started -- open models started just taking off one after another -- was it like pure geopolitical, just in the competition with the US? What's the reason behind Chinese action? I think DeepSeek was fairly ideological in it where they were in the purest scientific way of wanting to create knowledge and share lots of it with the world and share lots of the upsides. They kind of created an industry standard in China where DeepSeek was the spark that made Chinese companies interested in participating in AI and they saw that you could do this through open models.

[5:40](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=340s) So lots of them just consider that the default starting behavior. And if you talk to the companies, they're also extremely reasonable where they know that tech companies and potential customers in the US won't sign up for an API where data is sent to China and they pay. They've told me this. They're like, well, our only other option is open weights because then they could still most likely use it. And they know that there's other second order concerns where IT departments will be like, well, is the open weight safe? But it's at least a card that they can play.

[6:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=367s) And the companies realize this. I think they don't have a business model figured out any better than something like Llama would have had a business model figured out. In the next 1 to 3 years, we'll see how funding continues to evolve for open models in the US and China. My hunch would be that the US ecosystem has a lot more liquidity to fund model training efforts. But then why are the Chinese open models so legitimately close to the frontier still? We don't really know. Opus and GPT 5.2 are I would say clearly better than the best open models, but the best open models from China have probably exceeded my expectations and how legitimately good they are.

[6:46](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=406s) So there's guesses that you would have on the long term of the ecosystem and how much better the models would be, but there's also a lot of real information today that makes it seem like weird things might be going on that we can't quite easily tie a bow around. Yeah, I just had a conversation with Minimax researcher Olive Song and from the conversation with her, it just seems that they are shipping basically every month something new, some new version and the research just keeps going -- nights, dates, weekends, it just doesn't matter. People work in shifts.

[7:17](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=437s) I don't really see anything like this happening in the US, especially since Zuckerberg kind of backed off with Llama. Who is the player on that level? Like DeepSeek, Minimax, Qwen. I think the characteristic is that the absolute best talent in China is doing this at this level for open models. That vibe is very similar to what Anthropic, OpenAI and all these like Gemini -- the vibes are like right now, but the talent density there is definitely higher than in open models which are often linked to academic projects.

[7:48](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=468s) Like AI2 is obviously heavily influenced by UDub, Percy Liang's efforts at Stanford. Some of this is a bit different talent pools. I would say the closest thing that the US had is Nvidia's Nematron efforts which I think has made a lot of strides in the last 6 to 12 months where my read is that they've kind of figured out some of the internal team org and culture alignment which has let them put out a lot more in higher quality but it hasn't quite had the top end breakthrough that something like Qwen or Llama has.

[8:16](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=496s) So I think that they're going in the right direction but breaking into the absolute cutting edge of AI takes something special. It's like what does it take? Like Llama was so successful. Qwen is obviously successful. DeepSeek is. I don't think you can just brute force that into existing but Nvidia is close to that. They also find the business model behind it because they make it the way that people will use their hardware and software later. So that makes total sense for Nvidia. There's no other players who would have this opportunity like them.

[8:42](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=522s) Yeah. I did an interview with one of the VPs who leads -- there's three VPs that lead the Nematron effort which is their open models. That's what I asked him like why you do this and he was like "because we're at the frontier of language modeling research and Nvidia is going to sell more GPUs." I was like damn straight. At least they have a much clearer business model than anybody else does or has in open models. So for that reason I'm optimistic in the longevity of it.

[9:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=547s) What is the shift happening in the open ecosystem currently with all these new models in terms of research? What is the most interesting things for you happening? People are trying to figure out the right ways to make models and recipes that are extremely compelling like tool use agents. And the sense is that the closed labs, the frontier models have invested so much in these so-called training environments where they can do this post-training in so many different domains. Some of them probably useless, many of them fruitful.

[9:39](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=579s) Where the open ecosystem is in the early days of creating ecosystem-like systems for training there. I think Prime Intellect's verifiers is one. There are others out there where it's just so early and unclear on the open way to apply all of these into pretty comprehensive training runs. In academia, I'm trying to figure out what research means for open models and open specialized models to kind of be more cooperative in a coding agent setup in 6 to 9 months.

[10:08](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=608s) So, if you're running a lab that can train some smaller models and hill climb on tasks, but isn't going to compete with the likes of Claude or Qwen or these much more established rich agencies, it's like there's clearly going to be a multi-agent future where multiple models can be used. The absolute top end of academic work could still tap into things that are actually used, but the ecosystem doesn't really operate in that way where if I'm using Claude, I don't offload to a local model that can read all my files or something like this.

[10:36](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=636s) Like there's a future there that is coming and there's a lot of open-source coding agents like OpenCode and stuff like this where they are trying to figure out how to make coding agents around open models. So in terms of the area where there will probably be the most dynamism and excitement, I still think it's going to be this kind of tool use post-training. Hopefully generalized across many environments is exciting because it's the path towards what they're doing at the frontier, but there's the chance that the frontier kind of becomes even more separated from academic and small scale open models this year.

[11:08](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=668s) Compute spend that's forecasted to go up and up for all these frontier labs with more compute coming online. Is the post-training the hardest part to make open? They all have different challenges. I think pre-training data is the hardest legal part to get open because you want to get every corpus on the possible internet and human knowledge. Obviously, some of those have historically been fairly litigious if you put them out openly.

[11:32](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=692s) I think post-training tends to be fairly complex at the frontier with a lot of models and a lot of sequencing, hard decision-making process of what do you put together into the final recipe. There was a big increase in the complexity of infrastructure for post-training with this RLVR and scaling reinforcement learning revolution. At the end of the day, a lot of it seems like pre-training where the open source infrastructure for pre-training with things like Megatron LM from Nvidia are actually very strong.

[11:56](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=716s) So some of these labs that are raising hundreds of millions of dollars to train models, they just take this open Nvidia software and they make it work. I think reinforcement learning is in the era of many many libraries but over the years it'll distill down to a few libraries that actually work fairly well. But in the meantime the complexity to taking a library and doing it very well -- training very well at the cutting edge of post-training is pretty hard. Post-training data is potentially further behind but it's not like there's that much open datasets anywhere in the spectrum of training. I think that high-quality release data is just super super rare these days.

[12:33](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=753s) Do you work on this at the Allen Institute for AI? Yeah, we touch on all of these things. I think for example, we've released OLMo 3 in the fall and one of the current things is trying to transition this to be more agentic and more tool use. And what the workflow looks like is that you find the existing open datasets, you look at the evaluations you want to improve on, and you try them. And I expect that for some evaluations, you can look at like Artificial Analysis and go through the list. Some of them we will find open data that makes it fairly easy to hill climb on them.

[13:01](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=781s) And other things that might be super popular to closed labs, we would potentially have to buy data for like order of millions of dollars. And I just expect there to be gaps because so few people release the data. It would seem oddly convenient if academics happened to make all the datasets we need to try to do the things that the frontier models are doing when you know the frontier model playbook is to buy a lot of this data at least to get the flywheel going.

[13:24](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=804s) Right. So I think there's natural barriers, but the same thing why open models end up kind of having a natural lag behind closed models. It's like once the closed models are good at it, it's a lot easier to create training data with those models and put human effort into the gaps. That's kind of the ever evolving dance where people like to overhype open models in my opinion as like "oh they're going to cross closed models" and I just think the equilibrium is going to continue where the best open models are some 6 to 9 months behind the best closed models and that's fine. That's a pretty short timeline with how good the best closed models are now. But I don't see the dynamic changing.

[14:07](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=847s) If anything it might err slightly on the side of the closed models being more ahead. But even 9 months of a gap is crazy. We don't think they will catch up. There's no reason to think that the open models will -- they have fewer resources and resources normally determine the outcome. It's like resources and talent determine the outcome. Arguably in Chinese labs, the talent is proportionally similar to the likes of OpenAI and Anthropic, but the resources in terms of compute and ability to buy data is just so much lower.

[14:32](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=872s) At the end of the day, it's mostly compute that people need to make improvements to the model because compute is spent either on training or generating large amounts of synthetic data. I think for example in OLMo 3, this isn't super clearly documented, but we spent millions of dollars on synthetic data. A lot of it was through a grant from the federal government for this frontier supercomputer in the US to generate synthetic data. But like if even AI2 is spending millions of dollars on effective compute for synthetic data and training, you're going to guess that the compute expenditure there is almost like billions at these frontier labs. These are just huge costs constantly that the western companies are way more capitalized to do and these are closed labs right now.

[15:12](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=912s) Kind of not infinite. If they're behind only 6-8 months and if the usage is already possible by businesses and individual people, we can imagine the situation in 6 months that you will be able to use an open model just for your daily life as you use ChatGPT or Claude, don't you think so? It's a bit of a bet. I think on the chatbot side open models will definitely be there. I think there's some robustness and some tool use stuff where I haven't seen an open model that's quite as good as search as like GPT 5.2. But in this chat interface, I think it will be there.

[15:46](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=946s) The coding agents potentially not, but it comes down to a bet. And I would say that I bet on the side that the closed models are so early in their interest in these coding agents. A lot of the way this happens is that the researchers become interested in a domain in a way of using the models and then they take on improving them. If Claude Code and the likes came out last April and adoption is not front-led by the training teams.

[16:08](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=968s) I've talked to people at these companies and they're like, "Wow, yeah, Cloud Code with Opus 4.5 is when I finally started using coding agents a couple months ago." And if they're like still just getting obsessed with them, then they will decide, oh, we need to train the model that's really really good at this. So, there's still this catch-up time where they're going to start turning that crank and I think the coding agents will get much better.

[16:29](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=989s) It comes down to a bet. You could say, "Yes, these costs -- the upside is not going to be proportionate to the cost and open models will catch up." It just seems like there is no clear indication that the models will actually hit a wall. It seems like everybody I talked to is like there's a lot of low-hanging fruit -- it's complex technical work in terms of research and execution and cost and we need to keep turning the cranks.

[16:48](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1008s) I think obviously there's the macroeconomic sense where there's a time gating on the companies to show the value of it. But it seems like this Cloud Code moment is going to -- it at least buys them a lot more time to spend.

[17:02](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1022s) What low-hanging fruits do you see? It's literally like anywhere. A lot of it becomes like there are different flavors of "we put this dataset in and it helped a lot -- how do we make it 10x bigger?" Or "we put this dataset in, it helped a lot and we didn't really filter it very well yet, so let's filter it more." Or "our code for training only uses 60% of the GPU utilization so we can make it 10% faster by writing some better kernels and therefore all of our experiments are 10% faster."

[17:29](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1049s) And if you do that 40 times, you end up with like a 4x faster codebase and all of your experiments are way faster and you can do more complicated ideas and things. So it's pretty much all of these things from the most established which is "we have our pre-training dataset that we've been filtering and iterating on for years and they're still tweaking it" in order to better serve the current tasks of interest, in order to be more efficient.

[17:53](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1073s) To these cutting edge things of "we just spent $40 million on three training environments for coding agents because we have all these new users and Codex and we just plugged it in on our first run and some numbers go up -- how do we pick the next thing to do there?" Or complicated things like "we try to just make the model bigger and the numerical issues are too hard -- how do we come up with a new RL algorithm that handles this numeric better."

[18:15](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1095s) So I just think that there's constantly these little problems of what is better suited to the situation at hand. It's like a lot of the open models are switching to these hybrid architectures which is a mix of this linear attention and the traditional attention. And part of why I think that's the case is they're just more complex numerically, but also the upside on downstream RL and inference is so high where you can save so much on RL where it's just kind of like the industry collectively pushing through the next harder thing to do.

[18:45](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1125s) It seems like Qwen 3 has a linear model, Kimi has a linear model, like we're working on a linear model. People found it scraping GitHub. Why does that happen? It's like it's a collective readiness of infrastructure and ideas being tested at smaller scales so that they work. And I asked the lead of the project in AI2 like "do you think all the models will be hybrid in the future and why did this not happen two years ago when the hype for Mamba was really high?"

[19:10](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1150s) And it's just like a few settings on Mamba needed to be figured out where the Mamba models did really well on pre-training benchmarks but the actual text generation from them wasn't nice and there's just a few things in the architecture that needed to be changed. Balancing them better with traditional transformer style architectures and from that reason and a bit more tinkering the models are a lot more stable and people are trying to use them.

[19:32](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1172s) That's an example of over 2 and a half years where it's like I remember when the hype for Mamba and state space models was so high but now it seems to be really hitting a lot of places where like Nvidia had a model with a hybrid attention as well and it's just like how do you predict that timeline? I don't know. But I think there's definitely plenty of things like that that are going to continue to evolve from AI research to reality.

[19:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1194s) The big question is like what does Sam Altman do on the fundraising engine. I don't think that's going to be a bubble popping thing but there could be corrections on Nvidia stock or stuff in this time. People are rumoring OpenAI and Anthropic IPOs by the end of the year. I wouldn't be that surprised.

[20:11](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1211s) Yeah. What do you think about SpaceX and their excitement? Musk Industries, for better or worse, has a bit of villain vibes, but in a sci-fi movie, and I'm not enough of a businessman to comment on what is actually happening. I would guess that there's a reason other than just greed and recouping losses from X or xAI. For whatever opinion on Elon you have, especially on the political side that I think he should spend less time in, he has such a track record in building businesses that I have to think that there is a plan there.

[20:47](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1247s) And I think that the recent Tesla comments of stopping the Model S and the Model X is fairly shocking to me. I think particularly because the timeline on robotics seems too soon and my intuitions are that scaling robotics now is a bit too soon but when there are that serious of bets done by somebody like Elon it's like there has to be reasons and things that I don't know as a fairly ignorant person about SpaceX and Tesla and large scale robotic manufacturing. I'm not informed here so we'll see.

[21:21](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1281s) You can add a couple of years to what Elon Musk projects but eventually he will reach the goal. That's the track record. Yeah. Everything you described from this research point is mostly with transformers and with occasional hybrid models. Do you see serious research going into some other areas?

[21:38](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1298s) For whatever reason, there's this kind of continual learning hype point. I'm kind of of the bucket that some sort of like -- in the coding agents we have these CLAUDE.md files and agents.md and they're actually fairly good at learning from them and in the short term that'll do a lot but obviously there's such a big gain in something. I think continual learning is best thought of as an example for a research problem that will eventually be solved and is wonderfully motivated but you'll never know when a real solution hits at scale.

[22:05](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1325s) And the idea is that the model weights should change based on experience personal to you or the model in the world. And that is so reasonable when training is so expensive and it can seem like the models are so dumb in some situations as a motivation. It just kind of has to be true. I think the likelihood that we're on something that looks like a transformer in 20 years -- I don't know. It's not that high. We have the hybrid model thing a couple more transformations down the line. Are people going to still call it a transformer? I don't know. Seems like attention is pretty good. It's just so hard to predict the research evolution.

[22:38](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1358s) I'm asking this question because in the Lex Fridman podcast you said if you were starting you probably would not do transformers now. Is this for an academic? Yeah. The best research is further out. I think of deep learning and transformers as kind of fundamental skills that you have as a CS researcher where when I was starting it's just like okay, you just need to learn about how deep learning backprop libraries work and now people need to learn about how transformers work.

[23:05](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1385s) But if you're doing work that is so blatantly just improve on what we already have, it's going to be kind of hard to market it academically, unless you're at the absolute top end of academia where they have more resources and industry connections to really stay at this frontier. So like some of the labs at UDub and Stanford and Berkeley do this, but most of academia is not like this. So you need something that is less busy or takes longer.

[23:29](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1409s) I think another thing that could be described is like find space in between two popular ways of thinking or two subfields or just go where there are not people because there are some like -- things are by their nature very likely to work in AI. The amount of things that just kind of work is really really high if you set the optimization up right. I don't know if I love my own advice because it's easy to say like go off into somewhere but it's also hard to make it work.

[23:53](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1433s) Like "don't do transformers" -- it's like okay, what do I do? I don't know. I mean you did reinforcement learning when it was kind of a dump. Yeah. I think we also talked about robotics and working in robotics -- I've seen many people be so happy. It's a bit more grounded but you accumulate the benefits that is happening in this language model revolution while kind of being a bit more secondhand to it. So you're probably not subject to the whiplash but you're getting the upside of new types of things working and the ability to try new things in a specific domain.

[24:27](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1467s) If we talk more about practical implementation of open models -- first of all if you don't mind we start from kind of defining what open model, open source AI is and I don't even know if it matters. Do you think definitions matter? The definition doesn't matter in the classical sense of the debate of like assigning things to -- it matters in what the community definition is and the community definition of open source is really this like weights are released available openly so that anyone can use them.

[24:55](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1495s) And I think that chimes into the license discussion where it's one of the better things these Chinese labs have done is they're all just releasing their models with extremely permissive licenses where a lot of US companies have done things where it's like non-commercial or extra terms and conditions attached. And in enterprise situations, lawyers do not like vague extra terms attached. And a lot of people that end up writing these custom AI licenses do it in ways that there's vague language and exposure of risk.

[25:20](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1520s) So just having simple terms where it's clear that people can use them is great. And in the past there's been a lot of debate on like open source you need the code and the data available which is like yes these are valuable resources and yes that fits more with the open source software motivations of things are free and reproducible and modifiable and so on but it's just like the debate has kind of died down when people actually just use open weight models. It's like this is the thing and this is the paradigm by which people are operating in something that's more open with AI.

[25:48](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1548s) So I generally am like, "Oh, it's fine." I'm happy to not be in stupid debates on like, "Oh, is this open source or not?" That was not fun. And still, it's still almost fully transparent, fully open source. There's a group. It's like AI2, Percy Liang, Marin Thing, Hugging Face, LLM 360, which is also affiliated with MBZUAI. Yeah. The Swiss had a project that was very open.

[26:20](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1580s) So there are actually more people especially in the so-called western ecosystem that are doing this fully open thing which I think is a very good appreciation for scientific standards and scientific progress but it's still niche. It's not the fact that these are fully open is not causing dramatically more uptake. It would be nice if some company was like, "Oh, these are fully open, so I can iterate faster and so on." But just using a better model is way more benefit than having the model be fully open in terms of real world applications.

[26:49](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1609s) My bet is that 2026 will be the year when people would start and enterprises also will start using open models much more. How do you see it? I think they are -- I think it's been ongoing. A lot of companies' default position is they want to use open models for information security, more predictable costs, owning their stack and it takes a long time for companies to move into these new types of technology.

[27:16](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1636s) I suspect it'll look like all the same things where there's all these surveys of like are companies getting benefit out of using AI tools for co-working and it'll be noisy for a very long time but then it'll be seen that a lot of companies are using open models and fine-tuning them for their use cases in ways that creates a lot of value. It's just going to take a long time, especially if any training is involved where you just can't take a model off the shelf because it's enough to thoroughly test a model. To have to actually do the training yourselves is months long of a commitment.

[27:49](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1669s) And you need to have a team of dedicated people. I started to learn about economics of open source AI -- like if you're a company and if you want to use an open model, it's like sometimes prohibitively expensive. So it's like free open-source model, not cheap. Yeah, because you have to commit to a certain amount of compute to serve a model in a meaningful way and you could very easily buy the minimum compute to host said model and get nowhere near the inference load to frankly support that spend.

[28:21](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1701s) That's why some of the disaggregated inference companies make sense where it's just like they will get enough load to make certain features worthwhile and stuff like this that it just takes time to build that up.

[28:32](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1712s) How is it organized in AI2? Do you guys use open models in the institute? Some people do but not a lot. I think the core thing that AI2 needs to get to for the models to take the next step is to actually -- you have to dogfood the models and build development loops between using the model and improving the model and getting real feedback because until then it's somewhat of a toy project where there are users and there are things that are deployed into but if you're not touching it like why do you expect other people to do so with serious intent.

[29:04](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1744s) Do you actively use open models or closed APIs? I mostly use closed models. I try them especially around launches. They're not good enough or cheap enough where it's obvious that I want to switch my habits. And a lot of this is classic product patterns in tech where users have strong habits. And I see the slow evolution back and forth between say like ChatGPT and Claude for certain uses. And it takes a long time for those habits to shift.

[29:31](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1771s) Even though I think Claude's language on normal conversation is much much more succinct and palatable than a lot of the GPT5 things but Claude models have been like that for a while. It's just taken me months to reshift the habit and this evolution has happened multiple times in AI's recent history where I go to an open model and it's just fine. It's like why would I in a product sense use this?

[29:54](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1794s) I think Florian who works with me at Interconnects to study open models has a few things he uses them religiously for because only the open models do this -- which is like he uses one of Nvidia's Parakeet models which is speech to text and then as a much faster way to input comprehensively into Claude Code. So he will speak to his AI and then Parakeet will transcribe them and then Qwen FormB will rewrite it and pass it to Claude. That's a very real use case that he swears by and gets a lot of value out of. But I'm like, I haven't done it yet.

[30:27](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1827s) Yeah, it's surprisingly how hard it is to change a habit with a model. I still struggle with 5.2. I think they changed something. I don't know what they changed. No matter what character I choose, it sucks. I still use it for thinking. I still like deep research part of it, but when I read what it tells me, it just -- and it doesn't follow the rules. I don't know how to make them follow the rules.

[30:50](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1850s) It's pretty funny because it's like I feel like everybody that's deep in it kind of has these opinions just like "what is this problem?" And that's what I mean by the low-hanging fruit. It's like people just need to fix that. That's actually fixable. It just takes a lot of work. Training like to make the model actually remember the constant prompts like the same prompt that I put in the memory.

[31:11](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1871s) Yeah, it's just like changing data and creating new data. The process for getting that type of potentially niche problem to be represented when you're solving thousands of use cases at once is just like it takes a lot of balancing. You probably need to build an evaluation for it so that you can kind of automatically measure it and then you have one more evaluation. They probably have hundreds of evals. There's just a lot of moving pieces to get right.

[31:34](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1894s) So I think they're fairly conservative in what things are really the priority and they probably are willing to take regressions in some things like memory management to push the frontier in utility. If you're going to take a gigantic step in the things you were targeting and you have some second order effects that you're going to revisit later, you're probably going to do it.

[31:53](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1913s) Right. The focus now is on coding and in coding you don't really need this beautiful language. You said that you're interested in working on characters of the model. Is it the same with open models? Is it more complicated? It reduces to pretty much being data work, which is like you want all of your data to look very similarly. Closed models are famous for this where we have this GPT-4o thing where people still are fiercely loyal to it. And Claude's language is very different than GPT 5.2 and it's just very intentional.

[32:22](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1942s) Every layer of the stack. The first intervention is at post-training because it's most direct, but eventually you need to -- you would probably align pre-training to make sure that it helps achieve this. And it's largely an industrial habit because it fits so nicely into the model as the product. Therefore, the model's language is the interface by which the user sees the information. And therefore the model's outputs are so literally the product.

[32:48](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1968s) If the product is what keeps the person engaged and uses certain triggers to get people to come back to it and the output is so deeply entwined in how you would approach what is the user experience for this that it is much harder to study in academia. And I think that this is continuing with coding agents where it's like what is Claude presenting to me as I use it is a very interesting problem because the CLI is such a sparse interface but the few interactions I have with Claude are so impactful to the final outcome.

[33:18](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=1998s) And that is also very much of like literally the model's output in that harness is so existentially tied to this product. And I think that due to its importance to the AI industry is why I'm trying to help grow academic research around it but I don't expect it to take off. Other projects I've worked on in this area -- as far as like "oh there's no evaluation for reward models and reward models are important for RLHF and AI research" -- yeah that blew up because obviously people want to do post-training research.

[33:45](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2025s) But I'm not sure that character training will blow up in the same way because it's just not as easy to study. But it fits with my kind of goals of make it so that AI is -- if not accessible to everybody at the frontier -- it's just like you can understand what is happening and make your own bets about the direction of travel for the frontier to prepare whatever you think about.

[34:06](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2046s) What would you say are a few directions to follow for 2026 and 2027? Let's see. I think most -- this agentic thing is very real. So it's like how does this show up in multiple ways? One is what is the trajectory of the coding agents and then two how do behaviors like this show up in other types of agent applications or like coding agents expanding into more domains and what that means for models and software today and so on.

[34:33](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2073s) But it's much harder. There were rumors about Claude 5 being released today. So, I was thinking about what the story of it would be and I remember last year all the way back to like Claude 4, their releases were very muted on benchmarks. They had different benchmarks than OpenAI and Gemini. They didn't look as flashy. And this fits with their bet on code, but also fits with this kind of thing where evaluations for AI are much more about what some technical people call the harness or the product than just the model.

[35:02](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2102s) And I think Anthropic was very early on this where the comparison is Gemini 3 which came out late in the year and was so hyped as like "Google is back" but has almost zero really cutting edge impact right now and they kind of didn't have this agentic experience at launch and all these things they're still lagging behind on. So I think that gray area in between the lines of what is happening with releases is where the frontier is felt today.

[35:27](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2127s) And I expect Gemini to try to catch up and that explains OpenAI's efforts on Codex but it is just much harder to follow because it's not just like "oh numbers go up, open numbers went up the most." It's much more you need to try it and it might emerge into new domains. So for that reason I think following AI is a bit messier than it had been post-ChatGPT where it's just like smarter chatbot, turn the crank, GPT-4. It was much easier but the chat domain is so saturated it's really hard to see the differences between the models.

[35:58](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2158s) Do you see any differentiation coming? So we've talked about chatbots, robotics, agentic systems -- what are other big things? Off the top of my head I feel like I don't really know. The thing that goes with open models is that I think sovereign AI will continue to be a term that is used where yes the US and China so obviously have a foothold in AI. There's going to be a lot of continued international investment in compute and models.

[36:30](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2190s) That's why France is so happy with it -- they found a company that was willing to accept that narrative early on and that puts them in a really strong position in the EU but also elsewhere in the world and I think that will continue. But I don't think that's as important on the scale of the other things you're talking about. Like important things are power buildout and the sociological factors of this where GPUs themselves don't use a lot of water but construction is deeply personal to communities and construction materials do use a lot of water.

[37:04](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2224s) I'm not super well read on this issue but that's like a major blocker in how AI is viewed in the US right now and will still be a very important story. That's true. It's funny enough a lot of kids when they talk about AI, that's the first thing they say is that it uses a lot of water and I don't even understand where it comes from because that's not exactly how it works.

[37:25](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2245s) The way it is described will evolve and pass, but the construction limiting factor is real in the US where I think that'll continue to be a big story. I don't think it'll always be just water. I think that's kind of like almost like how memes are hard to predict. The language that people use to express the problem is often hard to know and predict. But the water thing is mostly just about the buildout of these wealthiest companies happening in ways that is not often actually that supportive of the environment and the communities where the buildout happens.

[38:00](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2280s) Even if the wealth creation from large data centers is so high in terms of tech company bottom line, it's a micro-scale problem that is blocking tech companies in a way that they haven't had to deal with in the past. But it kind of makes sense. It's a big picture lot of money happening, but it happens in a very small part of the world where they don't get that benefit.

[38:21](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2301s) Can open source be the opposite of this narrative, that it doesn't need so much resources? Most of the resources I see as being for inference and some inference you can obviously do on your computer, but things like coding agents -- that inference is not going to be offloaded onto your computer anytime soon if you want to be at the frontier of performance. Or like Gemini and OpenAI image generation -- both of those transformer-based image gen or video diffusion models is very far away from being cheap.

[38:52](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2332s) Much of the buildout, to the extent the AI world succeeds, has to be for inference because inference is what pays back. You can get a version of this on open models on your local computers and some of it -- and open models are good in ways where you don't have to retrain your model from scratch. But I think these are not defining pivot points. These are not things that are going to put us in a fork in the road between two different outcomes for AI. They're contributing and they can pile up but they're not the central issues.

[39:20](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2360s) Most likely it will be hybrid, open and closed together. Yeah. And we'll see if Apple pulls things together and stuff like -- there are companies like that where they should be figuring out what open models mean to consumers in much more compelling and easy to use and at scale ways where currently there's maybe no exploration to the general public in terms of real open model usage at scale. There should be a stack at least for the developers that they can use easily.

[39:50](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2390s) So there's a lot of problems as I can hear. What is clearly working in open source AI right now? Well, I think there's kind of insatiable demand for people wanting to own their stack and that's in US tech companies -- even in very large companies like banks and very large sectors of the US economy. It's a very financialized economy where these financial services are valuable. But internationally there's so much value as well where new economies aren't traditionally spending a lot on paid internet services.

[40:18](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2418s) So then open models become their only access to AI in whatever capacity fits in their current situation. I see open models as being on the path to be disconnected from the absolute frontier of performance which is these expensive agents with the AI subscriptions getting even more expensive. I pay a lot for them. When these models can be pushed even further and people pay even more -- that's not happening on an open model and that's where the frontier of AI is going.

[40:48](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2448s) But it's kind of this background of everything else where the ecosystem is so focused on SF and so focused on the cutting edge. There's still a lot of other AI use in the world to be filled in and to be evolved with and I think there's a lot of unknowns that are going to happen in open models where things are just kind of not as closely tracked and still very influential at the world scale. Research is just slow. Research and establishing technologies are both very slow trends and that's where I see open models to be so important.

[41:18](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2478s) If the tech sector being fast is just kind of a different story -- the timelines are just so different. So open models should be just like a field for experimentation. Yes. And we don't -- it's harder to see this experimentation and harder to track but just because it's slow it's still happening whether or not the frontier succeeds to the best ability. The more of the frontier succeeds, the more demand for open models will increase as well based on the certain use cases where they can't use closed models.

[41:50](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2510s) Yeah. But when you go to people to tell them about your project OLMo, what do you tell them? Why is it important? Owning the engine for AI innovation for decades to come and being the central source of influence in AI research. You can look at the media ecosystem around AI research where it's so obviously valuable to be seen as even associated with this at the cutting edge and we need strong open models so that the pipeline from research in the US to new startups and established companies building on top of research -- there's not this friction of "but wait that's a Chinese model, can I use it?"

[42:25](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2545s) It's not so expensive compared to the buildout that's underway -- just owning this innovation is so worthwhile to the US in terms of being -- it also will give us a better ability to understand how open models are used because people will come to the people building the models and be like "make it better for this" and so on.

[42:45](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2565s) So yeah, again the gap between academia and business and open models should be bridged to get more feedback, more use cases. Yeah it's tough. I'm also fading because I need to go eat lunch after this. I feel like that's not the best answer, but that's realistically what it is -- whether or not you believe in research as an engine for innovation and value. And I think much of the current tech ecosystem is built off of that. And some of that innovation is just downstream of basics like the internet.

[43:15](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2595s) But a lot of it is other research and databases and fundamental deep learning stuff which big tech has captured the value from. And I think that realistically these companies would capture the value from AI research being here. That has been the pattern for the tech ecosystem's history and letting Chinese companies take more ownership of that is just increasing the likelihood of those companies becoming the ones that capture the value. So I mean that's why Nvidia is investing. Nvidia sees the path. Nvidia knows how to make money for sure and how to capture attention.

[43:37](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2617s) If we throw in a very famous word AGI, do you think open source is vital for reaching AGI or just fundamental in tension with it? I have two views on AGI. One is what we have is an AGI. And two, I understand the colloquial SF lingo for AGI, which is like a drop-in replacement for a remote worker. I see -- I understand why they thought GPT-4 was not AGI. And I would push them again on Claude Code's latest form being pretty close to their definition of AGI if you really are flexible and work with it.

[44:10](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2650s) Open models can contribute to this and can cross the thresholds at their own time. I think open models are mostly just for education and reducing the risk of concentration of power and bringing transparency to an ecosystem where AGI is obviously this important thing and open models should help increase trust and awareness of the story that is evolving. And I think there are different forms of AGI that you can imagine with open models where you have tons of different models for different use cases and other kind of dramatic ideas.

[44:40](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2680s) Like what if an architecture changes where these models you can really swap in experts and things -- and these are mostly far out but sometimes far out ideas become reality and people will keep exploring this because it's happening. It's happening in the open whether or not people are following it.

[45:05](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2705s) Thank you. My last question is always about the book. What was the book that influenced you maybe in your childhood, maybe recently, but that you remember? I think the recent one still has a story and I mentioned this on the Lex podcast as well, which is I read "The Season of the Witch," which is a history of San Francisco from like the '60s, '70s, '80s, where there's multiple movements through the hippie movement, the Vietnam War, when the gay community came to the city.

[45:32](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2732s) And there's just so much turmoil and human challenge that transitioned San Francisco from almost like a New England vibe of a traditional Irish heavily Catholic city to this multicultural phenomenon of dynamic culture and people coming. And it's just so recent to have spent over 7 years of my life there and to not know most of this history and how it seems like so much of the tech culture is so separated from this and what is such a rich city's history.

[46:00](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2760s) I just think that more people should know about this and think about the community and area that they live in. So I recommend people that I know in SF to read it and I got the recommendation from multiple friends that had been reading it in my Bay Area circles. But I just think that this stuff still matters and there's currently a lot of friction between tech and society at large. I think largely due to this disconnect and lack of empathy towards very recent things that have happened.

[46:25](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2785s) Yeah, it was a very interesting time full of hope at that moment in America. I really like that period because people had so many ideas and dreams. I think that's what we lack currently in the states. Yeah. And it was a very human era where now it seems like it's somewhat of a meme, but big tech is dumping so much money into industry where big tech is defining the trajectory of the country's economy and things. And that is very dissociative to many people for good reasons.

[46:55](https://www.youtube.com/watch?v=mq_V6zAjsSI&t=2815s) And with open source, it kind of would a little bit prevent that. Realistically, I think they're going on the path whether or not people build open models. But building open models is a good way for people that do not want to be partaking in that economy and have other options to use AI and understand the world. Thank you. Thank you so much.
