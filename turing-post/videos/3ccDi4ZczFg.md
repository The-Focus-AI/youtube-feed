---
video_id: 3ccDi4ZczFg
title: "Why AI Intelligence is Nothing Without Visual Memory | Shawn Shen on the Future of Embodied AI"
channel: Turing Post
duration: 1675
duration_formatted: "27:55"
view_count: 1056
upload_date: 2024-12-20
url: https://www.youtube.com/watch?v=3ccDi4ZczFg
thumbnail: https://i.ytimg.com/vi_webp/3ccDi4ZczFg/maxresdefault.webp
tags:
  - Artificial Intelligence
  - AI Innovations
  - Future of AI
  - Machine Learning
  - Neural Networks
  - AI Breakthroughs
  - Technology and Society
  - EmbodiedAI
  - ComputerVision
  - MemoriesAI
  - WorldModels
  - EdgeAI
  - Robotics
  - VisualMemory
---

# Why AI Intelligence is Nothing Without Visual Memory | Shawn Shen on the Future of Embodied AI

## Summary

Shawn Shen, former Meta Reality Labs researcher and co-founder of Memories AI, presents a compelling argument that true AI intelligence requires long-term visual memory. Drawing from his background in computational neuroscience, Shen explains that human cognition operates through two parallel systems: intelligence and memory. He argues that for AI to become truly embodied - whether in robots, smart glasses, or wearables - it must be able to see and remember what it has seen, just as humans do with their visual memories.

The conversation explores the technical architecture behind Memories AI's Large Visual Memory Model (LVMM), which differs fundamentally from large language models. While LLMs are trained to be creative and intelligent, the LVMM is an all-in-one embedding model designed to transform multimodal data (video, audio, text, actions) into a unified embedding space that can be losslessly reconstructed. Shen discusses the limitations of current transformer architectures for understanding physics, time, and object permanence, leading their team to develop world model architectures to solve these problems.

A key innovation discussed is "encode for machine, not encode for human" - a paradigm shift in video compression that jointly trains compression and indexing algorithms specifically for AI processing rather than human viewing. The interview also covers practical challenges of on-device deployment, the data bottleneck for physical AI, privacy considerations, and Shen's perspective that personalized context may be more valuable than super intelligence for practical applications.

## Key Points

- **Embodied Intelligence Requires Visual Memory** ([1:19](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=79s)) - Future hardware like robots, smart glasses, and wearables must have visual memory because AI cannot be useful if it can see but cannot remember what it saw.

- **Cognition = Intelligence + Memory (Separate Systems)** ([2:05](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=125s)) - Human cognition operates with intelligence and memory as totally separate, parallel systems. AI should be architected the same way.

- **Large Visual Memory Model vs. LLMs** ([3:28](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=208s)) - Unlike LLMs trained for creativity, the LVMM is an all-in-one embedding model that converts all multimodal data into the same embedding space for lossless reconstruction.

- **Memory System = Indexing + Retrieval** ([4:34](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=274s)) - The ultimate memory model is a system with two components: indexing/encoding (the LVMM) and retrieval systems that enable multimodal AI agents.

- **Transformers Don't Understand Physics** ([5:48](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=348s)) - Current transformer architectures lack temporal contextual awareness - they cannot track object permanence or recognize a person who changes clothes or turns around.

- **World Model Architecture Development** ([6:42](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=402s)) - Memories AI is developing world model architectures to solve the temporal awareness problem that transformers cannot handle.

- **Neuroscience-Inspired Blueprint** ([7:17](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=437s)) - The team started by studying how human memory works, publishing "Human-inspired Perspectives: A Survey on AI Long-term Memory" to create their development blueprint.

- **Customer-Driven Development** ([8:15](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=495s)) - After launch, over 100-200 inbounds and 60-70 contracts revealed market needs: human tracking, object tracking, knowledge graphs around humans.

- **Differentiation from Competitors** ([9:12](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=552s)) - Unlike 12 Labs or Co-Active focused on studios/publishers, Memories AI targets humanoid robots and embodied AI with on-device processing.

- **Qualcomm Partnership for On-Device** ([10:28](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=628s)) - Future humanoids cannot rely on constant Wi-Fi; models must run locally on low-power chips, which is why they partner with Qualcomm.

- **Encode for Machine, Not Human** ([11:41](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=701s)) - Traditional video compression is designed for human decoding. They jointly train compression and indexing for direct AI processing.

- **Emergent Forgetting Mechanisms** ([12:32](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=752s)) - Rather than programmatic forgetting, the goal is emergence capability similar to how ChatGPT naturally remembers important logic but forgets fine details.

- **Data Bottleneck for Physical AI** ([14:14](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=854s)) - No large-scale internet data exists of human visual life memories labeled with actions - this is a key missing piece for training world models.

- **Data Flywheel Strategy** ([15:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=900s)) - Good models lead to good products (glasses, wearables), which generate quality data that people can contribute back for better models.

- **Multimodal Speaker Recognition** ([17:10](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1030s)) - Training models to recognize who is speaking using both audio and visual cues, not just audio-based speaker diarization.

- **Memory Models Are Safer Than Intelligence Models** ([18:24](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1104s)) - Memory models only index and transform data into latent space - they don't generate new ideas, making them inherently safer than creative AI.

- **Personalization Over Super Intelligence** ([20:02](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1202s)) - A home robot doesn't need super intelligence; it needs general intelligence that deeply understands you, your family, and your lifestyle.

- **AGI Timeline: 10-20 Years** ([23:50](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1430s)) - Scaling laws aren't working; we're going back to research era. True AGI (like sci-fi movies) is 10-20 years away, not imminent.

- **The Mom Test Book Recommendation** ([25:13](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1513s)) - Recommended reading for researchers becoming entrepreneurs: teaches how to ask good questions to discover real customer needs without proposing solutions first.

## Mentions

### Companies
- **Meta Reality Labs** ([0:26](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=26s)) - Where Shawn Shen previously worked before founding Memories AI
- **Memories AI** ([0:36](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=36s)) - Shawn's startup building the world's first large visual memory model, emerged from stealth in July 2024
- **Qualcomm** ([10:28](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=628s)) - Partner for on-device chip optimization for embodied AI
- **12 Labs** ([8:58](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=538s)) - Competitor in video AI space, focused on studios and publishers
- **Veloca** ([8:58](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=538s)) - Another company trying to solve the memory problem
- **Co-Active** ([9:47](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=587s)) - Competitor aligned with studios as customer profile
- **World Labs** ([24:33](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1473s)) - Company defining world models as 3D reasoning
- **OpenAI** ([20:43](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1243s)) - Mentioned as planning to launch their own hardware in the future

### Products & Technologies
- **Large Visual Memory Model (LVMM)** ([3:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=180s)) - Memories AI's core product, an all-in-one embedding model for multimodal data
- **Transformer Architecture** ([3:05](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=185s)) - Current architecture used, still most scalable option despite limitations
- **World Models** ([5:50](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=350s)) - Architecture being developed to solve temporal awareness problems transformers cannot handle
- **Mamba** ([3:15](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=195s)) - Alternative architecture mentioned, but not yet scalable
- **ChatGPT** ([13:42](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=822s)) - Referenced as essentially a memory model of internet data with emergent forgetting
- **Mem0** ([12:57](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=777s)) - Text-based memory company using agentic memory agents and context engineering
- **Letta** ([12:57](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=777s)) - Another text-based memory company mentioned
- **AirPods** ([6:18](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=378s)) - Used as example of object permanence challenge for AI

### People
- **Shawn Shen** ([0:26](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=26s)) - Co-founder of Memories AI, former Meta Reality Labs researcher, background in computational neuroscience
- **Ksenia Se** ([0:21](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=21s)) - Host of the Inference show on Turing Post
- **Jensen Huang (implied)** ([23:59](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1439s)) - Referenced indirectly when mentioning NVIDIA's comments on scaling laws not working

## Surprising Quotes

> "You can't have an AI to be able to see but not remember from what it has seen. So they need to have this visual memory."
> — [1:49](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=109s)

> "We call this encode for machine. Not encode for human. Encode for machine."
> — [0:13](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=13s)

> "ChatGPT is essentially also a memory model of the whole internet data. It's essentially a reconstruction of the whole internet data. It remembers what is important but also it forgets what is less important."
> — [13:50](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=830s)

> "Do you really need a super intelligence in your home to do laundry? Probably not. You probably need a general intelligence, but then really really understand you."
> — [20:15](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1215s)

> "The scaling law is not working and we are going back to the research era. Real AGI... will probably come in the next 10 to 20 years but not anytime soon."
> — [24:01](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1441s)

## Transcript

[0:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=0s) Anything that has a physical camera is going to be embodied with intelligence. It has to have visual memories. It is critical to be able to make the model very very small. We call this encode for machine. Not encode for human. Encode for machine.

[0:21](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=21s) Hello everyone. Today I'm joined by Shawn Shen, who has left Meta Reality Labs with a provocative thesis that intelligence without long-term visual memory isn't really intelligence. He co-founded Memories AI to build the world's first large visual model, visual memory model. And you guys emerged from stealth just recently in July of this year, right?

[0:48](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=48s) That was when I first noticed you because the idea of creating a long-term memory for the models was really fascinating. So, welcome to the Inference show, Sean. Thank you so much.

[1:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=60s) When I first heard about Memories AI, I thought about all the sci-fi movies that I watched because in these movies, the memory, the recognition, the search through the memories is solved. But we're not in the movies. Do you think that visual memory specifically is the thing that unlocks intelligence?

[1:19](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=79s) Yeah, exactly. So I think in the future the intelligence is going to be embodied. What that means is that in the future your hardware, robotics, smart glasses, right? Smart wearables or your cameras, cameras on street, camera in the home, anything that has a physical camera is going to be embodied with intelligence. I think that is going to be the real embodied intelligence and that is going to be the real intelligence.

[1:44](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=104s) And for that intelligence to work it has to have visual memories because you can't have an AI to be able to see but not remember from what it has seen. So they need to have this visual memory. And also we take it from a very first principle point of view - we think about how human memories works, how human cognition system works. Our cognition system works by two things, right? One is intelligence, one is memory. It's totally separate and in parallel.

[2:12](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=132s) And then if we are going to build a better AI similar to how human does then we also need to split this into intelligence and memory. So memories will make intelligence better. And then what is actually memories? So memories I define this as visual memories because most of our memories are actually visuals.

[2:27](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=147s) For example, if I ask you what was the last time you had an amazing burger? How many times you went to the gym? Are you having a healthy week? Are you eating healthy for the last week? If I ask you this question, you would usually most likely recall what you have eaten, the gym, etc. visually or vividly. And then you record that and then you reason on top of that using your intelligence. So what we build is this encoding and retrieval process. So we call this memories.

[2:57](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=177s) Fascinating but super hard to build it. And your model is called large visual memory model. But as far as I understand it's still based on transformer architecture, right? Or have you moved beyond them?

[3:09](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=189s) It's still based on transformer architectures because currently there are some other architectures, for example Mamba, etc. The only model architecture that still is sort of scalable that you can scale up using data - transformer is still like the best option. So we are still using transformer architectures but then we are training this model for very different purposes to other models leveraging transformers.

[3:34](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=214s) For example, large language models are trained to be creative, to be intelligent, and sometimes they will have some hallucinations. But then when we are training our large visual memory model, it is essentially an all-in-one embedding model. What that means is that it turns all the videos and actually all the contexts including audios, text, actions, everything into the same embedding space.

[4:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=240s) And then we don't need any creativeness. We just want to turn all those different multimodal data into embeddings that can ultimately be losslessly reconstructed back to the original formats. This is how we train the model. So the way that we train the intelligence model and that we train the memories model are fundamentally different.

[4:20](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=260s) I see. But when you talk about memories you talk more about brain and when you talk about the model you talk more about databases. So in your understanding, the ultimate memory model - is it closer to the brain or is it closer to the database?

[4:34](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=274s) So the ultimate memory model is actually composed of two parts. It's actually a system. It's not an end-to-end model itself. Because think about how human memories works. Our memories is actually based on retrieval and reconstructions. When we see things, for example, I'm 29 years old. I have 29 years of visual memories. Whenever I see the world, I just index the world in real time and store all of this - electrical signals in my memories. And whenever you ask me a question, I retrieve from it.

[5:06](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=306s) So our human brain also operates similar to a system. And this system comprises two very important key components. One is this sort of indexing encoding process. Two is this retrieval process. So what we build is also these two key parts. We build state-of-the-art indexing models which is our large visual memory model, and then we also build this very AI-native, video-native, robust retrieval system. And on top of that, people can build different multimodal AI agents to enable different applications for embodied AI.

[5:40](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=340s) Do you feel that transformer kind of limits you at that? Do you look for other architectures? Is there research for you in there?

[5:47](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=347s) Yes, of course. So now we are actually looking at leveraging world models to build this embedding model and index models because what we found out is that the current transformers or current model architectures, it doesn't really understand the world. It doesn't really understand physics. It also doesn't really understand the sort of temporal contextual awareness.

[6:10](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=370s) As a very simple example, when we are reaching for something, getting something, and when we put things back - how does it know that for example this AirPods is still my AirPods? And also how does it know that when Ksenia changes an outfit or changes her hair color or even turns her back to me, but I can still figure out this is Ksenia? How to enable this contextual awareness on the temporal side, of the object, of the human - current transformer architecture doesn't support that.

[6:42](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=402s) So now we're actually developing a world model architecture to solve that. We haven't launched it but this is our current state-of-the-art model. And you develop it all in-house? You don't like, from scratch, your own models?

[6:53](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=413s) Yeah, we develop all of the models in-house because we are a bunch of research scientists coming from Meta, coming from all the different big lab backgrounds. So we have pretty long-standing experience of building models and especially building multimodal AI models ourselves.

[7:08](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=428s) I wonder how your process is going. Like, do you have a big picture of the brain and like "we solved this part of it, now we're moving forward" - how do you plan what to solve next?

[7:17](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=437s) I think that's a really good question. So we actually started by working on how human memory works, how human brain works. I started as a computational neuroscientist as a background and then I moved to computer vision. And also at the same time, if you search "AI memory human survey," you will see a survey made by us called "AI memory from human perspective" or something like that. We specifically looked at how human brain works, especially how human memory works, and then we sort of made this big blueprint of this huge memory system.

[7:52](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=472s) Now we actually sort of solved most of the part. And I think the next thing that is leading us is not from the technology - it's actually from the demand. Our company process is like our phase zero is that we study how human memory works, and then we made this technology inspired from human brain. And then we made this technology, we launched it. Since there, it's been like three to four months since we launched it.

[8:15](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=495s) We get a lot of different traction. We have over 100-200 inbounds and we have sent over 60-70 contracts already. And all of those inbounds really told us what is the market needs, what customers want. So from those needs, what we figured out: the human tracking is very important, human identification is important, object tracking is important, object identification is important, how to build a knowledge graph around humans is important. All of this is actually coming from the customer needs. And now our next phase is to build models around that part.

[8:50](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=530s) Well, this demand is showing that it's really needed. There are other companies that try to solve the memory problem, right? Like 12 Labs, Veloca. What is different? How do they see memory? What is the difference from you?

[9:05](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=545s) The whole market is really really new, right? Like visual intelligence - making AI to be able to see the world just like humans do. The whole market is super new. So there aren't too many players. A lot of people will think that these are similar players. But we are different players. There's a number of different players around us, but we all sort of probably start similarly. For example, we are all building embedding models, we are all building video indexing models, we are all building video search systems.

[9:34](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=574s) But we serve different purposes. I'm not exactly sure what is the big picture of 12 Labs, but at least from what I see, their ideal customer profile are more aligned with studios and publishers. And also like for example some other companies such as Co-Active, I think they're as far as what I can see from their websites, that is also aligned with their customer profile.

[9:56](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=596s) But what we want to build is not that. What we want to build is actually building a really human-like visual memory system to power the future embodied AI, especially humanoids to have human-like visual memories. And because we have different ultimate goals, the technology approach will be very different as well, and also the go-to-market will be quite different too.

[10:19](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=619s) For example, if that is our goal, we have to make all the models or at least all the indexing data storage on-device. That is why we partner with Qualcomm. Imagine the future humanoids - you can't really imagine that future humanoids will need Wi-Fi every second and they upload all their videos to the cloud. That will create a lot of burden on the bandwidth, so it's just not feasible.

[10:48](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=648s) It is critical to be able to make the model very very small and also customize where that embodied AI sort of scenarios so that they can run in low power consumption local chips so that they can process all the videos in real time. That is sort of our goal. And then again, how to remember people's faces and how to build the personal profile and how to build the knowledge graph between all the different users - those are all specifically designed to suit the future embodied AI use case.

[11:17](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=677s) Yeah, there are so many things to unfold here because first of all video is brutally computationally expensive. Then making it on-device is brutally hard. The whole disconnection from Wi-Fi thing is also not solved yet. So do you completely rethink compression and indexing problem for the model because it's just so hard for on-device?

[11:41](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=701s) Exactly. So humans have spent decades inventing compression technologies for videos. But all those compression technologies have encoding and decoding process. So those compression technologies are made for human. So then when the videos are encoded, they can be decoded into video formats that can be readable or understandable by humans.

[12:03](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=723s) But in the future, do we actually need to make those encoding process so that this can be decoded back, that they can be understood by human? No. We are now specifically designing compression and indexing process specifically for AI. We call this encode for machine, not encode for human - encode for machine. When we train models, we actually jointly train the compression and indexing algorithms together so that the ultimate output is directly processed by AI itself.

[12:32](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=752s) How do you decide what should be remembered and what should be forgotten? Is it programmed? Is it learned?

[12:38](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=758s) What to remember, what to forget? I think that is less of a fundamental question here. The fundamental question as I mentioned is this indexing problem and this retrieval problem. And in terms of what to forget or to remember, there's two approaches to that, and we actually are doing both approaches as well.

[12:56](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=776s) One approach is how the current text-based memory companies are building it. For example, Mem0, Letta, etc. People are building sort of agentic memory agents or context engineering to determine what to forget, what to remember - like to write different pipelines for episodic memory, procedural memory, etc. All of them are like context engineering. You manage different context in different modules. So that is one way to manage all those contexts. I think currently it's probably pretty efficient to manage in that way.

[13:26](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=806s) But then what we are thinking is actually in the long run, when we are training this indexing plus this world model-based indexing, all this should be automatically figured out - that it will have this emergence capability similar to how ChatGPT when it comes out has all the emergence of capability. It can automatically - it has this core of memorizing things.

[13:50](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=830s) For example, ChatGPT is essentially also a memory model of the whole internet data. It's essentially a reconstruction of the whole internet data. It remembers what is important but also it forgets what is less important. It remembers the logic but it also forgets some of the very fine details. And I think we should do the same as well. So this is sort of the direction we're going to. We haven't fully figured this out but I think that's the direction we're trying to build.

[14:14](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=854s) I think what is currently lacking is not only the technical approach but also the data itself. There isn't any data that exists on the internet or large-scale data on the internet that composes the human life, and especially the visual memories of human life labeled with all the actions in it. So we just lack that data. But I think as long as we have enough of that data, there will be a way to train a real world model-based large visual memory model that has this fully emergent capability on forgetting mechanisms.

[14:51](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=891s) So do you think what you're doing with the model will help with data, with the lack of data, and will provide it also to the physical AI companies? How do you solve that?

[15:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=900s) I think the model ultimately will serve a product, and then our model will serve products such as AI glasses, robotics. Once the model is powerful enough to make a good product - for example a good AI glasses product or a good AI wearable product or a good humanoid product - when people are starting to use it, there will be more and more data generated from those products.

[15:25](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=925s) So I think we are sort of in the first phase of making a good model and then making a good product. And once we have good products, then we have good quality of data. And once we have good quality of data, then people can contribute their data. Of course we need to be very careful about privacy, but then people can contribute their own data to this research project to make this real actual intelligence with memories. So I think our technical approach goes phase by phase.

[15:54](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=954s) That's a very big dream, massive idea that you're building. What are the real bottlenecks that you immediately face? Can you list them?

[16:02](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=962s) Some of the bottlenecks are of course when we are actually deploying our models onto device - there's always a trade-off between how good the model is and how much efficiency you want it to be. Even though our models can run on device without any problem, but then when you're actually trying to make a good product, you still need to sacrifice a lot of efficiencies because there's a lot of other models running on device.

[16:25](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=985s) What we need to do is to make the model smaller, even smaller, and also to make the model running faster and also more accurate. And also now when we get all this sort of actual needs - for example as I said, human identification, object tracking - this capability is something that we haven't trained into the models before.

[16:42](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1002s) Now what we're trying to do is to train those features into the models so that the models can have this contextual awareness of the human face, of the objects - not only the human face but the whole human - so they can recognize the human in the temporal perspective by not only the face but also the walking style, dressing style, or the way that they behave. So we are also training those features into the model itself.

[17:10](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1030s) Another example could be multimodal speaker diarization or speaker recognition. Previously speaker recognition could only be done by using audio. Now we are training this model to have multimodal speaker diarization so that it can recognize who is speaking what and when - not only just for audios but also from the visuals.

[17:28](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1048s) All of this is very important once we talk to our customers. We are now working with some of the top AI smart glasses companies, some of the top smart mobile phone companies, some of the top humanoid companies. And they all have very similar needs around human tracking, human identification, speaker recognition, speaker diarization, object tracking, object recognition. The needs are similar, and now we're trying to train those features into our models.

[17:53](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1073s) What you're building is a system that basically never forgets what it sees, right? Which is very helpful for many industries as you just mentioned, but it also can be very dangerous. So how do you design the system, the memory, that is useful but not creepy?

[18:12](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1092s) When we are training the models again, we are not training an intelligent model that can be creative, that can output something that... What about emergence possibilities of it?

[18:24](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1104s) The emergence possibility is only the things that you forget, or basically how it organizes different information, but it won't give you wrong information. Of course there could be some potential hallucination, but we are trying to minimize the hallucination to very little. We're not trying to replicate the whole human memory system because humans can forget. Do you need machines to forget? Sometimes yes, sometimes no. But machines totally have the capability not to forget.

[18:51](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1131s) So when we're building this indexing, all these models, the model itself is only transplanting all the videos and context, all the different contexts into another context format but in the latent space. So it doesn't generate new things. It doesn't generate new ideas - even for the emergence capabilities. It's in the perspective that the data formats will be arranged in different formats and structures in this latent space, but no new data or no new ideas will be generated.

[19:20](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1160s) Compared to intelligence, it is much safer. Whereas intelligence itself - what people are afraid of intelligence is because when there's more emergence capability coming out of intelligence, it can easily create new ideas, new things that you don't know. It's a dilemma - you want the intelligence to be really really powerful to even think beyond humanity, but then at the same time you want it to think in a safe way as well. So that itself is a dilemma.

[19:48](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1188s) But for our models, for memories models, it's purely about indexing - how to index all the contexts into another type of context but stored in the latent space that can be understood by AI. So it inherently doesn't have a lot of risk compared with intelligence models or language models.

[20:04](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1204s) So in a sense you're not trying to solve intelligence?

[20:07](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1207s) We're not trying to beat intelligence. We're trying to power intelligence to be an intelligence that really really understands the world just like humans do. For example, do you really need a super intelligence in your home to do laundry? Probably not. You probably need a general intelligence, but then really really understand you - really really understand you and your family and knows your hobbies and knows how to do laundry well according to your own lifestyle or working style.

[20:39](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1239s) When you imagine now you have a smart glass, AI glass, or potentially OpenAI is going to launch their hardware in the future as well - those hardware are all going to have cameras, microphones, they're going to record your days, they're going to be sort of contextually aware of your day and then help you on your productivity. Do you need them to be again super intelligent that can do everything, or do you need them to really really understand you, personalized to your own context?

[21:07](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1267s) I mean I might not need it, but it seems that if you're able to solve this problem, if you're able to solve the memory problem, there will always be someone who will be eager to build on it. The memory problem is so complicated, but when you solve it, building on it towards super intelligence or AI - I'm very confused about the terms because everyone has their own description for it - but that's what I guess makes it a little scary. If you solve this problem then people can use it.

[21:37](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1297s) That's true, that's true. All technology is a double-edged sword. So I think all the technologies when it comes to sort of exceed people's imagination, it naturally becomes very scary because it exceeds people's imagination. But yeah, when people are building things on top of it for the wrong purpose, it can be scary.

[22:00](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1320s) But what we want to solve is that we really want to make good for humanity, to make the future AI not just focus on the super intelligent self, but actually focus on humanity - that they can really develop their own personality because they have memories, they can really understand you so that they are super personalized, and then eventually they can even create a bond between you and them. So this is the goal that we're targeting.

[22:25](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1345s) I think ChatGPT's intelligence is more than enough for me, but what I need is to really understand what I'm doing on the day-to-day basis so I don't need to type in all the prompts, all the context, all the background, but then just say "hey, give me this" and then instantly understand what I want to do. So I think that is something I really want - to decrease the friction between how AI communicates with humans or how humans communicate with AI.

[22:52](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1372s) Yeah, you seem to be a very practical person. Do you ever think about AGI and super intelligence just because you're in this industry?

[22:59](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1379s) Yeah, I mean AGI and super intelligence - people, again as you said, have different definitions on super intelligent or AGI. Some people even think super intelligence is already here. It is true that in some areas AI is better than some humans. I mean, ChatGPT is better than me in terms of medical medicines, in terms of laws, etc. But it doesn't really mean that it is better than human overall.

[23:24](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1404s) For example, a pen can fly but it doesn't mean that it can replace me, right? There are just different tools. I treat them as tools. I don't treat them as another type of... You don't think they're sentient? That's right. So I don't tend to overthink about it too much. I want to solve the problems I want to solve.

[23:46](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1426s) AGI could be something that is coming in the next 10 to 20 years. But I don't think AGI is really coming anytime soon. As NVIDIA also mentioned in one of the podcasts previously, the scaling law is not working and we are going back to the research era. The real AGI - at least to most people's understanding of what should be AGI, or those AI in those sci-fi movies - will probably come in the next 10 to 20 years but not anytime soon.

[24:17](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1457s) I called it the era of "I don't know," which is super resourceful for researchers. Exactly. And it's the same for world models. World models is still in the research phase - people have different definitions of world models. World Labs defines world models as sort of 3D reasoning.

[24:33](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1473s) Because they're so big in different architectures in world models, do you look for their advice? Do you talk to them? Yeah, we have actually a number of similar people who give us a lot of technical advice because I think there aren't too many multimodal AI researchers in the world. So it's a very small circle and we do have some other people who give us advice constantly, and I actually learn a lot from those people as well.

[25:10](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1510s) Yeah, multimodal is definitely one of the new frontiers for the breakthrough. Yeah, it's very new. It's super super new.

[25:15](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1515s) Well, thank you very much. My last question is always about books and what is the book that formed you or maybe influenced you just recently?

[25:24](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1524s) I come from the research background. I come from the academia background. I always think that it's about technology push but not the market pull. So we started with the technology. We built our product around the technology. But I think that's right to do in the first place. But now once we launched our product, which is a wrapper of technology, and when we get a lot of customers, now I think what is really important is to listen to the customers, trying to figure out what they need.

[25:52](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1552s) We are building the product around the customer needs, not around the technology. Now we're building the technology from the customer needs too. So there was a book that I think - a lot of sales people probably have read about it, which all product managers too - which is called "The Mom Test."

[26:10](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1570s) It basically tells you how to really ask good questions to your target users about what are the things that they really need, what are the things that they really want, and the things that they will potentially pay for. So this is quite an important book for me in my current stage. Building a technology, we want to build a technology that people want. We don't want to build a technology that people don't want.

[26:34](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1594s) Sometimes people just don't know what they want. Yeah, that is true. The Mom Test actually gives some good examples around that. You actually don't ask "what do you want?" You ask "what is the way that you're currently doing this?" You don't tell them the solution straight away. It's solution-neutral. You ask them about their current ways of doing this, the current solutions, how they find about this.

[26:59](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1619s) You don't propose your solution first because if you propose your solution first, either they will say "it's cool" but they don't really use it, or they will say "why is a car better than a horse? Horse is fast enough." But ultimately I think it is building something that people want, people need, and really make a deep dive into what they really need by asking good questions. I think that is really important.

[27:23](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1643s) But also at the same time, I think what we are trying to be good at is how to execute as fast as we can, how to build a quick MVP as fast as we can, and then we can also present to the users - "hey, just use it, let me see how it works for you." I think overall The Mom Test was a good book that I recently read.

[27:38](https://www.youtube.com/watch?v=3ccDi4ZczFg&t=1658s) That's very interesting. I haven't heard about it. So, perfect suggestion. Thank you so much. It was very interesting. Yeah, no worries, Ksenia. Well, thank you so much for inviting me to this show.
