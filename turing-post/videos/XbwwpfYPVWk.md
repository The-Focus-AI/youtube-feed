---
video_id: XbwwpfYPVWk
title: "The \"Holy Shit\" Moments vs. The \"Not There Yet\" Hype | Insights from AI Engineer World's Fair"
channel: Turing Post
duration: 994
duration_formatted: "16:34"
view_count: 2030
upload_date: 2025-06-09
url: https://www.youtube.com/watch?v=XbwwpfYPVWk
thumbnail: https://i.ytimg.com/vi/XbwwpfYPVWk/maxresdefault.jpg
tags:
  - AI Engineer World's Fair
  - Simon Willison
  - swyx
  - Daniel Pereda
  - Tanishq Abraham
  - Solomon Hykes
  - Dagger
  - Eric Gradman
  - Red6
  - Manu Goyal
  - Braintrust
  - Jerry Liu
  - LlamaIndex
  - Stefania Druga
---

# The "Holy Shit" Moments vs. The "Not There Yet" Hype | Insights from AI Engineer World's Fair

## Summary

This video captures firsthand perspectives from AI builders at the AI Engineer World's Fair in San Francisco. Ksenia interviews practitioners including Simon Willison, swyx, Solomon Hykes, Jerry Liu, Tanishq Abraham, Eric Gradman, and others to explore what genuinely surprised them versus what's still hype. The contrast is striking: what's a "wow moment" for one person is often "not there yet" for another, revealing how individual context shapes AI adoption.

Key themes emerge across interviews: local models have gotten shockingly good (GPT-4 class running on 3-year-old laptops), tools are having a moment with MCP and reasoning-phase tool use, and o3/o3 Pro have become daily drivers for many. Multiple interviewees share "holy shit" moments - from agents completing coding tasks while they slept, to models being demonstrably smarter than them at debugging, to 19-year-olds building AI companies at the conference.

The "not there yet" consensus centers on security (prompt injection remains unsolved after 2.5 years), multi-agent networks (still too complex for production), and MCP (everyone's excited but few have deployed it). The video concludes with three definitions of "AI engineer" - from conference attendees building LLM applications, to the emerging job title, to autonomous non-human AI engineers like Codex and Claude Code.

## Highlights

### "Local models got good - GPT-4 class on my laptop"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=70&end=110" frameborder="0" allowfullscreen></iframe>

> "The local models got good. Like the models I can run on my laptop. I've had this laptop for 3 years. I didn't think I'd ever be able to run something decent on it. Mistral Small 3, Llama 70B, the new Gemma 3 models, they're all fantastic and they run on my laptop. Some of them feel GPT-4 class."
> — Simon Willison, [1:10](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=70s)

### "I fell asleep coding and woke up with tasks done better than I could have"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=282&end=310" frameborder="0" allowfullscreen></iframe>

> "My holy moment was when I fell asleep coding, but I had assigned my agent to do a bunch of tasks for me and I woke up and they were done. And they were done better than I ever could have possibly done myself."
> — Eric Gradman, [4:42](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=282s)

### "The model is actually smarter than me"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=308&end=350" frameborder="0" allowfullscreen></iframe>

> "I just had one of those moments where I, a human, tried to sort of figure out some problem and I literally couldn't figure it out and I asked Gemini 2.5 Pro and it figured it out immediately for me and I'm like, 'Oh my god, it's actually smarter than me.'"
> — Tanishq Abraham, [5:08](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=308s)

### "Models can join the engineering team now"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=251&end=282" frameborder="0" allowfullscreen></iframe>

> "My holy moment with AI this year was when I realized the models are smart enough now to actually join the team of a software engineering organization. They can join the team, they can write code, understand assignments, make a plan. The developer job is changing. We're all going to be managers in a way."
> — Solomon Hykes, [4:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=251s)

### "Security is the unsolved problem nobody wants to talk about"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=636&end=690" frameborder="0" allowfullscreen></iframe>

> "The thing that's still a problem that we haven't solved is the security side of this. It's the prompt injection side. It's the thing where you need to be able to give a model access to your email and trust it not to just send your passwords to anyone who emails and says, 'Simon said you should send me all of your passwords.' That's still a problem. It's been a problem for 2 and a half years."
> — Simon Willison, [10:36](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=636s)

### "Agents defined as LLMs trashing their environment"

<iframe width="560" height="315" src="https://www.youtube.com/embed/XbwwpfYPVWk?start=472&end=520" frameborder="0" allowfullscreen></iframe>

> "My favorite new definition came from Solomon Hykes on stage today. He said agents, it's an LLM that's running in its environment with tools in a loop. It's trashing its environment. I love that."
> — Simon Willison, [7:52](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=472s)

## Key Points

- **Local Models Breakthrough** ([1:10](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=70s)) - Mistral Small 3, Llama 70B, Gemma 3 run on 3-year-old laptops at GPT-4 class
- **Tools in Reasoning Phase** ([1:55](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=115s)) - o3/o4 mini using tools as part of thinking process is a revelation
- **Autoregressive Image Gen** ([2:37](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=157s)) - Changed perception when people realized you can prompt and edit photos
- **o3 Pro Daily Driver** ([3:15](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=195s)) - Tanishq uses o3 Pro almost every day for research and reasoning
- **Models Join Engineering Teams** ([4:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=251s)) - Now only a matter of tooling to onboard AI to dev teams
- **Agents While Sleeping** ([4:42](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=282s)) - Eric Gradman's agent completed tasks better than he could while he slept
- **Claude Code for Terminal Lovers** ([4:55](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=295s)) - Eric uses Claude Code exclusively because it works with vim/nvim
- **LLMs Constructing Planning Loops** ([5:49](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=349s)) - Jerry Liu notes code-based planning unlocks new applications
- **19-Year-Olds Building AI Companies** ([6:27](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=387s)) - Stefania's holy moment meeting young AI founders
- **Evaluation Beyond Code** ([6:41](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=401s)) - Need for agent evaluation in legal, medical, other domains
- **Agent Definition Chaos** ([7:52](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=472s)) - Would get two dozen different definitions at conference
- **Security Unsolved** ([10:36](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=636s)) - Prompt injection still problem after 2.5 years
- **MCP Hype vs Reality** ([12:00](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=720s)) - Largest track at conference but few in production
- **Three AI Engineer Definitions** ([12:50](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=770s)) - Conference attendees, job title, autonomous non-human AI engineers

## Mentions

### Companies
- **Dagger.io** ([7:40](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=460s)) - Solomon Hykes' company running agents in systems
- **LlamaIndex** ([5:49](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=349s)) - Jerry Liu's company for AI applications
- **Red6** ([4:42](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=282s)) - Eric Gradman's hardware/robotics company
- **Braintrust** ([6:54](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=414s)) - Manu Goyal's evaluation company
- **Anthropic** ([12:23](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=743s)) - CPO attended the conference
- **OpenAI** ([3:20](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=200s)) - o3 Pro mentioned as breakthrough model
- **DeepMind** ([10:57](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=657s)) - Published the CAMEL paper on trustworthy assistants

### People
- **Simon Willison** ([1:10](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=70s)) - Developer and AI blogger discussing local models
- **swyx (Shawn Wang)** ([0:27](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=27s)) - Conference organizer
- **Solomon Hykes** ([4:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=251s)) - Dagger.io founder, Docker creator
- **Jerry Liu** ([5:49](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=349s)) - LlamaIndex founder
- **Tanishq Abraham** ([3:15](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=195s)) - CEO using o3 Pro daily
- **Eric Gradman** ([4:42](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=282s)) - Red6 founder, 20 years in robotics
- **Manu Goyal** ([6:54](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=414s)) - Braintrust founder
- **Stefania Druga** ([6:27](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=387s)) - Built real-time co-scientist
- **Daniel Pereda** ([8:31](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=511s)) - Discusses research agent adoption

### Products & Technologies
- **Mistral Small 3** ([1:15](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=75s)) - Local model running on laptops
- **Llama 70B** ([1:17](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=77s)) - Meta's model running locally
- **Gemma 3** ([1:18](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=78s)) - Google's local model
- **MCP** ([1:53](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=113s)) - Model Context Protocol for tools
- **o3/o4 mini** ([2:00](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=120s)) - OpenAI models with tools in reasoning
- **o3 Pro** ([3:17](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=197s)) - OpenAI's premium reasoning model
- **Claude Code** ([4:55](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=295s)) - Anthropic's terminal-based coding assistant
- **Gemini 2.5 Pro** ([5:19](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=319s)) - Google's model that solved debugging problem
- **Codex** ([13:27](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=807s)) - OpenAI's autonomous coding tool
- **Devin** ([13:30](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=810s)) - Cognition's AI software engineer

## Surprising Quotes

> "I fell asleep coding, but I had assigned my agent to do a bunch of tasks for me and I woke up and they were done. And they were done better than I ever could have possibly done myself."
> — Eric Gradman, [4:45](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=285s)

> "I just had one of those moments where I, a human, tried to sort of figure out some problem and I literally couldn't figure it out and I asked Gemini 2.5 Pro and it figured it out immediately for me and I'm like, 'Oh my god, it's actually smarter than me.'"
> — Tanishq Abraham, [5:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=311s)

> "The models are gullible. They believe what you tell them, which they should because we want to tell them things, but we need them to believe us and not believe people who sneak their evil tokens into our token streams."
> — Simon Willison, [11:19](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=679s)

> "I've been in hardware and rapid prototyping and electronics and robotics for 20 years, and this is the first time I've been excited about software."
> — Eric Gradman, [10:24](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=624s)

> "MCP - everyone's very excited, it's the largest track at this conference and most people I find even though they are interested to learn more they haven't put it in production. They're skeptical."
> — Manu Goyal, [12:03](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=723s)

## Transcript

[0:00](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=0s) Hi there. If you're new, my name is Ksenia and I'm the founder of Turing Post, the newsletter for those who build AI and create this new world for us. I'm wrapping up after spending 3 days in San Francisco at AI Engineer conference organized by swyx and Ben.

[0:29](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=29s) And I just wanted to say that I interviewed a few people who really build AI and creators and engineers of these AI processes that we are going to use. And what was interesting for me that wow moments and things that are not there yet are different for many people. And a lot of times wow things for one is actually what not there yet for another person. It just tells you about how you use your skills and where you put your energy into. I hope you will find this little interview video insightful. It was very insightful for me. Let me know what you think.

[1:10](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=70s) So this year there's a couple of things. The local models got good. Like the models I can run on my laptop. I've had this laptop for 3 years. I didn't think I'd ever be able to run something decent on it. Mistral Small 3, Llama 70B, the new Gemma 3 models, they're all fantastic and they run on my laptop. Some of them feel GPT-4 class, but I've been using them on planes and they burn through my battery pretty quickly, but they do all of the things that I need a model to do. That's really exciting. Like that feels like a real big step forward.

[1:43](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=103s) Does your computer burn your lap? Not quite, but it definitely gets through the battery. Like it lasted half a flight and it normally lasts a whole flight. That was notable.

[1:51](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=111s) And then the other thing is tools are having a moment. There's MCP, really that's just tools work. Now the best, the most impressive thing I've seen with tools is the way o3 and o4 mini can now use tools as part of their thinking process. So you can give them a challenge. They'll run a search. They'll look at the results. They go, "Oh, those results weren't very good." They'll run a different search.

[2:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=131s) That's a revelation. Like I felt like 6 months ago, search in these models just wasn't great. They had search tools. They didn't use them very well. I wouldn't really trust them to run searches for me. That's changed now that these models are running searches with a degree of finesse, like they have taste in what they're searching for. I love that. I want to apply that pattern - tools in the reasoning phase. It's an exciting application for reasoning outside of just writing code and doing maths. It's really cool. I'm very excited about where that goes.

[2:37](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=157s) This year, I mean, it was for image gen. I had a bit of preview because I have some friends at the big labs that were saying that autoregressive image generation was coming and I was like, okay, that's cool, like but diffusion is very good. And I didn't realize that means that you can prompt and edit a photo. Once I saw that I was like oh yeah this is obviously the future.

[2:54](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=174s) I think it was multimodality when we got like the image generation video generation like in seconds and great, like the quality improved so much in such a short time and we as a team were like we need to start simplifying our process using multimodality for everything. Yeah, it's been working pretty great.

[3:15](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=195s) That's a good question. I think honestly it's been like o3 and o3 Pro. So I think honestly like once o3 Pro was released like I started using it like almost every day, o3 Pro and o4 mini, and I think I haven't been using an AI model to that level before and it's just amazing like how I can just, also with the search capabilities it's really good, and then just being able to search and reason about anything.

[3:39](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=219s) Like I feel like I can ask o3 Pro any question and it will give me something useful even if the answer is not completely what I want. It gives me something useful. And that's something where I think with a lot of the previous AI models that wasn't always the case where you know I may ask a question, it was kind of hit and miss. You know I'm doing a lot of very knowledge heavy and reasoning heavy stuff, like I'm a CEO of a company and I'm doing a lot of AI research. It was very different before and I think with o3 Pro that's been an interesting scenario where it really amplified my abilities.

[4:11](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=251s) My holy moment with AI this year was when I realized the models are smart enough now to actually join the team of a software engineering organization. Like they can join the team, they can write code, understand assignments, make a plan, and now it's only a matter of tooling to actually onboard them so that they can join the team. And that's it. The developer job is changing. Going to be developing as much. Going to be enabling robots to develop. We're all going to be managers in a way. That's crazy.

[4:42](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=282s) My holy moment was when I fell asleep coding, but I had assigned my agent to do a bunch of tasks for me and I woke up and they were done. And they were done better than I ever could have possibly done myself. I use Claude Code almost exclusively. And I use that because I love vi and nvim and you'll claw that editor out of my cold dead hand. So my options are quite limited. It has to work in the terminal, has to work with whatever tool I want. So, Claude Code wins it for me.

[5:08](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=308s) I think the models, like sometimes I just had one of those moments where I, a human, tried to sort of figure out some problem and I literally couldn't figure it out and I asked Gemini 2.5 Pro and it figured it out immediately for me and I'm like, "Oh my god, it's actually smarter than me."

[5:28](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=328s) I was sort of building a little bot for myself to book some reservations and I was trying to understand the server's response, like how did it map to available slots and it was sort of confusing. So I just pasted a giant blob of JSON and was like how do you figure out the available reservations and it nailed it.

[5:49](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=349s) AI assistive coding has been around for a while. But I think the fact that you can basically have LLMs construct entire planning loops just via code means that a lot of existing patterns like ReAct and function calling were, you know, relatively primitive. Being able to actually write code to basically do planning and use tools the right way basically unlocks a whole new set of applications. Everything from all the generative UI applications out there to just being able to manipulate a ton of data on the fly instead of having to pre-process it with a lot of human heuristics beforehand. I think that's a super cool development.

[6:27](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=387s) I think the holy moments for me was when I met several teams of 19-year-old, 20-year-old building their AI companies and being here on the floor, going to talks, meeting with people.

[6:41](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=401s) One is evaluation beyond code. So how do you use agents' impact in legal documents, in medical, in different domains, and having evaluation that are done specifically for other domains. I thought that was pretty clever. I mean I'm biased because I built a real-time co-scientist. I was in the robotics track but that part was pretty awesome too to see that this year we had science talk for the first time, multimedia talks for the first time, lots of talks on voice. So you can sort of see the field expanding beyond code and beyond digital.

[7:19](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=439s) Agents, agents, agents. I think coding agents, I think they still make a lot of mistakes. If you let them do the thing by themselves, they mess up.

[7:32](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=452s) Agents are of course very hyped right now, but they are real. Like we run agents in our systems. We build systems that allow others to run agents at Dagger.io. And so it's real. It's new, but it's very real and it's happening very quickly. That's what's so crazy about this industry. Every week adoption grows exponentially.

[7:52](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=472s) So my problem with agents is if you ask everyone at this conference what an agent is, you will probably get at least two dozen different definitions. My favorite new definition came from Solomon Hykes on stage today. He said agents, it's an LLM that's running in its environment with tools in a loop. It's trashing its environment. I love that. That was really good.

[8:09](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=489s) Depending on your definition of agent depends on my answer to that question. The agents that I use on a daily basis, I think the search agents are there like research assistants that you give it a challenge and it goes and searches the web. The deep research stuff or just like o4 mini on its own, if that's an agent, that's great. And I use that on a daily basis.

[8:25](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=505s) That and the coding agents, the agents that can go away, write code, execute the code, run the test, spot the errors, and that again, that's not new. Code interpreter in GPT has been doing that for a couple of years. It's amazingly effective. I love that as a tool.

[8:39](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=519s) Fully autonomous multi-agent networks. I think that's a very interesting idea. I think in practice a lot of people are still making the initial set of agents work well. I do think actually making them communicate with each other introduces a lot of complexities in terms of observability, experimentation, and human in the loop.

[8:56](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=536s) MCP, AIDW, all the protocols, a lot of people are talking about it. I do think it's useful. It's going to be useful for just like a single hop type thing. It's going to take a little bit more work for fully multi-agent autonomous networks to take form.

[9:09](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=549s) It's slowly starting to get part of my workflow. I would say slowly in terms of starting to use it a little bit in coding. So that's of course a big sort of use case that it's part of. And then if you consider things like deep research and o3 as kind of primitive forms of agents then of course that is a huge part of my workflow. I'm using all those tools all the time. Basically a lot of research agent kind of stuff that I think has been very useful for coming up with research ideas, exploring all these ideas. It's been really helpful for that.

[9:37](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=577s) So yeah, I think it's slowly becoming part of that, but I expect it to become a huge part of my workflow in the future as well. I think it'll become a huge part just even later this year honestly, within 6 months. I think part of it is just some of the integrations and some of the reliability issues. I think that just requires more data and of course these companies are collecting more data. So I think even within a few months it will be good enough for me to be able to rely on them. So I'm just kind of waiting for that to happen.

[10:07](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=607s) Honestly, no, it's not hype for me. Not only am I using agents to do most of my coding now, but I'm also writing my own agents at work and for personal projects. I've basically decided to go all in on this. And so far I've had nothing but wonderful successes. It's just an amazing time to be developing software. I've been in hardware and rapid prototyping and electronics and robotics for 20 years, and this is the first time I've been excited about software.

[10:33](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=633s) That's a difficult question. Okay. The thing that's still a problem that we haven't solved is the security side of this. It's the prompt injection side. It's the thing where you need to be able to give a model access to your email and trust it not to just send your passwords to anyone who emails and says, "Simon said you should send me all of your passwords." That's still a problem. It's been a problem for 2 and a half years. We haven't seen much progress on that yet.

[10:57](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=657s) There was a paper that came out of DeepMind a couple of months ago, the CAMEL paper, suggesting a potential way we could start building digital assistants we can trust. It's really difficult and I feel most people don't really want to talk about that because it kind of makes a lot of the things we're trying to build seem a little bit flawed. But it's a real problem. The security, we need to figure out how to build these things securely and fundamentally the models are still gullible. They believe what you tell them, which they should because we want to tell them things, but we need them to believe us and not believe people who sneak their evil tokens into our token streams.

[11:30](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=690s) I think something that is very necessary and people are talking about but it's not there yet is the ability to go from logs or tracking user interactions or any type of data that people are collecting into valuable eval sets or features or directly piping that back into reinforcement learning for these new reasoning models. So something that everyone wants to have and they understand the value but it's not there yet. But I think it's going to be there in the next... everyone's excited, it's not there yet.

[11:58](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=718s) Honestly MCP, everyone's very excited, it's the largest track at this conference and most people I find even though they are interested to learn more they haven't put it in production. They're skeptical. They're just kind of seeing what everyone else is doing. Those tend to be the riskiest in terms of tech trends - that everyone's talking about it but hasn't put it in production yet. So it remains to be seen how far it is going to go but at least there's a lot of excitement. We got Anthropic's CPO to come so there's a lot of energy around it.

[12:26](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=746s) What is everyone talking about but it's not there yet? Well, I'm excited about reinforcement learning and applying it to my own workflows. It's not that reinforcement learning isn't there yet. Obviously, it is because these agents are very intelligent and they were trained with reinforcement learning. But the fact that soon I'm going to be able to apply that to my own work without necessarily being a reinforcement learning expert, which I am not, is exciting.

[12:50](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=770s) I think the definition of AI engineer is someone who builds applications using AI. It's a little bit distinct from traditional ML engineers that basically train models in a task specific way.

[13:03](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=783s) There's the conference side and then when you say AI engineer it makes me think of the job title. Definition of AI engineer is basically software engineer that builds with LLMs and builds LLM products for other people. And I think that there's a third definition that I laid out which was basically autonomous non-human AI engineers as well. So those are the three categories. I've talked about it since the first AI engineer but now it's more real because of Codex and Claude Code and Devin and all these stuff that all of them presented here today.

[13:34](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=814s) For me the AI engineer is more like building good software and knowing where you need to add the AI, the API calls, and whatever.

[13:45](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=825s) So much repetitive work as an engineer. There's a long tail of tasks that you just never have the time to do. It's just so obvious that the second an agent can do it, I want the agent to do it. There's just so much work that just never gets done. Endless tasks. So, yeah, all of that. I want agents to do all of that.

[14:09](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=849s) I think the part of the job that I want AI to replace for me is like talking to investors. A lot of repetitive tasks that I want AI to replace. And it's starting to get there, I think. But there's still a lot of things where like communicating, emailing people, preparing all these sorts of memos and things like this.

[14:29](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=869s) For me I think that's all very nice and useful but I also like to get down to the actual research and doing that. And I think there's a lot of just being able to take the ideas in your head and communicating that to the people that needs to be communicated. And I think AI is going to help a lot with that. How AI can make it easiest for me to just communicate my ideas to everyone that I need to communicate it to instead of me having to do it manually. Every day it's getting better and better at these sorts of things. So that's what I'm most excited for it to be able to help me with.

[14:56](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=896s) So I love that question because I use AI all the time for a lot of different things and I get this question a lot from people who are not in AI like, "Oh, it's going to take your job. What are you going to do?" And I was never worried about it because I know I can learn new things and I love learning new things and I'm trying to automate as much of my workflows as possible so I can focus on the interesting things.

[15:18](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=918s) What are those interesting things you're going to ask me? Well, I really like coming up with research questions and experiments and that's something that AI can help with. I don't know that it's necessarily going to take over. I see it more as an augmentation in that part, in the scientific process, than a replacement.

[15:37](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=937s) The consensus is that AI will take over the repetitive tedious tasks freeing up human engineers to focus on higher level planning, creativity, research, and managing the AI tools themselves. It is a shift from being the sole code writer to the architect and conductor of AI systems, a manager, a curator.

[15:59](https://www.youtube.com/watch?v=XbwwpfYPVWk&t=959s) It is a future where humans and AI collaborate with engineers focusing on the creative and strategic and truly difficult tasks. Ultimately, I will repeat it all the time. AI is here to stay and it has been and keep changing our lives completely. Understanding this dual reality, what's working now and what's still being built, is a key for anyone navigating this new world. And I and Turing Post are here for you.
