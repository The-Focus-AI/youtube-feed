---
video_id: 5P8kgRcLF5Q
title: "Can We Control AI That Controls Itself? Anneka Gupta from Rubrik on AI Resilience"
channel: Turing Post
duration: 1618
duration_formatted: "26:58"
view_count: 354
upload_date: 2024-11-15
url: https://www.youtube.com/watch?v=5P8kgRcLF5Q
thumbnail: https://i.ytimg.com/vi_webp/5P8kgRcLF5Q/maxresdefault.webp
tags:
  - Artificial Intelligence
  - AI Agents
  - Cybersecurity
  - AI Governance
  - Enterprise AI
  - AI Resilience
  - Rubrik
  - Future of Security
  - Machine Learning
  - AGI
---

# Can We Control AI That Controls Itself? Anneka Gupta from Rubrik on AI Resilience

## Summary

In this episode of Inference by Turing Post, host Ksenia Se interviews Anneka Gupta, Chief Product Officer at Rubrik, about the challenges of security and governance in the age of autonomous AI agents. The conversation centers on the theme of "designing for uncertainty" - exploring how organizations must fundamentally rethink security when AI systems can cause failures on their own rather than just responding to them.

Gupta outlines a three-pillar framework for AI resilience: visibility (monitoring what agents are doing), governance (setting guardrails for agent behavior), and reversibility (recovering when agents inevitably make mistakes). She emphasizes that AI agents are "the human problem on steroids" - they will be more numerous, faster, and more unpredictable than human operators, making comprehensive logging and monitoring essential. The discussion also covers the mental shift required for product teams moving from deterministic code to outcome-driven AI development.

The conversation concludes with thoughts on AGI, where Gupta views it as an inevitability that will accelerate the existing cat-and-mouse game between attackers and defenders. She shares her love of science fiction, particularly Isaac Asimov's philosophical approach to robotics, which influenced her path to becoming a technology builder focused on imagining and solving future problems.

## Highlights

### "AI Agents Are the Human Problem on Steroids"

<iframe width="560" height="315" src="https://www.youtube.com/embed/5P8kgRcLF5Q?start=112&end=165" frameborder="0" allowfullscreen></iframe>

> "AI agents are on steroids. They're going to be many more of them. They're going to be taking actions a lot faster and again in a very non-deterministic and maybe unpredictable way."
> — Anneka Gupta, [2:11](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=131s)

### "The Three Pillars of AI Resilience"

<iframe width="560" height="315" src="https://www.youtube.com/embed/5P8kgRcLF5Q?start=158&end=225" frameborder="0" allowfullscreen></iframe>

> "First step is visibility. So, how can you actually monitor, get visibility into what is happening? What AI agents do you have running? What are the actions they're taking? What applications do they have access to?"
> — Anneka Gupta, [2:54](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=174s)

### "Log Everything Your AI Agent Does"

<iframe width="560" height="315" src="https://www.youtube.com/embed/5P8kgRcLF5Q?start=401&end=455" frameborder="0" allowfullscreen></iframe>

> "Log every single thing that your AI agent is doing. And if you have that... you can go back, you can assess for quality challenges, like is this actually producing the results that I think it should be producing?"
> — Anneka Gupta, [6:41](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=401s)

### "People Who Fear AI Usually Have Never Used It"

<iframe width="560" height="315" src="https://www.youtube.com/embed/5P8kgRcLF5Q?start=887&end=940" frameborder="0" allowfullscreen></iframe>

> "The majority of times I hear from people that they are afraid of AI is when they never used it."
> — Anneka Gupta, [14:51](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=891s)

### "AGI Could Give Foreign Governments Very Broad Outcomes"

<iframe width="560" height="315" src="https://www.youtube.com/embed/5P8kgRcLF5Q?start=1399&end=1455" frameborder="0" allowfullscreen></iframe>

> "With AGI, you could give a very broad outcome. So you could have from a national security perspective, you could have a foreign government that's trying to wage war on another country give very broad outcomes for what it's looking for and AGI will just go figure out how to execute on those outcomes."
> — Anneka Gupta, [23:19](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1399s)

## Key Points

- **AI Creates Maximum Uncertainty** ([1:15](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=75s)) - Nothing creates more uncertainty than AI today; the non-deterministic nature means agents execute outcomes without regard for the specific changes they'll make.

- **AI Agents on Steroids** ([2:09](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=129s)) - AI agents amplify human problems - they're more numerous, faster, and can cause unintended downtime by actions like dropping database tables.

- **Three Pillars of AI Resilience** ([2:54](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=174s)) - First visibility (monitoring agents), second governance (guardrails on actions and outcomes), third reversibility (recovering from inevitable mistakes).

- **Five Rules for Deploying Agents** ([4:21](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=261s)) - Define outcomes and use cases, choose your tech stack, build observability, manage agents with evolving rules, and have a recovery playbook.

- **Log Everything** ([6:41](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=401s)) - The most practical advice is to log every action your AI agent takes, allowing you to assess quality and security posture retrospectively.

- **MCP Server Monitoring** ([7:56](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=476s)) - Need to track calls between agents and models, understand MCP server actions, and string together the complete timeline across multiple systems.

- **Agent Action Timeline** ([9:31](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=571s)) - Just like security needs attack timelines, AI governance needs agent action timelines - too many signals without correlation has been security's historical failure.

- **Mental Shift Required** ([10:56](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=656s)) - The biggest barrier is the mental shift from deterministic code to outcome-driven experimentation with clearly defined evals.

- **Root Cause Analysis is Hard** ([12:31](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=751s)) - Traditional debugging doesn't apply - determining good vs bad output requires human expertise that's difficult to codify.

- **70-90% Stuck in Prototyping** ([17:03](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1023s)) - Most large enterprise AI projects never make it to production due to efficacy and governance concerns.

- **Experiment Constantly** ([13:41](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=821s)) - Building AI intuition requires hands-on daily experimentation; people who fear AI usually have never used it.

- **Rubrik Agent Cloud** ([19:42](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1182s)) - Rubrik's product provides visibility into running agents, blocks unwanted actions through rules, and enables quick reversal of unintended changes.

- **AGI as Inevitability** ([20:51](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1251s)) - AGI is a matter of when, not if - it will accelerate both attacks (broader outcomes for hackers) and defenses.

- **Reversibility Challenge** ([22:41](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1361s)) - AI agents can touch hundreds or thousands of systems, making monitoring, signal detection, and reversal extremely challenging.

- **Dogfooding AI Products** ([19:03](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1143s)) - Organizations building AI products should use them internally first - you learn enormously from trying solutions yourself.

## Mentions

### Companies
- **Rubrik** ([0:34](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=34s)) - Data security company where Anneka Gupta serves as Chief Product Officer, focused on AI resilience
- **Rubrik Agent Cloud** ([19:42](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1182s)) - Rubrik's product for enterprise AI agent visibility, governance, and reversibility

### Products & Technologies
- **MCP Server** ([8:12](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=492s)) - Model Context Protocol server that AI agents use to take actions, connected to email, Slack, CRM systems
- **AI Agents** ([1:40](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=100s)) - Autonomous systems programmed to execute outcomes without predetermined steps
- **AGI** ([20:51](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1251s)) - Artificial General Intelligence discussed as an inevitable future development

### People
- **Anneka Gupta** ([0:32](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=32s)) - Chief Product Officer at Rubrik, interview guest
- **Ksenia Se** ([0:24](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=24s)) - Host of Inference by Turing Post
- **Isaac Asimov** ([25:03](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1503s)) - Science fiction author whose I, Robot series influenced Gupta's thinking about AI ethics

## Surprising Quotes

> "AI agents are on steroids. They're going to be many more of them. They're going to be taking actions a lot faster and again in a very non-deterministic and maybe unpredictable way."
> — [2:11](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=131s)

> "The majority of times I hear from people that they are afraid of AI is when they never used it."
> — [14:51](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=891s)

> "Something like somewhere between 70 and 90% depending on whose data you look at [of AI projects] get stuck in prototyping and never make it to production."
> — [17:03](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1023s)

> "With AGI, you could give a very broad outcome. So you could have from a national security perspective, you could have a foreign government that's trying to wage war on another country give very broad outcomes for what it's looking for and AGI will just go figure out how to execute on those outcomes."
> — [23:19](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1399s)

> "Will we have AGI where it's like not even human directed? Now you're starting to talk about total science fiction world, which like is certainly possible, but then we might have a lot bigger problems on our hands than what we're talking about here today."
> — [24:04](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1444s)

## Transcript

[0:00](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=0s) I'm not sure that there's anything that creates more uncertainty than AI. The challenge with AI is that it's non-deterministic. Log every single thing that your AI agent is doing. I tell my team to just experiment all the time.

[0:24](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=24s) Hello everyone, welcome to Inference by Turing Post. My name is Ksenia and joining me today is Anneka Gupta, chief product officer at Rubrik and her team is rethinking what security means in the age of autonomous agentic systems. Hi Anneka and welcome. Thank you. Thanks for having me.

[0:43](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=43s) When I was thinking about our conversation today, I wanted to set a theme and the theme is designing for uncertainty. So let me form it into a question. We used to think of security more of after a crash, right? Restore, patch, move on. And now we're in a different situation. We're in the world before a crash where AI systems, you know, can cause one on its own. So in this world, what does failure even mean when the system can create it by itself?

[1:15](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=75s) Yeah. I mean, you talk about uncertainty. I'm not sure that there's anything that creates more uncertainty than AI today. Obviously there's uncertainty about what the future of AI looks like from a cyber security front there's a lot of questions about you know what are the ways AI is already being used by cyber attackers what does that mean but then when we think about organizations themselves and their adoption of AI the challenge with AI is that it's non-deterministic and the challenge with AI agents is that they are programmed to go execute an outcome without regard to what are the ways that they're going to actually the changes that they're actually going to make to achieve that outcome.

[1:52](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=112s) Um, and so when you talk about like the negative consequences, what are the risks that we need to be thinking about proactively as we're deploying AI agents more at scale? One is how do I make sure that my AI agents don't create unintended downtime for my applications? Humans, we're worried about humans creating unintended downtime for applications. AI agents are on steroids. They're going to be there going to be many more of them. They're going to be taking actions a lot faster and again in a very non-deterministic and maybe unpredictable way.

[2:21](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=141s) And that can lead to downtime of your application if they do something like drop a database table or make some change that's really difficult to reverse. So I think that's like part of the unpredictability that is inserted even more so with AI agents is kind of taking the human problem and putting it on steroids.

[2:38](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=158s) I just want to see your thinking process when you think about this uncertainty and in failures. What does it mean? How do you define control error and recovery in this new era? Yeah, I think when we're talking about solving for these problems, um the first step is visibility. So, how can you actually monitor, get visibility into what is happening? What AI agents do you have running? What are the actions they're taking? What applications do they have access to? Like having that visibility is step one of helping solve for this problem.

[3:11](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=191s) And then the next step is well, how do you govern then what these AI agents are doing? What are the guard rails that you're putting on these agents to both control for what the are the actions that they can take and what they can access as well as like what are the guard rails around the outcomes and the quality of those outcomes as well. And then the third piece is that you know with anything you have to anticipate that something is going to go wrong. You're not going to set the guardrails perfectly and be able to anticipate every situation ahead of time.

[3:42](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=222s) And so what do you do when an AI agent inevitably makes a mistake and how do you actually end up reversing those mistakes? So that's the thinking that we're seeing and as we're talking to our customers and and our own thinking is evolving around what do you really need to be solving for where the problems and jobs to be done in order to have more resilience for your organization even while accelerating your AI adoption.

[4:06](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=246s) I want to make it really practical for the viewers and and readers. Can you give let's say a simple set of rules three five rules that every organization needs to think about when they you know deploy agents.

[4:21](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=261s) Yeah. I think step number one is really figuring out first of all what is the outcome and what are you trying to solve for with your agents that is just you know practically speaking we can talk about AI agents but they need to be solving specific use cases and having an idea what is that use case what's the ROI that you're trying to drive what's the productivity. The second question I would ask is what are the tools and infrastructures you're going to use to set up that agent there's a lot of different options out there about different kinds of tools and technology. You know, it depends on if you have an engineer that's building an agent or a non-technical user. So, really understanding what that tech stack is going to look like.

[4:58](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=298s) If you're an individual or if you're an organization, how does that fit into then the overall technology strategy of your organization? I think step number three then starts getting into kind of this proactive measurement of or but proactive um work to make sure that your AI agents are set up properly. How are you going to build that visibility into what AI agents are running? what are they doing in the system? What do they have access to? Who within your organization is using these AI agents? Like that kind of observability piece and monitoring piece is really important.

[5:29](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=329s) And then I think the questions after that are what are you going to do to manage your agents on an ongoing basis. So how are you going to go back and make sure you're looking at what they're actually doing and either and then creating new rules. You're going to have some rules that you're going to create around what sanctioned and unsanctioned behaviors. You have to create those rules. and then how do you look at those on an ongoing basis and update those rules based on what you're actually seeing and that way continuously improving the way that your AI agents are running to make sure that you know if there's they do end up doing an action that you don't like how do you prevent that in the future.

[6:04](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=364s) And then the last but not least piece is like okay what are you going to do what is the technology playbook process you're going to do when your AI agents inevitably make a mistake how are you going to recover from that.

[6:13](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=373s) Well you talk a lot about observability visibility interpretability in machine learning that was one of the hardest problems. Now with uncertainty what are the main bottlenecks how do you deal with them? There is a lot of uncertainty and I think there is again an even greater amount of unpredictability than in the traditional machine learning world that we're seeing with AI agents.

[6:41](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=401s) A lot of what you can do very practically is log every single thing that your AI agent is doing. And if you have that and you're not going to be perfect on day one of saying, "Hey, I've set up my rules and everything properly." But if you're logging everything that's happening, you can go back, you can assess for quality challenges, like is this actually producing the results that I think it should be producing? And you can look at the the kind of security posture, just the overall posture of your AI agents and say, is this actually are these activities that I want to allow or not?

[7:11](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=431s) And at the end of the day, it's like I think a very important thing is saying like, hey, how am I building these guard rails on a continuous basis of what's allowed and what's not. And that's both from again a quality and getting the ROI that you're looking for for your use cases, whether that's external or internal to your own employees too. And the second piece is being able to monitor this in case your AI agents do end up doing something malicious because maybe you have an attacker that gets into your organization and starts using them maliciously or you have unintended consequences of an AI agent taking actions that actually increases your data exposure risk or other kinds of security risks.

[7:49](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=469s) Is it some sort of an additional agent that does this logging? How do you deal with that? So I think like one is looking at like what are specifically like what are all so there's a call that's happening between the agent and the model itself right so you're making some call to a model the model is coming back with a set of next steps to go do that agent is typically going to go hit like some kind of MCP server that MCP server is also going to take some actions and understanding what's actually happening behind the scenes there is super important as well.

[8:20](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=500s) And then again this can happen in cycles you could have multiple you have agents talking agent you could have a lot going on, but if you actually know all of those model calls that are happening, both what's being the input that's being put in, the prompt that's being put in, and the result that's coming back, and as well as like what's happening behind the scenes with the MCP server when a specific call is being made, and you're able to string all of that together, then that is really going to give you a very solid picture of what's happening behind the scenes.

[8:48](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=528s) Now, this is easier said than done, right? Because often this is not one system. These are multiple systems. Your MCP server might be connected to your email. It might be connected to your Slack. It might be connected to your CRM. And all of these systems are going to be doing various things behind the scenes. So you have to be able to create a holistic view that's really understanding what's happening at every layer which requires multiple entry points in terms of creating that monitoring multiple calls to different kinds of APIs that are going to have that logging and then being able to go back and construct from all of these various logs what is actually the path that was taken what actually happened in sequence and how do you build that timeline.

[9:31](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=571s) So it's not an easy problem to solve and I think that over time though it's like we will continue to evolve like all the third party tools will continue to evolve to provide that visibility and then it's incumbent upon various vendors to really be able to string that together into a way that's actually actionable for a business or technical user because we can't expect people to wade through 50 different logs and try to correlate these things on their own. That's actually where we've failed quite a bit in security historically is like it's just too many signals and you can't correlate them together into an attack timeline and it's similar with agents like you need to understand the agent action timeline.

[10:07](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=607s) But you can build an agent for that. You can build an agent for that but it you have to get access to the underlying data of course yeah I totally believe that building the timeline is easier today than ever before because of agentic AI but getting access to the data and being able to do that well and interpret the logs there's still work that needs to be done. It's not like if you just put an out of the box agent on top of logs is going to magically give you an answer. All of this is iterative. All of this takes an understanding fundamental understanding somewhere of the underlying data to be able to pull that together.

[10:40](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=640s) If we talk about people who building this and your experience is very varied from SAS era now to the autonomous era. What mental shift it requires on the engineers from the product teams to think and solve these questions? I love this question because often what I talk about even internally is that the biggest barrier is the mental shift that everyone needs to make in this era because it's a very different yes AI is technology but it operates very differently and we're very used to saying okay we write code and we know the code is deterministic we know what is going to happen or we program automation into our internal systems and it's very clear that these are the five steps that are going to happen every single time.

[11:28](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=688s) And when we think about AI, then it's actually about like, well, how do you define what the outcome is that you're looking for and how do you determine your evals? And then it's about speed of experimentation because no one can tell you, you know, just right by looking at something that, hey, by adding this data source, we're going to improve the quality or by, you know, being able to connect these different tools together, this is going to really change the outcome that we can drive. You just have to try things and you have to experiment quickly and the faster you can experiment within having like a very clearly defined outcome and very clearly defined evals that's going to make or break your ability to get you know more use cases out there and to deliver them with a high enough quality and efficacy.

[12:10](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=730s) And that is so so different than how so many of us operate in our day-to-day. And it's so different especially when we think about building technology and building technology first with an outcome and eval versus thinking about technology as kind of a set of steps or workflows or tools.

[12:26](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=746s) How do you even approach this root cause analysis when it's so probabilistic? It's hard. No one has a perfect answer for this. And obviously root cause analysis like what are you doing the root cause on even it's not even like there's traditional bugs like there's some output that's coming out and like is that a bad output or a good output is that you know is it there's so much that is you look at it as a human you're like oh I can and you're a knowledgeable expert you can be like yes this is good and bad but how do you explain that in a much more detailed way it's very difficult to do it is a really tough problem.

[13:00](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=780s) And I think if it were easy I think we would all be seeing like a massive amount of AI adoption especially in larger enterprises where things are much more complex but right now all of these things are still being figured out we're trying to learn and no one has a perfect answer of how to do it no one has a perfect clarity around hey I have this use case is this something that is truly a use case I will actually be able to solve well today or do I need to wait for the technology to advance and of course the technology advancements and model advancements are happening so fast that it's like something that works today or doesn't work today could work six months from now and be great. It's just hard to predict.

[13:37](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=817s) How do you change this mental shift with your team? I tell my team to just experiment all the time. Like I practice that I try to practice that visibly as well. It's like even in my personal life it's like every single thing can I try AI first? Fine. Like you know it some of the time for some use case it doesn't work well at all and for other use cases it works great. And what I found is that like that changes month to month and that might change based on the model that I'm using as well.

[14:05](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=845s) So I think taking that mindset both like in the your personal life it's easier because you have you can use more tools in your work environment you might be limited in the set of tools but whatever is available to you just try things and think about how like I have a problem I need to solve like is there a way that AI could help me let's let me try it and and see where I land and I think it's that experimentation it's the usage of like really being hands-on every day that gives you the intuition about what works and not because intuition is I believe very important. It's very important in developing products. It's very important in developing AI agents to solve various problems within your own organization or within your product portfolio for your customers.

[14:47](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=887s) But that intuition can't be built without like very very hands-on experience and just building the reps and doing that. The the majority of times I hear from people that they are afraid of AI is when they never used it. So I think exactly building this intuition helps to understand and think about security as well. Right. Absolutely.

[15:05](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=905s) But there's a tension like if you do it personally it's much easier. You can control many things. If you do it for an enterprise or a company there's always the tension between like letting agents act and then keeping them safe. So how do you think about these design boundaries that don't paralyze innovation and don't stop companies from experimenting?

[15:24](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=924s) This is a great question. So most large enterprises have some kind of AI governance committee or AI committee. Maybe it's not called governance, but they play a governance role in addition to hopefully helping with the acceleration of adoption of AI within these enterprises. And I think one of the bottlenecks that often comes up is, hey, I'm thinking about this AI use case. It might be an external vendor I want to bring in. It might be something I want to build using various agent builders internally in my company or a combination of both.

[15:56](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=956s) A lot of the bottleneck occurs because there's a huge concern around you know what's going to happen like what do these systems what are these technologies what are these AI agents have access to and what could they potentially do with this data and that could be things like okay in coding are they going to insert vulnerabilities into my code that I need to be really thoughtful of or anything that might be a real concern to the security posture of my company or like customer trust it could be things like hey I'm worried about that there might be unintended data exposure.

[16:28](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=988s) In you know large enterprises usually you need real segmentation of data between customers you don't want one customer seeing someone else's data you start opening things up with AI agents and the boundaries get blurred like how do you create those guardrails internally with employees how do you make sure that like okay I might be sitting in the product organization I shouldn't have access to HR data especially for another organization those are the concerns that come up in these meetings when you know the AI committees are trying to evaluate both like what is the risk.

[17:00](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1020s) And then of course there's a question about the efficacy as well because a lot of AI projects get stuck in prototyping and never make it to production something like somewhere between 70 and 90% depending on whose data you look at and a lot of that is because the efficacy isn't there too so you have to have both efficacy and then the what is the kind of risk that you're taking and if you're able to deliver efficacy with greater guard rails that protect around the data access and governance piece of it. What do these AI agents have access to and what are they able to what kind of actions are able to take? I think then is like when you can unlock faster innovation, but it's both things. It's like how do you actually really get that efficacy and then how do you have the governance guard rails that allow you to say okay actually I'm ready to move faster.

[17:46](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1066s) How often do you change your company plans depending on this uncertainty in the company's uncertainty with their behavior and with this super fast race of models and all the developments? Well, I think like for us, like for me in product development where we're developing products for our external customers, the plans around AI are changing constantly because while we are anchored on the problem of at Rubrik of how do we accelerate AI adoption for large enterprises and large highly regulated enterprises often as well, we know what the problem space is, the solution space is like and the possibilities of these solutions because the technology is changing because people haven't even centralized on a single stack yet of what that their layers are going to look like for different kinds of AI agents. It's changing so fast.

[18:35](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1115s) So we have to be really responsive to what are we seeing when we're talking to our customers when we're putting betas out there, what's working, what's not, and how is the landscape changing. So it feels like while the high level strategy isn't changing every week because we know what problems we're trying to go solve, like how we're thinking about solving those problems is changing quite a bit. And I think like the best thing is as if like for every organization who's building AI products for external customers is can you dogfood that your solutions first and we are doing that and I think you learn so much from just trying it and you may not be your ideal customer profile but that's okay you will learn a lot by being able to try it yourself and see how it works in the way that your company operates and for the kind of personas within your organization that might be using these products.

[19:22](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1162s) Having a strong point of view on the problem but being flexible on the solution and then iterating quickly using real real scenarios internally and externally with customers is the best way to kind of continue to evolve with so much uncertainty in this space.

[19:39](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1179s) Can you just name me the list of the problems that you're solving? We have a product that we launched called Rubrik Agent Cloud. And the problems that we're really trying to solve are how do we help large enterprises get visibility into all of the agents that are running in their environment? Understand what they have these agents have access to and what are they actually accessing and doing within various systems. Being able to block unintended or unwanted actions through predefined rules, but then also evolving those on an ongoing basis and then helping organizations when an AI agent makes an unintended change in or unwanted change in any of your environments, data applications, being able to quickly reverse that change. So that's the set of problems that we think are really important to solve in order to be able to really accelerate adoption of AI within the large enterprise.

[20:35](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1235s) I usually ask builders what do they think about AGI and for you I have a double question. How do you feel about AGI and related to what you were saying just now what would a real undo button reverse button for AGI look like?

[20:52](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1252s) So I think what do I feel about AGI? My sense is it's an inevitability. It's a matter of when, not if it's going to happen. I think many things are going to change. Rubrik as a security and AI company from a security landscape, it's going to be yet another tool that attackers are using. Already attackers use AI to have highly personalized and realistic social engineering that's leading to more compromised identities. Certainly AGI will help that, but I think it's going to completely change even once attackers break in how quickly they're able to compromise an organization and their data.

[21:28](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1288s) And so just all of this is going to mean that the timeline for what happens and how quickly people get breached and the impact of that is much higher. It kind of goes back then to like even whether it's a cyber attack or unwanted AI agent actions, how do we think about reversibility? So I think like reversibility, it only makes the problem more acute. The solution to the problem is still probably not that different which is being able to really I talked earlier about how do you understand this whole timeline of what an agent actually did having those guard rails in place if you understand those you have the guard rails yes you're going to miss some stuff in the guardrails but you can understand you can look at all the actions and rank them essentially by their risk and AGI will help with that to make the risk understanding of that risk a lot a lot better a lot fewer false positives and really and false negatives and really elevating the things that are the highest risk actions.

[22:23](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1343s) And then it's about how do you have the integrations into all the different technologies that allow you to quickly understand what changed and go into that system and reverse that change. And that technology is like that's what like our bread and butter of what we do every day at Rubrik is helping people recover whether it's from cyber events, operational failures, and now unwanted AI agents. It's definitely not an easy problem to solve because AI agents can be touching hundreds of different systems and devices, maybe even thousands, into being able to monitor and track all of those, pick out the signal, and then be able to actually do the reversal really quickly. It's a really exciting and challenging opportunity.

[22:57](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1377s) But do you think this system, AGI system will act on itself, whatever decision it is, or it still will be hackers who decides? Well, I mean, I think there's uh it's a good question. I don't know the answer to that. I think it's a great question. I mean, I think AGI some version of it will be used by humans that are giving a very like very broad outcome. Whereas today with AI agents, you have to give a very specific outcome. Right? With AGI, you could give a very broad outcome.

[23:26](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1406s) So you could have from a national security perspective. You could have a foreign government that's trying to wage war on another country give very broad outcomes for what it's looking for and AGI will just go figure out how to execute on those outcomes. I do think that it's like again it's an accelerant of what's already happening. There'll probably be new vectors that AGI discovers of hey this is like an interesting vector of an attack or on the defense side as well. Here is like an interesting vector of things that we can defend against both proactively and reactively. It's just this accelerating the cat and mouse game that already exists.

[24:04](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1444s) From my perspective, will we have AGI where it's like not even human directed? Now you're starting to talk about total science fiction world, which like is certainly possible, but then we might have a lot bigger problems on our hands than what we're talking about here today.

[24:17](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1457s) It's funny that builders just do not talk a lot about sci-fi. It's everything is, you know, in the trenches, hands-on, we build it. So that's why I like to ask this question because are we thinking about sentient systems or it's just a technology that we need to deal with on a different scale? I love sci-fi. I read sci-fi all the time. So it's stuff that I like to think about, but I don't know. I mean, it's as a builder, it's hard to predict that kind of future, which would probably mean a fair amount of social upheaval and a lot of different things that monumentally affect our day-to-day life.

[24:49](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1489s) Well, then my last question would be about books. What is the book that formed you or influenced you? And it can be a recent book or something from the childhood.

[24:59](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1499s) Like I said, I'm a big science fiction reader. There's not probably just one. I do feel like related to AI, the Isaac Asimov's I, Robot series was so influential to me growing up because the thing that I love about sci-fi is that it helps you imagine so many alternative realities and what they could mean. And what I specifically love about Asimov books, not just I, Robot, the rest of his books, is that they're very philosophical as well. And so there's so much that it makes you think about in terms of ethics and how would you handle these very different kinds of situations that you know it's hard to predict. They're very different, right? They're not situations that you're dealing with today.

[25:43](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1543s) And I think like honestly my love for sci-fi is what led me into being a builder because I love thinking about the future. Now, some futures are probably too far ahead for what I need to think about today. But I find that so exciting and trying to think about and predict what the future could look like even in small increments and then thinking about how we can innovate and solve those kinds of newer problems that are going to be emerging in new ways using the latest and greatest technology.

[26:10](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1570s) Thank you. That's fascinating. Is there any sci-fi book that you would love to be true in the future? Most of the sci-fi books are quite dystopian. The great technology advancements come with a lot of negatives as well. I guess like if I could have one sci-fi technology, I would love teleportation. Yeah, I love it too. That would be like the best thing to be able to just like travel anywhere and without having to like spend the time traveling there or getting to places like other planets without and other solar systems and all of that. That would be incredible.

[26:45](https://www.youtube.com/watch?v=5P8kgRcLF5Q&t=1605s) Maybe that's something we will come to pretty soon. Thank you so much for this conversation. It was very insightful.
