---
video_id: EwhHUQuqiaQ
title: "Richard Sutton and Dwarkesh Patel – speaking two different languages"
channel: Turing Post
duration: 637
duration_formatted: "10:37"
view_count: 45708
upload_date: 2025-09-29
url: https://www.youtube.com/watch?v=EwhHUQuqiaQ
thumbnail: https://i.ytimg.com/vi/EwhHUQuqiaQ/maxresdefault.jpg
tags:
  - Artificial Intelligence
  - AI Innovations
  - Future of AI
  - Machine Learning
  - Neural Networks
  - AI Breakthroughs
  - Technology and Society
  - AI in Real Life
  - Turing Post
  - AI Revolution
  - AI Trends
  - Thought Provoking AI
  - Innovators in AI
  - AI Interviews
  - Richard Sutton
  - Dwarkesh Patel
  - Imitation
  - AGI
  - Four-Part Agent Model
---

# Richard Sutton and Dwarkesh Patel – speaking two different languages

## Summary

This video from Turing Post analyzes the viral interview between Richard Sutton, the father of reinforcement learning and 2024 Turing Award winner, and podcaster Dwarkesh Patel. The host argues that the interview sparked massive debate on Twitter because the two participants were essentially speaking different languages, using the same terms but with fundamentally different meanings rooted in different paradigms of AI understanding.

The video breaks down key terminology mismatches: "prediction" for Sutton means predicting consequences of actions in the world with ground truth feedback, while for Patel it means next-token prediction in LLMs. Similarly, "goal" for Sutton signifies real-world outcomes that define intelligence, whereas Patel uses it to describe training objectives. The concept of "imitation" also differs, with Sutton emphasizing that children always imitate with a purpose or goal, not mindlessly.

The analysis culminates in Sutton's four-part model of intelligence (policy, value function, perception, and transition model) and his view that AI succession to digital intelligence or augmented humans is inevitable. The host notes that while Sutton may seem harsh on LLMs, he acknowledges their surprising effectiveness while predicting that experience-based learning systems will eventually supersede them, as another instance of his famous "Bitter Lesson" - which he himself dismisses as merely an empirical observation about a particular 70-year period in history.

## Key Points

- **Richard Sutton's Background** ([0:25](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=25s)) - Sutton comes from the old school of AI, quoting Alan Turing and John McCarthy, and just won the 2024 Turing Award for inventing reinforcement learning

- **Why the Interview Went Viral** ([1:05](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=65s)) - The conversation set Twitter on fire because the two participants spoke very different languages at different levels of understanding and abstraction

- **Prediction Mismatch** ([1:21](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=81s)) - For Sutton, prediction means "if I act in the world, what happens next?" with ground truth, while for Patel it means next-token prediction in LLMs

- **Prediction Tied to Surprise** ([1:46](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=106s)) - Sutton's prediction is tied to surprise that changes how you act, whereas LLM prediction has no feedback loop about real-world consequences

- **Goals Define Intelligence** ([2:38](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=158s)) - For Sutton, having a goal is the essence of intelligence, quoting McCarthy that intelligence is "the computational part of the ability to achieve goals"

- **Goal Mismatch** ([3:05](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=185s)) - When Sutton says goal he means real-world outcome; when Patel says goal he means internal training objective

- **Imitation Debate** ([3:28](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=208s)) - Patel argues kids learn from imitation, but Sutton counters that children always have a goal behind imitation, not mindless repetition

- **World Model Confusion** ([4:35](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=275s)) - Patel claimed LLMs are the best world models in AI, but a world model should predict consequences, which LLMs cannot do

- **Sutton's Precision vs Patel's Imprecision** ([4:41](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=281s)) - Sutton understands and has invented the terms he uses, while Patel juggles them without full precision or complete understanding

- **AGI Discussion Breakdown** ([5:39](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=339s)) - Sutton questions Patel's reasoning about AGI, noting his ideas seem to presume AGI already exists

- **Four-Part Intelligence Model** ([6:31](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=391s)) - Sutton summarizes intelligence as: policy, value function, perception, and transition model of the world

- **Networks Not Models** ([7:11](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=431s)) - Sutton prefers calling LLMs "artificial neural networks" rather than "models" to avoid confusion with world models

- **Experiential Paradigm** ([7:22](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=442s)) - Sutton calls his framework the "experiential paradigm," meaning intelligence based in experience

- **LLMs Not Dead** ([7:26](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=446s)) - Sutton acknowledges LLMs are surprisingly effective at language tasks but expects experience-based systems to eventually supersede them

- **AI Succession Inevitable** ([8:32](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=512s)) - Sutton's four reasons AI succession is inevitable: no unified human governance, we will figure out intelligence, reach superintelligence, and the most intelligent entities will gain power

- **Digital Intelligence or Augmented Humans** ([8:47](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=527s)) - Succession to digital intelligence or augmented humans is inevitable and should be based on experience

- **Two Languages Talking Past Each Other** ([9:49](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=589s)) - The conversation sounds like a debate but is really two languages of intelligence talking past each other

- **Bitter Lesson Dismissed** ([10:08](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=608s)) - Sutton dismisses his own Bitter Lesson as merely "an empirical observation about a particular period in history" that may not apply to the next 70 years

## Mentions

### Companies
- **Twitter** ([1:05](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=65s)) - Platform where the interview sparked massive debate and misunderstandings

### Products & Technologies
- **LLMs (Large Language Models)** ([1:56](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=116s)) - Systems that predict text token by token, which Sutton says lack true prediction and goals
- **Reinforcement Learning** ([0:44](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=44s)) - The learning paradigm Sutton invented, which earned him the 2024 Turing Award
- **TD Learning** ([6:40](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=400s)) - Temporal difference learning, referenced as the source of value function in Sutton's intelligence model
- **IMO (International Mathematical Olympiad)** ([9:23](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=563s)) - Competition mentioned as a potential benchmark for LLM capabilities

### People
- **Richard Sutton** ([0:25](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=25s)) - Father of reinforcement learning, 2024 Turing Award winner, represents old-school AI thinking
- **Dwarkesh Patel** ([0:10](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=10s)) - Podcaster who interviewed Sutton, representing modern LLM-era perspective
- **Alan Turing** ([0:29](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=29s)) - Pioneer quoted by Sutton, framed foundational questions about machine intelligence
- **John McCarthy** ([0:32](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=32s)) - AI pioneer who defined intelligence as "the computational part of the ability to achieve goals"

## Surprising Quotes

> "Intelligence is the computational part of the ability to achieve goals."
> — [2:49](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=169s), quoting John McCarthy

> "It's surprising. Yeah, you can have such a different point of view... Don't be difficult. I mean this is obvious."
> — [9:27](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=567s), Richard Sutton responding to Dwarkesh Patel

> "I'm not sure this your idea makes sense because it seems to presume the existence of AGI and then that we've already worked that out."
> — [6:16](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=376s), Richard Sutton questioning Patel's AGI reasoning

> "The bitter lesson. Oh, who cares about that? That's an empirical observation about a particular period in history. 70 years in history doesn't necessarily have to apply the next 70 years."
> — [10:15](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=615s), Richard Sutton dismissing his own famous paper

> "In it you can hear the old and the new AI not fighting but failing to translate."
> — [10:00](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=600s), the host summarizing why the interview went viral

## Transcript

[0:00](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=0s) Hello from San Francisco. It's Misty here and foggy and I'm sick, but I'm ready to discuss the latest interview with Richard Sutton and Dwarkesh Patel. They speak two different languages, old and new, and we need to understand if we understand what each of them mean. Let's go.

[0:25](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=25s) So, Richard Sutton comes from an old school. He quotes Alan Turing and John McCarthy, the man who first framed such questions as can machines think, what is intelligence? How do machines learn? And Sutton is famous for being one of the inventor of reinforcement learning and he just won the 2024 Turing award for his role in inventing reinforcement learning. This is like Nobel Prize.

[0:56](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=56s) And suddenly his conversation with podcaster Dwarkesh Patel set internet and especially Twitter on fire. Why? What happened? Why there was so many misunderstandings? Why there was so many posts arguing with each other? I think it is because these two people spoke very different languages and on very different level of understanding and level of abstraction.

[1:21](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=81s) First word that shaped the whole conversation was prediction. Sutton's prediction means if I act in the world, what happens next? When you predict what will happen, you predict and then you see what happens. There is ground truth. He says you touch fire, you get burned, you move a chess piece, face the consequences. Prediction is tied to surprise and this surprise changes you in how you act.

[1:50](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=110s) Prediction that Dwarkesh has in mind is very different. What is the next word? That's his prediction. So this is LLM's predicting text which often looks like reasoning and sometimes even like planning but it just token after token. There is no actual prediction in a sense that the model or as Richard prefers to call it network the neural network cannot predict what you as a human will do after the network gave you this token after token reply and there is no prediction here I guess Richard said so they both use this prediction but it's a completely different meaning and that single mismatch set kind of the mood for the conversation.

[2:38](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=158s) And then there were goals. So goals simple word you might think but it really was a next structure for Sutton having a goal is the essence of intelligence. He quoted McCarthy that intelligence is the computational part of the ability to achieve goals. So for him goal means sign for intelligence.

[3:02](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=182s) And Dwarkesh says, "No, LLMs have goals, too. Next token prediction." But that's not a goal, says Richard Sutton. It doesn't change the world. You can't look at the system and say it has a goal if it's just sitting there predicting. So when Sutton says goal, he means a real world outcome. When Dwarkesh says goal, he means an internal training objective. Very different.

[3:28](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=208s) Then the big discussion on internet was about imitation. Again they use this word imitation and Dwarkesh says kids will initially learn from imitation. They repeat the words the actions and that's how they got their cultural skills. That's how they used to learn how to hunt, how to propagate seeds and all that stuff.

[3:49](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=229s) He immediately argues that it's of course not true because he sees an exploration. He sees that in this imitation there is always a goal. Kids do not just imitate. They have a goal. Why do they say this word? Why do they do these things? Do they test the boundaries? Do they want milk? So they have a goal and that's why they imitate. It's not just, you know, mindless and goalless repetition of stuff or things.

[4:19](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=259s) I would attest to that as a parent. Kids certainly have a goal why they do all these things all over again time after times repeating it's not just for imitation it's to achieve a goal it's an important difference.

[4:35](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=275s) There was much more of that they used world model differently and here's actually a big important moment because when you listen to Richard Sutton he understands the terms he's been using them he's been inventing them for many many years. When Dwarkesh uses terms, he juggles them without full precision, without complete understanding what they mean.

[4:57](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=297s) He said, for example, that LLMs are the best world models we've made to date in AI. And it's just not true because a world model again would enable you to predict what would happen after something that model T and it might happening in simulation but it's a controlled environment. We don't have our LLMs as world models.

[5:23](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=323s) There were more. But there was also a very funny moment about AGI and how different the thinking process is and how smart and present Richard Sutton is just following what words Dwarkesh Patel throws at him.

[5:39](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=339s) "Well, how did we get to this AGI? And you want to presume that it's been done. So suppose it started with general methods but now we've got the AGI and now we want to go..." "We're done." "Interesting. You don't think that there's any anything above AGI?" "Well, but you're using it to get AGI again." "Well, I'm using it to get superhuman levels of intelligence or competence at different tasks. So these AGIs, if they're not superhuman already, then the knowledge that they might impart would be not superhuman." "I guess there's different gradations..." "I'm not sure this your idea makes sense because it seems to presume the existence of AGI and then that we've already worked that out."

[6:27](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=387s) If Dwarkesh thinking about AGI doesn't make sense for Richard, then what does? Richard Sutton summarizes intelligence in four parts. Its policy in the situation I'm in. What should I do? Its value function and number from TD learning. How well is it going? Its perception, the state, your sense of where you are. And then transition model of the world. If you do this, what will happen? Your physics of the world.

[6:58](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=418s) And notice how the world model here is explicitly a model of consequences and not a giant text prior because as I said he even asked not to use the word model when Dwarkesh spoke about models LLMs because he said these are networks artificial neural networks details but still the whole thing he calls Richard Sutton calls the experiential paradigm which means based in experience.

[7:26](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=446s) Did he say that LLMs are dead as it was in the title? Not exactly. He said that it's surprising how effective artificial neural networks are at language tasks. There was a surprise. It wasn't expected. But he added that yet I expect there to be systems that can learn from experience which could perform much better and be much more scalable.

[7:48](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=468s) In which case it will be another instance of the bitter lesson that the things that used human knowledge were eventually superseded by things that just trained from experience and computation. And this is the axis where he says LLMs do not seek.

[8:02](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=482s) What's interesting is that it's absolutely inevitable for Richard Sutton that AI will succeed. And here I would like to stop for a second and say that for some people he might sound too harsh about LLMs. But from what I hear, he doesn't yet know what exactly will happen and how we will do it. So I think LLMs can be at ease for now. We not going to cancel them yet. They still should go as a development.

[8:30](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=510s) These are his four steps why AI succession is inevitable. No unified human governance. We will figure out intelligence. We will reach super intelligence and the most intelligent entities will gain power. Succession he says to digital intelligence or augmented humans is inevitable and it should be based on experience.

[8:50](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=530s) It was actually very nice and refreshing to hear from Patel these exact words that succession to digital intelligence or augmented humans is inevitable. This is the understanding of how AI will augment us which I'm a true believer in.

[9:11](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=551s) So where did this discussion land? Basically the same place where they began. Dwarkesh thinks they already LLMs are good prior and if they can get gold at IMO maybe they are the right scaffolding.

[9:27](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=567s) But Richard says "it's surprising. Yeah, you can have such a different point of view." And then he says "don't be difficult. I mean this is obvious."

[9:34](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=574s) And at this moment you see that they are on such a different level of understanding of the level of detailization how it's built that the conversation sounds like a debate when it's really two languages of intelligence talking past each other.

[9:53](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=593s) And that's why this interview lit up the internet because in it you can hear the old and the new AI not fighting but failing to translate.

[10:05](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=605s) And the funny thing is that young people cling so tightly to the bitter lesson article that Richard Sutton wrote while he himself the author says "the bitter lesson. Oh, who cares about that? That's an empirical observation about a particular period in history. 70 years in history doesn't necessarily have to apply the next 70 years."

[10:25](https://www.youtube.com/watch?v=EwhHUQuqiaQ&t=625s) Thank you for being with Attention Span. Leave your comments, subscribe, and see you next time.
