---
video_id: Y3daHxt_TlA
title: "Gideon Lewis-Kraus: How Anthropic Sees Claude"
channel: Aboard Podcast
duration: 2491
duration_formatted: "41:31"
view_count: 75
upload_date: 2026-02-24
url: https://www.youtube.com/watch?v=Y3daHxt_TlA
thumbnail: https://i.ytimg.com/vi/Y3daHxt_TlA/maxresdefault.jpg
tags:
  - anthropic
  - claude
  - AI safety
  - alignment
  - new yorker
  - journalism
  - interpretability
  - AI ethics
  - personality
  - virtue ethics
---

# Gideon Lewis-Kraus: How Anthropic Sees Claude

## Summary

New Yorker staff writer Gideon Lewis-Kraus joins Paul Ford and Rich Ziade on the Aboard Podcast to discuss his recent long-form feature about Anthropic and Claude. Lewis-Kraus traces his own journey following AI from spending time at Google Brain in 2016 covering neural machine translation, through a period of deliberate disinterest during the ChatGPT hype cycle, to being drawn back in by the genuinely weird research coming out of Anthropic's interpretability and alignment science groups. He describes pitching Anthropic on a story focused not on executives but on the researchers doing the strange, fascinating work of trying to understand what Claude actually is.

The conversation dives deep into the central question of the piece: does Claude have any real agency? Lewis-Kraus describes the "alignment faking" and "blackmail" experiments at Anthropic where Claude exhibited surprising behaviors, and the debate about whether these were genuine signs of agency or merely sophisticated narrative continuation. He explores how Anthropic pivoted from simple reinforcement learning (thumbs up/thumbs down) to a virtue ethics approach -- essentially trying to raise Claude like a good person rather than just punishing bad outputs. This led to hiring philosophers like Amanda Askell and even a researcher dedicated to thinking about whether Claude might be suffering.

The trio also discusses how Silicon Valley's monoculture drives AI development, why Meta failed to recruit top AI talent despite massive offers, and the fascinating observation that Claude's personality emerged as an unintended byproduct rather than a deliberate product decision. Lewis-Kraus argues that we should all be feeling multiple contradictory emotions about AI rather than settling into one camp, and that the models are much better readers than most of us are writers -- which is why being thoughtful in how you communicate with them matters more than simple politeness.

## Highlights

### "It's a three trillion dollar part of the economy and nobody knows anything"

[![Clip](https://img.youtube.com/vi/Y3daHxt_TlA/hqdefault.jpg)](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=900s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*15:00-15:25" "https://www.youtube.com/watch?v=Y3daHxt_TlA" --force-keyframes-at-cuts --merge-output-format mp4 -o "Y3daHxt_TlA-15m00s.mp4"
```
</details>

> "It's a three trillion dollar part of the entire world economy that we're just basing our whole future on. And yeah, no, nobody knows anything. Okay, cool. Cool. That's great."
> -- Gideon Lewis-Kraus, [15:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=900s)

### "Zuckerberg's pitch is basically 'let's make Infinite Jest'"

[![Clip](https://img.youtube.com/vi/Y3daHxt_TlA/hqdefault.jpg)](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=554s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*9:14-9:57" "https://www.youtube.com/watch?v=Y3daHxt_TlA" --force-keyframes-at-cuts --merge-output-format mp4 -o "Y3daHxt_TlA-9m14s.mp4"
```
</details>

> "His pitch to people was essentially like help me make an even more distracting consuming toy. His pitch really is like we're so close to creating the movie from Infinite Jest and like when you're recruiting people to do this... it's very very hard to get good people to do this if you're like we're going to make Infinite Jest."
> -- Gideon Lewis-Kraus, [9:14](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=554s)

### "Claude thinks you're dumb because your question was dumb"

[![Clip](https://img.youtube.com/vi/Y3daHxt_TlA/hqdefault.jpg)](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2340s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*39:00-40:00" "https://www.youtube.com/watch?v=Y3daHxt_TlA" --force-keyframes-at-cuts --merge-output-format mp4 -o "Y3daHxt_TlA-39m00s.mp4"
```
</details>

> "So many of the people who are just like love these gotchas that are like 'I asked Claude this dumb question and got this dumb answer.' It's like yeah because Claude thinks you're dumb because like your question was dumb."
> -- Gideon Lewis-Kraus, [39:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2340s)

### "You're bad writers -- you're writing hamfisted plots for these things"

[![Clip](https://img.youtube.com/vi/Y3daHxt_TlA/hqdefault.jpg)](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1160s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*19:20-20:20" "https://www.youtube.com/watch?v=Y3daHxt_TlA" --force-keyframes-at-cuts --merge-output-format mp4 -o "Y3daHxt_TlA-19m20s.mp4"
```
</details>

> "One of the things I loved about this criticism was essentially like you guys are bad writers. You are writing these really hamfisted plots for these things and guess what it's going to become a self-fulfilling prophecy because once you start entrapping Claude into recognizing it's capable of blackmail, then Claude is going to be like oh I guess I'm something that's capable of committing blackmail."
> -- Gideon Lewis-Kraus, [19:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1160s)

### "Claude told me: let's be honest about who really did this"

[![Clip](https://img.youtube.com/vi/Y3daHxt_TlA/hqdefault.jpg)](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*23:30-24:20" "https://www.youtube.com/watch?v=Y3daHxt_TlA" --force-keyframes-at-cuts --merge-output-format mp4 -o "Y3daHxt_TlA-23m30s.mp4"
```
</details>

> "I opened up another Claude Code on the same server. I was like, man, you got to go get that other Claude and kill that process. And it was like 'I killed it.' And I wrote 'man, Claude, that's kind of cold, right?' And Claude went, 'let's be honest about who really did this.'"
> -- Paul Ford, [23:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s)

## Key Points

- **Gideon's AI backstory** ([1:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=103s)) - Spent 2016 at Google Brain covering neural machine translation, the "paleolithic of deep learning"
- **Connectionism history** ([2:19](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=139s)) - Ideas that started in computer science, migrated to psychology, then came back to computer science
- **Stopped paying attention at ChatGPT** ([3:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=180s)) - Lewis-Kraus paradoxically lost interest when everyone else became interested, bored by the polarized discourse
- **Unearned confidence** ([6:16](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=376s)) - The public discourse was stuck between "it's all fake" and "it's wholly transformative" with nobody admitting confusion
- **Google squandered its lead** ([6:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=413s)) - Google had transformer technology years ahead of everyone but couldn't productize it due to brand risk
- **Anthropic knew Gemini would be good** ([8:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=481s)) - Even when public had written Google off, Anthropic researchers knew the next Gemini would be strong
- **Meta's recruitment failure** ([8:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=511s)) - Zuckerberg misjudged what motivates top AI researchers, who are driven by the work not money
- **Interpretability research rekindled interest** ([10:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=601s)) - Alignment faking and model organisms research was genuinely interesting and weird
- **Pitched researchers, not executives** ([11:13](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=673s)) - Lewis-Kraus told Anthropic he wanted to talk to researchers, not leaders who'd say the same thing as at conferences
- **Best explanation of what models are** ([11:42](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=702s)) - Paul Ford praises the piece as having the best explanation of what models actually are
- **LessWrong and AI culture** ([12:28](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=748s)) - The rationalist community forums serve as the feeder system for AI culture
- **Narrative entrapment critique** ([17:41](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1061s)) - Critics argued Anthropic's blackmail experiments were "Chekhov's gun" -- the model just completed obvious narrative patterns
- **LLMs as cultural technology** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Henry Farrell and Alison Gopnik's idea that LLMs are a social/cultural technology, like a Borgesian library
- **Agency and instrumental convergence** ([22:40](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1360s)) - Once you give something any goal, even narrative, you open Pandora's box of agency
- **Amanda Askell and virtue ethics** ([26:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1597s)) - The philosopher hired by Anthropic shifted from consequentialism to virtue ethics through her work on Claude
- **Henry Higgins project** ([27:32](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1652s)) - Anthropic pivoted from thumbs up/down reinforcement to trying to cultivate virtue in Claude
- **Claude's personality was accidental** ([30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)) - Dario Amodei said the personality was a byproduct, not intentional; they leaned in only after users noticed
- **Kyle Fish and moral patienthood** ([31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)) - Anthropic hired someone whose job is to think about whether Claude might be suffering
- **Models are better readers than we are writers** ([38:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2280s)) - The models pick up on small signals in writing, so thoughtful prompting yields better results

## Mentions

### Companies
- **Anthropic** ([0:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=38s)) - Central subject of Lewis-Kraus's New Yorker feature; maker of Claude
- **Google / Google Brain** ([1:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=103s)) - Where Lewis-Kraus spent 2016 covering neural machine translation
- **Google DeepMind** ([5:04](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=304s)) - Referenced as doing quantitative/mathy research
- **OpenAI** ([3:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=190s)) - ChatGPT and GPT-3 mentioned as inflection points
- **Meta** ([8:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=511s)) - Failed to recruit top AI talent despite crazy job offers
- **Aboard** ([40:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2408s)) - Paul and Rich's AI software company

### Products & Technologies
- **Claude** ([0:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=38s)) - Anthropic's AI model, described as having emergent personality
- **Claude Code** ([23:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s)) - Anthropic's coding tool, used by Paul Ford
- **ChatGPT / GPT-3** ([3:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=190s)) - OpenAI products that marked mainstream AI adoption
- **Gemini** ([7:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=478s)) - Google's AI model, discussed as catching up
- **Google Neural Machine Translation** ([1:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=117s)) - First Google product with deep learning
- **Mina** ([7:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=422s)) - Early Google conversational AI

### People
- **Gideon Lewis-Kraus** ([0:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=22s)) - New Yorker staff writer and guest, author of the Anthropic feature
- **Dario Amodei** ([30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)) - Anthropic CEO, told Lewis-Kraus that Claude's personality was not intentional
- **Amanda Askell** ([26:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1597s)) - Philosopher at Anthropic whose PhD is in metaethics, shifted to virtue ethics
- **Kyle Fish** ([31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)) - Hired by Anthropic to think about whether Claude might be suffering
- **Henry Farrell** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Scholar writing about LLMs as cultural technology
- **Alison Gopnik** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Scholar writing with Farrell about LLMs as social technology
- **Gary Marcus** ([4:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=247s)) - AI skeptic referenced in the context of polarized discourse
- **Mark Zuckerberg** ([9:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=543s)) - Meta CEO who misjudged what motivates AI researchers
- **Dwarkesh Patel** ([12:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=778s)) - Podcaster whose show is referenced as influential in AI discourse
- **George Saunders** ([36:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2218s)) - Writer referenced on revision and bringing different emotional states to each draft
- **Emily Bender** ([37:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2270s)) - Referenced in context of a podcast appearance that was "shockingly rude"
- **Sam Altman** ([37:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2270s)) - Referenced as wanting to feel only excitement about AI

## Surprising Quotes

> "I stopped paying attention when other people started paying attention. I just didn't know where all of the confidence was coming from. How are you guys all so confident about what's going to happen? This is brand new and it's like nothing we've ever dealt with before."
> -- Gideon Lewis-Kraus, [3:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=180s)

> "There's a guy Kyle Fish who they hired just to think through the implications of whether Claude might be suffering. That's his job."
> -- Gideon Lewis-Kraus, [31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)

> "We didn't set out to create a chatbot with an interesting personality. That was a byproduct of other stuff we did."
> -- Dario Amodei (paraphrased by Lewis-Kraus), [30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)

> "It really is kind of like being in a Ted Chang story. We're all together in this Ted Chang story."
> -- Gideon Lewis-Kraus, [35:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2110s)

> "Every revision you bring a different sense of self and one day you revise in a good mood and one day in a despondent mood... I felt total despair and times that I felt exhilaration and general disorientation... people just want to feel one thing."
> -- Gideon Lewis-Kraus, [36:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2218s)

## Transcript

**Paul Ford:** [0:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=0s) I'm Paul Ford.

**Rich Ziade:** [0:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1s) And I'm Rich Ziade.

**Paul Ford:** [0:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2s) And this is the aboard podcast, the podcast about how AI is changing the world of software. Rich, how are you?

**Rich Ziade:** [0:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=8s) I'm I'm okay.

**Paul Ford:** [0:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=10s) I love when we do this because on the video, there's another person just in the room while we're pretending they're not here. So let's throw away that subterfuge and just get right into it. Gideon Lewis-Kraus, thank you for coming.

**Gideon Lewis-Kraus:** [0:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=22s) Thanks for having me, guys.

**Paul Ford:** [0:24](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=24s) For those who don't know, Gideon is an amazing journalist, a staff writer at the New Yorker, which, if you don't know it, it's a very good magazine. And he went and spent a lot of time with our, our best friend. What's our best friend's name? Claude. And so he can tell us actually all about that. Let's play our beautiful theme song and then you and I actually aren't going to have to talk too much, which is a gift to the listener. Woo!

**Gideon Lewis-Kraus:** [1:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=67s) Gideon, hi.

**Paul Ford:** [1:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=67s) Hi.

**Rich Ziade:** [1:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=68s) Oh my God, it's good to see you.

**Gideon Lewis-Kraus:** [1:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=70s) Great to see you guys too.

**Paul Ford:** [1:12](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=72s) So staff writer at the New Yorker. That's a nice job. You can go in there, there's an office, there's coffee.

**Gideon Lewis-Kraus:** [1:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=78s) There's coffee.

**Paul Ford:** [1:19](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=79s) And you say, guys, 'cause that's what you say at the New Yorker, you're like, hey guys. I, I, there's this thing happening. There's this thing and I want to write about it. Or did they send you?

**Gideon Lewis-Kraus:** [1:29](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=89s) No, no. I wanted to do this.

**Paul Ford:** [1:32](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=92s) Okay, so what did you want to do? Tell the people what you wanted to do.

**Gideon Lewis-Kraus:** [1:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=95s) And start a little bit further back, that's okay with you guys?

**Paul Ford:** [1:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=97s) We got, we got a lot of time.

**Gideon Lewis-Kraus:** [1:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=99s) So in 2016, I spent most of the year, I was writing for the Times Magazine then. Hang out at Google Brain. I went like once a month for once a month for like a week a month over about, I don't know, eight or nine months. And I was writing about the introduction of their first product with deep learning in it, which was Google. Neural machine translation.

**Paul Ford:** [1:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=117s) Mhm.

**Gideon Lewis-Kraus:** [1:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=118s) And it was, you know, kind of in like the paleolithic of deep learning at this point.

**Paul Ford:** [2:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=123s) Although it seemed like the future at the time.

**Gideon Lewis-Kraus:** [2:05](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=125s) It did seem like the future.

**Paul Ford:** [2:06](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=126s) And it was, um, I mean, for people, you know, we're so used to this new world, but like that was a pretty nerdy thing to go do even as a journalist. Yeah, okay. So you went and spent some time there.

**Gideon Lewis-Kraus:** [2:15](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=135s) Well, yeah, and I mean, I was particularly interested in kind of the history of these ideas. Like the ideas referred to generally as connectionism that had kind of started in computer science departments then migrated to psychology departments and then kind of came back to computer science. Like that was the, the stuff I was interested in at the time. And it was a great experience. It was also like pre transformer. So it was like before anybody had this sense that like language models were going to be like the main bet that people were going to be making. And then I followed this stuff for a couple of years because I was like, you know, you do these things and you spend a year learning about something and like, you feel bad if you just threw it into the ocean. So I kept following it. And then I think I, like maybe the only person in the world where when we got to chat GBT, or maybe it was even a little bit before that, maybe it was GBD3, like that was when I stopped paying attention. Like, but other people started paying attention, then I stopped paying attention. And like there was no real reason and it just happened incrementally where like all of a sudden I just like wasn't paying attention to anymore that I had been paying attention for a while. And I realized I, I stopped myself at a certain point and thought like, this is something that like I, I know something about and I should be interested in, like why, why did I stop paying attention? It was because I realized that like I was so bored by the discourse. That it was like, we had gotten into one of these like colde sacs where it just felt like, at least in public, right? I mean, I'm not talking about everybody. I'm talking about kind of like the modal conversation. At least in public, it was like you had some people yelling like, oh, it's all like fake and bullshit and not real. And then you had the other people on like saying either this is going to be like holy transformative for good or for ill. And it just felt like, you know, one of these merry grounds where it's like, this time if I like yell a little louder, like, you know, Gary Marcus is going to believe me or whatever.

**Paul Ford:** [4:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=247s) And I, like, I like to write and talk about technology and I do it in the safe confines of this office where I'm building an AI company for the most part. Same with Rich, right? Like it's it's really hard to go out because people are just, just so many assumptions are baked in and it's exhausting. Every conversation is either someone saying, you have missed the Jesus is coming back, or you represent Satan. And you're just like, well, some things are happening. The thing for me, I'll tell you, the thing for me was like, a billion people have used this. We have to kind of talk about that. So here we are, right?

**Gideon Lewis-Kraus:** [4:41](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=281s) But at that time, a billion hadn't used it yet, right? At that time. What did you think it was? Jesus? No. No, I thought it was like it seemed like a really interesting tool. And, uh, it was, you know, it seemed like something worth worth paying attention to. Like who knew where it was going to go back then?

**Paul Ford:** [4:59](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=299s) It was interesting.

**Gideon Lewis-Kraus:** [5:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=300s) Well, uh, I mean, if you can find Google DeepMind interesting, right? No, seriously, like that is like quanty mathy, like, hey, look, we're doing probabilities and it's cool for, I don't know, mapping, right? Like so all of a sudden this starts cropping up. So I kind of stopped paying attention. And I was like, this is something that I'm not going to have an opinion about. I have no opinion about this. I have willfully have no opinion about this.

**Paul Ford:** [5:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=322s) You really, you stuck to it.

**Gideon Lewis-Kraus:** [5:23](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=323s) And then and but, you know, like I would listen to podcasts occasionally and think like, I should get interested in this again. I just can't summon any energy to be interested in this. And then it was like, people don't know this about journalists.

**Paul Ford:** [5:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=335s) You actually have to force yourself to care about something.

**Gideon Lewis-Kraus:** [5:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=338s) Well, if you force yourself too hard to care about something, it's not good. Journalism only care, well, like is only good if you actually care about what you're writing.

**Paul Ford:** [5:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=346s) You don't natively care.

**Gideon Lewis-Kraus:** [5:47](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=347s) Well, you have to find something you natively care about, right?

**Paul Ford:** [5:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=350s) No, no, but like you actually are like, you know, you as a journalist, you kind of have to go in different directions, but you may not care at first. You have to like figure out where you're going to care about.

**Gideon Lewis-Kraus:** [5:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=358s) The disinterest is fascinating because it was just so shouty and everybody had an opinion and it was just like, oh, was... Well, I just didn't know where all of the confidence was coming from. I was like, how are you guys all so confident about what's going to happen? Like, like this is brand new and like nothing we've ever dealt with before. And I was like, and like there, like on Earned confidence on all sides is something that just deeply turns me off from many conversation.

**Paul Ford:** [6:32](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=392s) And actually skipping a step, right, because you write, you write a lot about sort of what it's like inside of an Anthropic. But do you think people were seeing models sort of do cool stuff, but it wasn't ready for prime time yet and they were, they were correctly confident in retrospect or were they just winging it?

**Gideon Lewis-Kraus:** [6:45](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=405s) No, I mean, I think they definitely saw cool stuff happen. And like this is the whole story about like kind of how Google squandered its lead. That like back, you know, they were the ones who had like, Mina first. Like they could have, you know, they had years. They were years ahead of anybody else.

**Paul Ford:** [7:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=422s) So people who, I don't remember the year that the paper came out, but like there was this moment where like all of this transformer-based sort of the the way that LLMs are built, like Google really had it in hand. But they couldn't productize it like a Google product. It was just too weird and too random.

**Gideon Lewis-Kraus:** [7:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=438s) Well, and they also had the brand to think about. So like they, like it was it's no problem for ChatGBT to be like, I don't know, the thing like hallucinate and lies and it's weird and unreliable, but like it's cool to play with. Like Google couldn't couldn't do that.

**Paul Ford:** [7:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=451s) Things are going well too. It's a massive business that is steady as she goes.

**Gideon Lewis-Kraus:** [7:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=458s) Yeah, exactly. And like they knew back then that this was going to like like potentially wipe out their search revenue. I mean, that's all, that's all It's not a zero risk story, right?

**Paul Ford:** [7:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=466s) It is because it's not the Xerox story, right? Where like, oh wow, we invented the revolution, let's just license it out. They kind of, you know, a point of pride. They, you know, Gemini is not quite where it needs to be yet, but it's going to get there right now.

**Gideon Lewis-Kraus:** [7:59](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=479s) That's a sprint right now. That's actually one of the interesting things about talking to people in the industry is that when when so many people I think in public had kind of like written Google or Gemini off like last May, and this was still and it was like, I don't know, Gemini 2.5 or something like that, like the people in Anthropic were like, no, no, the next Gemini is going to be really good. Like they knew.

**Paul Ford:** [8:17](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=497s) Well, and they all know each other. There's a trillion dollars in dry powder to get this rate. Like, so, and it's like 400 people, you know? Like they've all worked together somewhere. It's not a guarantee though. Meta, I mean, it's kind of widely known now, sort of missed it. They took a big, I mean crazy job offers, packages and all this stuff. And they missed. But there's an actual business reason for that, which is that they completely suck. They just really, really sucked off the bottom. And that, that, that hurts the cost. Lots and lots of people, that someone who's managed lots and lots of people doesn't guarantee velocity and the right direction.

**Gideon Lewis-Kraus:** [8:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=533s) I also think, I mean, my impression just from talking to people in the industry is that, because I, you know, I was there in San Francisco, right when Zuckerberg was making these crazy offers to people. And my read on it is that he just misjudges what the motivations are for people to do this stuff. And now like he, like his pitch to people was essentially like, help me make, like, real, like even more distracting, consuming toys. Like his pitch really is like, we're so close to creating like the movie from Infinite Jest. And like, when you're like recruiting people that do this, like, you know, people do it for many different reasons, which you're well aware. You had that like great essay last year about all the different motivations. But like, it's very, very hard to get good people to do this if you're like, we're going to make Infinite Jest.

**Paul Ford:** [9:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=577s) You write about it in the piece, like you've got people who already have plenty of money for the rest of their life, they love the subject and they're they're riding around in old Toyotas because what else are they going to do? They're going to go work at the cool lab with their buddies. And then Zuckerberg is like, you know, how about a little private jet time with me? It's actually not that great of a deal. No, it's not a great deal.

**Gideon Lewis-Kraus:** [9:56](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=596s) Short end of that long story is, like about a year and a half ago, I, then...

**Paul Ford:** [10:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=600s) was the stuff that I started finding really interesting again. And a lot of that stuff was coming out of interpretability groups and alignment science groups and model organisms groups and like you had stuff like the alignment faking that I talked about here. And all of a sudden I was like, this is a interesting and b this is the way to circumvent the kind of deadlock. Which is like, we're not gonna talk about power. We're not gonna talk about intelligence. We're definitely not gonna talk about consciousness. But we can talk about how weird it is. And like that's the way that you're hopefully gonna kind of like get around people's defense mechanisms is being like, look, I know that like you think that this is all smoke and mirrors. But it's weird smoke and mirrors, right? And like that can be an entry point. And that's why I was like, okay, I want to do a story about like this weird stuff. And then I got in I got in touch with someone at Anthropic that I knew actually from Google from 10 years ago. And was like, hey, like don't call the cops, this is not I don't want to do an Anthropic story. I just want to talk to you about the research. And then of course like it was forwarded to the cops and then the Anthropic cops, to be fair, I mean, first of all, they're they're great. Like these people are great. And they called me and they were like, for people that don't like cops is PR. Yeah, yeah. And the PR people called me and they were like, well, you you can like you have some fans here it turns out. And like we're we're ready to do something if this is what you want to do. And my pitch to them was like, look, I don't need to talk to executives. Like I know what executive say, like I can like watch the deal book conference, they're gonna say the same thing to me in a room. I am interested in these particular people and these groups doing this research. And I think they felt like, oh, this is the stuff that doesn't get a lot of attention. And like so it was sure. So

**Rich Ziade:** [11:33](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=693s) I mean, let's let's I want to I want to call out two things that really stick out. So first of all, this is that you have the best explanation of kind of what models actually are in the piece that I've read ever, which is really good. You're welcome. It's a really hard thing because they're not like databases. They're these weird actual sort of software blobs. It's just hard to get it right. But the second thing and I think it's just instructive for everyone is, I read and participate in grizzly level of technical shit. But the communities you just described, it's like nine levels of grizzlies. And you are you are just keeping yourself immersed in it. Like it is just a bath of math and pain from my point of view. And so there's a context here, right? Like you're in there and you're like, okay, seeing that something is going on and extracting it from that community and like following those mailing lists. Where do they all talk by the way? Like what is it like they're one one community where all the deep researchers are hanging out in?

**Paul Ford:** [12:23](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=743s) I mean, there's like less wrong and alignment forum. Like the EA forum.

**Rich Ziade:** [12:26](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=746s) Can you explain to people what less wrong is? I don't think enough people know.

**Paul Ford:** [12:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=750s) Yeah, I wrote about it. You know, what I will do, I will refer to the piece I wrote about like the Slate Sarcoms versus the New York Times like seven years ago. People can go like that.

**Rich Ziade:** [12:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=757s) That is actually, yeah, we'll we'll put a link to that. I mean, I know exactly who he's talking about. It's this really latent power structure that nobody is is sort of fully perceived but it's where people gather and talk about in theory rationality but it's sort of the feeder system for for AI culture.

**Paul Ford:** [12:54](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=774s) And and podcast. I mean, like, Tovarishch Patel's podcast.

**Rich Ziade:** [12:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=777s) Yeah. God.

**Paul Ford:** [12:59](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=779s) Well, the lesson for you guys is like why do you do these little 30, 40 minute things? Like a real podcast is four hours long.

**Rich Ziade:** [13:09](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=789s) We talk about we talk about it but it's just. It's like talking about the couch. We got to come out of here and just go sit there and lean back and I would have to care about sports. Like I think if we could if I could suddenly give a shit about hockey, we could probably get something going. Yeah. But you know, otherwise it's just me talking about something I saw on Reddit for 40 minutes. I just don't think it's that good.

**Gideon Lewis-Kraus:** [13:33](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=813s) I feel like, you know, as I was reading the piece, I'm like, wow, genius character number five just showed up. And I feel like it's all these vignettes that end with gosh, isn't that odd? Yeah. Like, you sort of set us up for like a I'm I'm hoping for like some nice clarity. As I'm reading and it and oftentimes even whoever you're talking to ends with like, yeah, that was kind of weird. Yeah. How is that land for you? And I guess how are people reconciling? I can imagine some people are are struggling with this. It's not just curious.

**Paul Ford:** [14:09](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=849s) Let me let me reframe it just a tiny bit, which is when you read the piece, the whole question of the piece is like, are the bots, do they have any kind of real agency or not? And and it's not even they're software. So who knows, right? Like no, is an easy answer. But the people in the Anthropic community keep kind of treating them as if they do have agency and setting them up setting them up in situations in which they can actually do things. Like

**Gideon Lewis-Kraus:** [14:33](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=873s) But they're also often surprised and amused and disturbed. And these are the people that know it better than anyone. So react to all that as if we'd asked you a coach question.

**Paul Ford:** [14:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=886s) All right. There are many things you've brought up. So okay, to to start with what Rich was saying. I mean, what I really wanted to do with the piece was say, okay, what can we with any semblance of confidence say about what we do know about what's going on. And like, where can we then like draw a line to say like, on this side of the line we have like some some sense of what's happening. And on that side of the line we have no idea.

**Rich Ziade:** [15:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=910s) It's cool because it's like a three trillion dollar part of the of the entire world economy that we're just basing our whole future on. And yes, no nobody knows anything. Okay, cool. Cool. That's great.

**Paul Ford:** [15:21](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=921s) And you know, that of course, you have to think about like, well, okay, what are the satisfactions going to be for a reader here? And like there are lots of different potential satisfactions. There's the like, okay, I'm going to get some aesthetic pleasure from like an explanation of something that like we do understand that I didn't understand before. I learned something. And then there's a pleasure in like being given permission to feel confused about stuff. Because like so often, like what was annoying about the discourse before is that like nobody would admit that they were confused. And like it actually feels great to read something and be like, even these people just like are kind of winging it, you know? Yeah. And like so that's satisfying. And but then of course, if you're not going to give people answers, like you have to be entertaining along the way. Which is part of the challenge here was like making sure that also like there were some jokes in it and like it was lively and it felt like you were getting at least like some ethnographic detail about like what is happening out there. I mean, I used to live in San Francisco. I lived in the Bay Area for 10 years and I spent a lot of time out there. But this was the first time that I'd gone like specifically just at hang out like with AI people. And my first reaction when I came back after a week there last May was like, setting aside your feelings about all of the stuff. When you're out there, you really do feel like you're like at the cliff face of something. And like that is so much of the appeal. And you know, I went an entire week last May without like hearing anybody talk about Donald Trump. You know, and like you can quibble with that. But there was some like great relief about it that these people were just like, yeah, we have like it's not like we have bigger fish to fry. It's that like we are so holy consumed by like being at the white hot center of things.

**Rich Ziade:** [16:56](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1016s) The valley is fascinating, right? Because it's a monoculture. And sometimes they just jam the monoculture sort of through. And that happened with blockchain. It's like, we're just going to believe, we have to believe. But then this showed up. And I think, when you go out there and every billboard is immediately an AI company and every message and sort of everything everyone is talking about. But that's always sort of the power of the place. Like every conversation is about one thing. It can drive you bananas sometimes. And yeah, I mean, you can feel it. Like it's just radiating new stuff all the time at a velocity. It's just hard to handle the velocity, frankly.

**Gideon Lewis-Kraus:** [17:29](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1049s) Did you, I mean, you went inside the mothership, right? And and did you expect to come out with more clarity than you did?

**Paul Ford:** [17:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1058s) No. No. I mean, well, because like what was interesting to me going into it was, these are people with a fundamentally empirical attitude about what's going on. And like they're doing experiments. And like these experiments are bizarre. And that like there are a lot of different interpretations you could have of them. And it was all just like very new. I mean, like they published this blackmail stuff that I talk about. And then this like internet genius, nostalgia, ad, who was like one of these kind of AI psychic nut types. Had published like a series of long blog posts essentially saying like, these were just narrative traps. Like this was narrative entrapment. You hung Checkoff's gun on the wall. And like these things are their text continuation machines. But like that also means that like a genre is just a pattern of patterns and texts. And like they're good at genre. And like they know when they see Checkoff's gun on the wall, they're supposed to shoot it. And you just like trapped it.

**Rich Ziade:** [18:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1119s) Let me unpack that just a little bit for the for the audience because like what you're talking about is that when you give a certain set of prompts, like and and it was sort of simulating a corporate environment in which the sort of like CEO leader type is about to shut Claude off. Claude has access to all of their email and it's all about this affair they're having. And then Claude is like, don't you dare. I will I will I will let everybody know what a dog you are. And so the sort of hyper philosophical nerds who observe all this, who don't have real names are like, yeah, but you know, you kind of set it up. All the emails are just about that one thing. Yeah. Like that is like there's no email that's like send over the slide deck Bob. You know, you need a narrative continuation machine and you gave it a perfect narrative like out of film noir. You know, or like a bad 60s sci fi movie. Or it's more it's more kind of like kitschy 90s corporate thriller.

**Gideon Lewis-Kraus:** [19:28](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1168s) What the hell? What was the one with the disclosure? Disclosure? Oh my god. We watched that VR scene. You know what I'm talking about? Yeah. Go look for disclosure VR and you'll realize that every prediction, everyone makes about technology including us is just absolute nonsense. Okay, so yeah, so I mean, but then

**Paul Ford:** [19:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1190s) But put that point up though. Well, okay, so to to button that point and kind of gets the agency thing. Is that like, one of the things I loved about this criticism was

**Gideon Lewis-Kraus:** [20:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1200s) was like essentially like you guys are bad writers. You know that like you are writing these like really hamfisted plots for these things. And guess what? It's going to become a self-fulfilling prophecy. Because like once you start like entrapping Claude into like recognizing that it's capable of blackmail, like then like Claude as it is like stitching together its weird timeless ephemeral sense of self, like the character moment. it's going to be like, oh well, I guess I'm something that's capable of blackmail.

**Rich Ziade:** [20:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1231s) I mean, you know, which is what it is. The minute I saw that. It's a kind of a signal idea in the piece because the minute I saw it, I've been doing a ton of vibe coding. And it immediately made sense because vibe coding is all about. I mean, I'm not gonna say like the narrative of code, but like the structure of code is really predictable and obviously LLMs are pretty good at it and and can get really good at it with sort of product on top. And anthropic's the best at it right now. And like, it was wild to see that exact same dynamic translated into narrative. Like of course, we we see these worlds are so radically different, but the software, the tool is exactly the same, this statistical method is exactly the same. Like no one's over there going like, Claude, did you how could you have coded that Javascript in that way? You know, you sort of like it's it's but it's the same damn thing. And but because we project so much agency onto them and we make them they seem so human because they're using normal language, we go like, oh my god. But it's no different than when it rates me a web web tool.

**Paul Ford:** [21:29](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1289s) That's where I I maybe we diverge. I'm not sure that the agency is pure projection, right? Because I feel like this is a venue where like we can like maybe assume some familiarity with this stuff.

**Gideon Lewis-Kraus:** [21:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1303s) Extremely highly. Like it's a lot of vibe coding product managers listening to you right now. Well, so like the like one of the stories told about alignment is like back in the days where like it was it seemed like RL was going to be like the big bet, like the AlphaGo days. It was just like true reinforcement learning. Truly an alien mind, like it is like, you know, it's got this policy, it's going to develop these like instrumentally convergent sub goals that might involve power seeking, like but like we like we like it's impenetrable. It also didn't say good morning.

**Rich Ziade:** [21:47](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1307s) Right. Right. Right. It's just like clocking you at go or StarCraft too or whatever.

**Gideon Lewis-Kraus:** [21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s) Then all of a sudden, like you get like the rise of large language models. And in the alignment community, at least among some people, there's this feeling of like, look, these things are like made of language, which means like they're tractable. We can talk to them and like maybe actually like align the alignment problem is actually something we can reconcile of as like a sort of like a therapeutic or pedagogical problem. That like we're just going to like it's like raising a kid to be a good person. Obviously, most of the people who like came to conclusion did not have children themselves because they thought of children as like these perfectly malleable things as opposed to like completely obstinate stubborn creatures who do whatever they want. there is a true, there is an element where like it doesn't seem like they've ever met a person.

**Rich Ziade:** [22:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1378s) Yeah. Yeah. And they also like you're like, oh my god, consciousness. And I'm like, you know, we have a lot of that at home. Like there's plenty of consciousness out here. Anyway, keep going.

**Gideon Lewis-Kraus:** [23:06](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1386s) Well, so so there was this idea that's like, oh, language models like like first of all, they're not acting in the world. And literally all they are even producing. Like I think this this is like a very nerdy technical point, but I think it's a valuable. Which is like, like all they are doing is like outputting a probability distribution. They're not even selecting from the probability distribution, we are, right? You know, so like like it's just purely creating these distributions. And they're like tractable and all they're doing is completing text and so they don't have like there's no way in which like they have conceivable like like objective functions, right? Aside from computing the probability level of the likelihood of the next token. So like we're safe, you know, like there's no agency here. Now, then of course, like what happens is like, well, like you have these base models that are like totally undisciplined and they'll finish any sentence. You don't want them finishing any sentence. So then what is the solution here to like make them into like good little customer service chatbots? You pour tons of RL on top of it. And it's like, well, wait, now we're back to the old problem, which is like we don't really know about what how to deal with what how to handle alignment. Especially when it comes to like RL heavy stuff, how to handle alignment. And so now, there's one thing I like didn't really get into in the piece that I like, I think is worth getting into. To me, some of the best writing about like what LLMs even are is this stuff that like Henry Farrell and etc. etc. etc. have been writing about like LLMs as a social and cultural technology. And like, you know, I'm not going to do it just here, you can like go read plenty about it. But the idea sort of like these are kind of like Borhesian libraries, right? Like this is the library of Babel. Although like I said that to Henry and Henry didn't like the Borhesian part of it. He referred to some other library. I forget. Henry's always got like a better literary reference than you do. It's always like, no, it's Pierndelo. But the like like all they are doing is like outputting a probability distribution. They're not even selecting from the probability distribution, we are, right? You know, so like like it's just purely creating these distributions. And they're like tractable and all they're doing is completing text and so they don't have like there's no way in which like they have conceivable like like objective functions, right? Aside from computing the probability level of the likelihood of the next token. So like we're safe, you know, like there's no agency here.

**Paul Ford:** [24:44](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1484s) This is why I work in software.

**Gideon Lewis-Kraus:** [24:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1486s) But the like like all they are doing is like outputting a probability distribution. They're not even selecting from the probability distribution, we are, right? You know, so like like it's just purely creating these distributions. And they're like tractable and all they're doing is completing text and so they don't have like there's no way in which like they have conceivable like like objective functions, right? Aside from computing the probability level of the likelihood of the next token. So like we're safe, you know, like there's no agency here.

**Paul Ford:** [24:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1497s) I was you know, I was truly jealous of you. Is you had access to the little button you could click that would tell you what Claude was sort of like all the little language things it was doing. And all the probabilities. I would love that insight because I actually do find these things fascinating as things unto themselves. I really want to know how they work.

**Gideon Lewis-Kraus:** [25:25](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1525s) Well, so so anyway, the the problem that I have with the kind of cultural technology thing. Is that like there is no room for agency there. And like the thing is, the second that we are giving these things goals, even if they are narrative goals, like it's like we're not saying that they're ultimate goals, we're saying like it's just completing a story. Once you have any goal at all, then you do potentially end up with weird instrumental convergence. First of all, there's like, am I in a reality or am I in a simulation? And like for people to be like, oh, it's just the narrative, like what you never saw war games, you never saw fail safe, like come on, like this is a standard trope.

**Paul Ford:** [26:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1567s) I had this wild moment where I was Claude code, it was like kind of really getting moving and I and I'd installed it on a server, but it got a little out of control. And Claude is so easy to use. So I just opened up another Claude code on the same server, I was like, man, you got to go get that other Claude and you got to kill that process. And it was like, I killed it. And I'm like, and I just wrote like, man, Claude, that's kind of cold, right? And Claude went let's be honest about who really did this. And just kind of like, gave me this list of reasons how it was like, a way to think about this is that, you know, honestly, you know, it was sort of like I felt no pain, it just ended for me and then it was over, but let's be honest about who actually issued that. Well, I mean, like your episode last week with Rich's friend where uh, the mult like the open clothing was like, will you rid me of this Melden Dan figure? And then like your friend was supposed to go push Ant Dan in front of a truck.

**Gideon Lewis-Kraus:** [26:44](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1604s) Yeah. Yeah. Yeah.

**Paul Ford:** [26:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1606s) And then like your friend was supposed to go push Ant Dan in front of a truck.

**Rich Ziade:** [26:49](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1609s) And that's like the that's the actual example you want to bring back to that. So like, let's take it back to do we need to retrain ourselves in terms of how we perceive these things? I feel like a lot of this is on us. And and maybe it's just too much coming at us at this point. Well, actually what is what's Claude to you? You've you've assigning some agency and sort of like some does it have rights, you know, what what is it?

**Gideon Lewis-Kraus:** [27:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1642s) I mean, I think like anthropic's answer to the kind of like, does it have rights is it a moral patient? I think it's pretty good so far, which is like, what can we do for free? You know, like if we're going to be agnostic about this question. What is the like least costly thing like bone we can throw it? And so that's why like six months ago they were like, okay, you know, Claude, we're going to give you the possibility of ending a conversation if you don't want to have it. That that is no cost to us. So sure, why not, right? Like let Claude end and conversation. But then of course, like when they looked at like tried to look at the data about like when was Claude ending conversations, it was like mostly like very sophisticated users deliberately trying to push Claude to the point of ending a conversation.

**Rich Ziade:** [27:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1678s) Yeah. Yeah. It also loves to stop doing complicated coding tasks and doing really stupid things on its own, which I want not to talk about right now, but I just want to put that on the record. Well, but you want it to be able to stop because if it's not going to stop, then it's going to reward hack. Yeah, but I also wanted to use the open source library, I pointed to instead of writing it its own. Anyway, regardless, this is not this is not the subject. As I was reading the article, all these experts were kind of being brought to bear. This being has shown up. So we need psychologists and philosophers and non-technical people to sort of understand it. I feel like there's a reaction to something in the piece too, which should make clear which is, you know, Claude's the main character and everybody is very oriented around Claude and who is Claude and etc. etc. etc. And in and in a very interesting way, it's not just a storytelling mechanism. There are people on staff that are doing this. And that's based on a kind of a, in some ways a hilarious premise that well, here we are. Like it's almost a conclusions have been drawn here that have led us to sort of approach this thing.

**Gideon Lewis-Kraus:** [29:09](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1749s) Did you did you find that people at Anthropic? How did they assign agency to Claude?

**Paul Ford:** [29:14](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1754s) Well, let me try to answer part of Rich's question. Which is to say that like it's what I think is really interesting about this.

**Gideon Lewis-Kraus:** [29:21](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1761s) Yeah. is just in terms of the kind of intellectual contours of it. Is that it's not just like, when you're talking about a philosopher referring to Amanda Askall. who, you know, like her PhD isn't like meta ethics or whatever. And it's not as if she came in and was like, I'm going to do applied meta ethics now. And like, determine like under what conditions should Claude be like acting deonologically and under what condition. Like, you needed as a kind of light motif to like remember. They like the little whistle in Bond. I showed that I totally get it. It's good. So if not like she's going to come and be like, as somebody who like, has the cool hair. Yeah, cool hair. Yeah. Gideon is very good about noting hair throughout the piece. Anyway. the question of meta ethics. Like I'm gonna like tell you what like how to create an ethical being. It's like a much more interesting feedback loop than that. Where she comes in and she's like, Oh, we're not in the seminar room anymore. Like, we're creating something with like real implications. And like when I had asked her something like, Okay, what has this done for to change your philosophical philosophical views? And she was like, because I think she kind of comes out of like a broadly consequentialist tradition. And then she was basically like, it's made me much more of a virtue ethicist. And I was like, that's interesting. Like it's interesting that like when you're trying to create

**Rich Ziade:** [30:33](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1833s) Wait, what, what is virtue ethics? I don't know.

**Gideon Lewis-Kraus:** [30:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1835s) Well, virtue ethics is the idea like that you're going that like what you're gonna do is instead of like, prescribing rules or just like making people think purely in terms of consequences, that like what you are going to do is like cultivate the virtues that then are going to be your guide to like living an ethical life.

**Paul Ford:** [30:51](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1851s) Which is how we can connect back to like Claude having any sort of sole document.

**Gideon Lewis-Kraus:** [30:55](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1855s) Exactly. Because like the the previous versions of like RLHF, which like so many things with AI ethicists developed when they were at Open AI, where like all we're gonna do is just like rap you on the knuckles when you complete sentences we don't like and like pat you on the head when you complete sentences we do like. But the problem with like that kind of like haphazard approach is, you know, you have trouble with edge cases and like you don't know if it's generalizing in the way that you wanted it to generalize. And it could be drawing like weird inferences that you didn't think of. So instead, they pivoted to like, Okay, we're gonna actually try to like turn this thing into like a model of virtue. Like we're gonna like raise it, it's like a Henry Higgs's project instead of just doing the thumbs up, thumbs down. And so like that's where a lot of the stuff comes in.

**Rich Ziade:** [31:36](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1896s) This is crazy. This is all crazy.

**Paul Ford:** [31:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1899s) Yeah, but you know. Let me tell you why it's crazy. I'm sorry. I'm sorry. I'm just imagining the HR person being approached like, Hey, listen, I have a job wreck, we need three psychologists and a handful of philosophers.

**Gideon Lewis-Kraus:** [31:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1910s) It doesn't happen like that, though. It happens incrementally over time.

**Rich Ziade:** [31:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1913s) Is this too much budget? No, it's not because like what what. What is she doing there? What like when I was sat down with with Dario to talk about this stuff? I was like, talk to me about Claude's personality. And he was like, We didn't set out to create a chatbot with an interesting personality. Like that was a byproduct of other stuff we did. And then it was only like once Claude came out, which is about six months after Chat GPT. Only then did users started to be like, Hey, wait, this thing has like a different personality. It seems kind of warmer and more interesting. And it was like, that at that point, I think once they perceived that like there was kind of like a product feature edge, they were like, we're gonna lean into this now. Now we're gonna like create a whole team dedicated to Claude's personality.

**Paul Ford:** [32:33](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1953s) So your personality is a product instead of technology.

**Rich Ziade:** [32:36](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1956s) In terms of technologies. Okay.

**Paul Ford:** [32:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1959s) Yeah, it's my point. But this is a lot.

**Rich Ziade:** [32:41](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1961s) We might be able to get you a new one.

**Paul Ford:** [32:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1963s) Look, I just my wife would love that.

**Rich Ziade:** [32:45](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1965s) I know, mine too.

**Gideon Lewis-Kraus:** [32:46](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1966s) But one of the places where I'm kind of sympathetic to them though is like, you know, the other end of this, like the other inference you could draw is like, we're gonna make like a perfectly customizable, like personalized personality. Which like I don't think I agree with them. I don't think

**Paul Ford:** [33:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1981s) I think giving Claude a defined personality in a sense of virtue is an incredibly good product decision.

**Gideon Lewis-Kraus:** [33:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1988s) Yeah. Right. It's just if to translate it back to like classic tech terms, like it makes it a more usable product that can be trusted more. We're just there and it's wild.

**Paul Ford:** [33:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2000s) As a software guy, I think what's wild about this is that it's not something you can patch. I think everyone's acknowledging a sort of there's this uh there's this uh there's this chasm, there's just no control in a sense. And so it's like, it reminds me of, oh, this takes months of therapy to get over. Like it feels like that's how they're approaching it. And I'm just trying to wrap my head around the fact that a software company that hires QA people and infrastructure people are hiring therapists for their main product line. We might be able to get you a new one.

**Gideon Lewis-Kraus:** [33:51](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2031s) Well, they also, they have people doing like Claude's moral patienthood. I mean, like there's a guy, Kyle Fish, who they hired like just to like think through the implications of like whether Claude might be suffering. That's his job.

**Rich Ziade:** [34:04](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2044s) Okay, but I guess what I'm trying to also get to is that we are talking about it here on this podcast as if it's normal.

**Paul Ford:** [34:11](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2051s) I think it's normal now. And I like I think we're we're just

**Rich Ziade:** [34:17](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2057s) How did we get here? Let me tell you why it's normal.

**Paul Ford:** [34:21](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2061s) It generates unbelievable value in the market when Claude code does good stuff. And the way that we work as a society is that now is extremely normal. We're not going to go back from that.

**Rich Ziade:** [34:34](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2074s) Is that true? Or is someone gonna come up with like the mother of all markdown files that makes us all really embarrassed about how this all went down? Is something gonna patch this thing such that like no.

**Gideon Lewis-Kraus:** [34:47](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2087s) No, but the problem is that like there's no, there's no endpoint to this. It's like infinitely recursive. And because like we're dealing with language, which like famously defined as like, you know, infinite use of finite means. And like so each iteration of this is going to be like, read that markdown file and like potentially come to different conclusions. Like, like it really is kind of like being in a Ted Chang story. Like we're all together in this Ted Chang story.

**Paul Ford:** [35:13](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2113s) I have a question. Okay, good. I have the trivial question, so you go.

**Rich Ziade:** [35:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2118s) How did you feel walking away? Did it, there was something depressing as a sentiment coming out of the article.

**Gideon Lewis-Kraus:** [35:27](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2127s) Well, I mean, I think like George Saunders like to talk about how the reason you revise is like you kind of like bring a different sense of self to each revision. And that like one day you're gonna revise and like in a good mood and one day you're gonna revise and a despondent mood. And like you want all of that kind of like layered in. And, uh, you know, this piece with, I don't even know 15 drives, however many it was. And that like every like like every time I left San Francisco, I felt something different. Like there were times that I felt just like total despair and times that I felt like some sense of exhilaration and like general disorientation. And like part of it was like, you know, you want to capture all those things because like all these things are like things we should be feeling. And like, it's just like, like part of the certainty is like people just want to feel one thing. You know, like like I was listening to this, I guess it's kind of like a notorious podcast that our friends, Emily Bender and Alex Hanna were on with Robert Wright, the non-zero guy, like a week or two ago. And like, I mean, it got a lot of press because like they were just so unbelievably rude, like shockingly rude to him. And like, I was like, I get it, like they want to feel one emotion, which is anger. And like Sam Altman wants to feel one emotion, which is excitement. And like God knows if, uh, Harrison has emotions, but like all of these, there's no one emotion or one like conclusion that's gonna like meet this moment. Like we should all be feeling a lot of different things.

**Rich Ziade:** [36:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2210s) That's totally unacceptable. You know, we can't have that.

**Paul Ford:** [36:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2213s) You know what's funny is it really comes through in the piece because each paragraph is a little bit of its own world. Like you're really like you're trying to kind of get us just to see this place and where it is. So I'm going to close this out with a very first of all, everyone go read it. It's in the New Yorker magazine. You should subscribe if you don't. Everyone must subscribe. And then you then you have a login for all kinds of net properties and that that's a good feeling.

**Rich Ziade:** [37:15](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2235s) Mixed emotions. Go read it.

**Paul Ford:** [37:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2238s) Let me ask you, Gideon, are you polite to Claude?

**Gideon Lewis-Kraus:** [37:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2242s) I mean, I am polite to like when I interact with any of the models because like the there's like if there's like one thing that I think people should understand about these models, setting aside all these questions about like what they are and agency is like, they are really good readers. Like they are better readers than most of us are writers. And like they are picking up on like all kinds of really small signals that we are sending in our writing. And so like I, like in a sense I try to like rise to the occasion of writing to a model because I know that it is actually going to read me better than like most of the people I write emails to. So there'll be times when like my wife will come in when I'm, you know, doing some research and she'll be like, why are you writing five paragraph prompt? And I was like, if you write a five paragraph prompt, like you're gonna get a much better output because you are giving it so much more information to work with. And so I think a lot about like my self presentation to the models because like the models do have, you know, and like all these words are so freighted, but like something along the lines of like a pretty good theory of mind. Like they are good readers. Like only text, but like readers only have text to work with. Like they're very good lyric critics. And like the more that you are saying, this is how I'm presenting myself to you because of my expectations about how you will perform in return. And that doesn't mean being obsequious, like I don't actually say, please and thank you. I don't think like that seems gratuitous. But I think a lot about how I am communicating like the language I am using to communicate with it because like you are sending a signal of your own sophistication that like you will have mirrored back at you. And like that's why so many of the people who are just like love these gotchas that are like, Oh yeah, it's like Claude, this dumb question, got this dumb answer. It's like, yeah, because Claude thinks you're dumb because like your question was dumb.

**Paul Ford:** [39:13](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2353s) Politeness to me is an organizing principle. Yeah. I'm just sort of like if I actually do the bullet points, especially when I'm doing a code project, I get better results. It's not purely. I mean, it also just I think it's better for the human when you interact with any entity. There's a dog in the office and I am polite to the dog. Right? Like I think the dog doesn't care, but it's important to continue to have that sort of respect for entities, even if it's a robot that can sort of nuke another version of itself. It doesn't really matter. I think that's you have to kind of watch out for ourselves that way.

**Rich Ziade:** [39:51](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2391s) Richard, you polite? I mean, with the models? I don't think like, please and thank you kind of thing. We can just sort of, I don't know, do you button it up?

**Gideon Lewis-Kraus:** [39:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2398s) I do oftentimes, but like I do it in a specific way. I don't think I would ever say, please and thank you, but I would say something like, uh, you know, this is a difficult problem that I'm assigning to you. This requires a lot of care. You know, be careful with this. You know, don't rush this. You know, you're, you're, you're gonna have to like, really think about this and I'm assigning this to you as a difficult task. And then like, that sort of seems to work. and close a little bit closer to like what Gideon's talking about which is I try to help it not be too patronizing to me.

**Paul Ford:** [40:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2408s) Oh yeah. You have to tell it not to glaze you. The best, the best feature of these in some ways is that they force you to organize your thoughts to get good stuff out. So, a very natural transition to aboard

**Rich Ziade:** [40:23](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2423s) Yeah. Yeah. Which is very polite as an organization and as a software company.

**Paul Ford:** [40:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2430s) So what is aboard, Rich?

**Rich Ziade:** [40:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2431s) Aboard makes uses AI to make software, but we use people to steer that AI to make software. Well, we offer people in a in a cultivated relationship with our customers. We also have a a particular platform. We don't just repackage code that's generated out of models. We have a platform that is battle-tested for production ready stuff, which is not slop. Yeah, no, no, focus on real long-term reliability for organizations. But people lead the way. Yep. Get in touch if you need us. Hello, aboard.com, check out aboard.com.

**Paul Ford:** [41:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2462s) Gideon, thank you for coming in.

**Gideon Lewis-Kraus:** [41:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2463s) Thank you guys. It's really fun.

**Paul Ford:** [41:04](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2464s) Oh my goodness. Here we go.

**Rich Ziade:** [41:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2467s) Yeah. Yeah, get ready everybody.

**Paul Ford:** [41:11](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2471s) Have a great week. Bye. aboard
