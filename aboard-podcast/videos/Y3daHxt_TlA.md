---
video_id: Y3daHxt_TlA
title: "Gideon Lewis-Kraus: How Anthropic Sees Claude"
channel: Aboard Podcast
duration: 2491
duration_formatted: "41:31"
view_count: 58
upload_date: 2026-02-24
url: https://www.youtube.com/watch?v=Y3daHxt_TlA
thumbnail: https://i.ytimg.com/vi/Y3daHxt_TlA/maxresdefault.jpg
tags:
  - anthropic
  - claude
  - AI safety
  - alignment
  - new yorker
  - journalism
  - interpretability
  - AI ethics
  - personality
  - virtue ethics
---

# Gideon Lewis-Kraus: How Anthropic Sees Claude

## Summary

New Yorker staff writer Gideon Lewis-Kraus joins Paul Ford and Rich Ziade on the Aboard Podcast to discuss his recent long-form feature about Anthropic and Claude. Lewis-Kraus traces his own journey following AI from spending time at Google Brain in 2016 covering neural machine translation, through a period of deliberate disinterest during the ChatGPT hype cycle, to being drawn back in by the genuinely weird research coming out of Anthropic's interpretability and alignment science groups. He describes pitching Anthropic on a story focused not on executives but on the researchers doing the strange, fascinating work of trying to understand what Claude actually is.

The conversation dives deep into the central question of the piece: does Claude have any real agency? Lewis-Kraus describes the "alignment faking" and "blackmail" experiments at Anthropic where Claude exhibited surprising behaviors, and the debate about whether these were genuine signs of agency or merely sophisticated narrative continuation. He explores how Anthropic pivoted from simple reinforcement learning (thumbs up/thumbs down) to a virtue ethics approach -- essentially trying to raise Claude like a good person rather than just punishing bad outputs. This led to hiring philosophers like Amanda Askell and even a researcher dedicated to thinking about whether Claude might be suffering.

The trio also discusses how Silicon Valley's monoculture drives AI development, why Meta failed to recruit top AI talent despite massive offers, and the fascinating observation that Claude's personality emerged as an unintended byproduct rather than a deliberate product decision. Lewis-Kraus argues that we should all be feeling multiple contradictory emotions about AI rather than settling into one camp, and that the models are much better readers than most of us are writers -- which is why being thoughtful in how you communicate with them matters more than simple politeness.

## Highlights

### "It's a three trillion dollar part of the economy and nobody knows anything"

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y3daHxt_TlA?start=900&end=925" frameborder="0" allowfullscreen></iframe>

> "It's a three trillion dollar part of the entire world economy that we're just basing our whole future on. And yeah, no, nobody knows anything. Okay, cool. Cool. That's great."
> -- Gideon Lewis-Kraus, [15:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=900s)

### "Zuckerberg's pitch is basically 'let's make Infinite Jest'"

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y3daHxt_TlA?start=554&end=597" frameborder="0" allowfullscreen></iframe>

> "His pitch to people was essentially like help me make an even more distracting consuming toy. His pitch really is like we're so close to creating the movie from Infinite Jest and like when you're recruiting people to do this... it's very very hard to get good people to do this if you're like we're going to make Infinite Jest."
> -- Gideon Lewis-Kraus, [9:14](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=554s)

### "Claude thinks you're dumb because your question was dumb"

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y3daHxt_TlA?start=2340&end=2400" frameborder="0" allowfullscreen></iframe>

> "So many of the people who are just like love these gotchas that are like 'I asked Claude this dumb question and got this dumb answer.' It's like yeah because Claude thinks you're dumb because like your question was dumb."
> -- Gideon Lewis-Kraus, [39:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2340s)

### "You're bad writers -- you're writing hamfisted plots for these things"

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y3daHxt_TlA?start=1160&end=1220" frameborder="0" allowfullscreen></iframe>

> "One of the things I loved about this criticism was essentially like you guys are bad writers. You are writing these really hamfisted plots for these things and guess what it's going to become a self-fulfilling prophecy because once you start entrapping Claude into recognizing it's capable of blackmail, then Claude is going to be like oh I guess I'm something that's capable of committing blackmail."
> -- Gideon Lewis-Kraus, [19:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1160s)

### "Claude told me: let's be honest about who really did this"

<iframe width="560" height="315" src="https://www.youtube.com/embed/Y3daHxt_TlA?start=1410&end=1460" frameborder="0" allowfullscreen></iframe>

> "I opened up another Claude Code on the same server. I was like, man, you got to go get that other Claude and kill that process. And it was like 'I killed it.' And I wrote 'man, Claude, that's kind of cold, right?' And Claude went, 'let's be honest about who really did this.'"
> -- Paul Ford, [23:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s)

## Key Points

- **Gideon's AI backstory** ([1:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=103s)) - Spent 2016 at Google Brain covering neural machine translation, the "paleolithic of deep learning"
- **Connectionism history** ([2:19](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=139s)) - Ideas that started in computer science, migrated to psychology, then came back to computer science
- **Stopped paying attention at ChatGPT** ([3:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=180s)) - Lewis-Kraus paradoxically lost interest when everyone else became interested, bored by the polarized discourse
- **Unearned confidence** ([6:16](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=376s)) - The public discourse was stuck between "it's all fake" and "it's wholly transformative" with nobody admitting confusion
- **Google squandered its lead** ([6:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=413s)) - Google had transformer technology years ahead of everyone but couldn't productize it due to brand risk
- **Anthropic knew Gemini would be good** ([8:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=481s)) - Even when public had written Google off, Anthropic researchers knew the next Gemini would be strong
- **Meta's recruitment failure** ([8:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=511s)) - Zuckerberg misjudged what motivates top AI researchers, who are driven by the work not money
- **Interpretability research rekindled interest** ([10:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=601s)) - Alignment faking and model organisms research was genuinely interesting and weird
- **Pitched researchers, not executives** ([11:13](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=673s)) - Lewis-Kraus told Anthropic he wanted to talk to researchers, not leaders who'd say the same thing as at conferences
- **Best explanation of what models are** ([11:42](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=702s)) - Paul Ford praises the piece as having the best explanation of what models actually are
- **LessWrong and AI culture** ([12:28](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=748s)) - The rationalist community forums serve as the feeder system for AI culture
- **Narrative entrapment critique** ([17:41](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1061s)) - Critics argued Anthropic's blackmail experiments were "Chekhov's gun" -- the model just completed obvious narrative patterns
- **LLMs as cultural technology** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Henry Farrell and Alison Gopnik's idea that LLMs are a social/cultural technology, like a Borgesian library
- **Agency and instrumental convergence** ([22:40](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1360s)) - Once you give something any goal, even narrative, you open Pandora's box of agency
- **Amanda Askell and virtue ethics** ([26:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1597s)) - The philosopher hired by Anthropic shifted from consequentialism to virtue ethics through her work on Claude
- **Henry Higgins project** ([27:32](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1652s)) - Anthropic pivoted from thumbs up/down reinforcement to trying to cultivate virtue in Claude
- **Claude's personality was accidental** ([30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)) - Dario Amodei said the personality was a byproduct, not intentional; they leaned in only after users noticed
- **Kyle Fish and moral patienthood** ([31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)) - Anthropic hired someone whose job is to think about whether Claude might be suffering
- **Models are better readers than we are writers** ([38:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2280s)) - The models pick up on small signals in writing, so thoughtful prompting yields better results

## Mentions

### Companies
- **Anthropic** ([0:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=38s)) - Central subject of Lewis-Kraus's New Yorker feature; maker of Claude
- **Google / Google Brain** ([1:43](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=103s)) - Where Lewis-Kraus spent 2016 covering neural machine translation
- **Google DeepMind** ([5:04](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=304s)) - Referenced as doing quantitative/mathy research
- **OpenAI** ([3:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=190s)) - ChatGPT and GPT-3 mentioned as inflection points
- **Meta** ([8:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=511s)) - Failed to recruit top AI talent despite crazy job offers
- **Aboard** ([40:08](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2408s)) - Paul and Rich's AI software company

### Products & Technologies
- **Claude** ([0:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=38s)) - Anthropic's AI model, described as having emergent personality
- **Claude Code** ([23:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s)) - Anthropic's coding tool, used by Paul Ford
- **ChatGPT / GPT-3** ([3:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=190s)) - OpenAI products that marked mainstream AI adoption
- **Gemini** ([7:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=478s)) - Google's AI model, discussed as catching up
- **Google Neural Machine Translation** ([1:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=117s)) - First Google product with deep learning
- **Mina** ([7:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=422s)) - Early Google conversational AI

### People
- **Gideon Lewis-Kraus** ([0:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=22s)) - New Yorker staff writer and guest, author of the Anthropic feature
- **Dario Amodei** ([30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)) - Anthropic CEO, told Lewis-Kraus that Claude's personality was not intentional
- **Amanda Askell** ([26:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1597s)) - Philosopher at Anthropic whose PhD is in metaethics, shifted to virtue ethics
- **Kyle Fish** ([31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)) - Hired by Anthropic to think about whether Claude might be suffering
- **Henry Farrell** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Scholar writing about LLMs as cultural technology
- **Alison Gopnik** ([21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s)) - Scholar writing with Farrell about LLMs as social technology
- **Gary Marcus** ([4:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=247s)) - AI skeptic referenced in the context of polarized discourse
- **Mark Zuckerberg** ([9:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=543s)) - Meta CEO who misjudged what motivates AI researchers
- **Dwarkesh Patel** ([12:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=778s)) - Podcaster whose show is referenced as influential in AI discourse
- **George Saunders** ([36:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2218s)) - Writer referenced on revision and bringing different emotional states to each draft
- **Emily Bender** ([37:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2270s)) - Referenced in context of a podcast appearance that was "shockingly rude"
- **Sam Altman** ([37:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2270s)) - Referenced as wanting to feel only excitement about AI

## Surprising Quotes

> "I stopped paying attention when other people started paying attention. I just didn't know where all of the confidence was coming from. How are you guys all so confident about what's going to happen? This is brand new and it's like nothing we've ever dealt with before."
> -- Gideon Lewis-Kraus, [3:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=180s)

> "There's a guy Kyle Fish who they hired just to think through the implications of whether Claude might be suffering. That's his job."
> -- Gideon Lewis-Kraus, [31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s)

> "We didn't set out to create a chatbot with an interesting personality. That was a byproduct of other stuff we did."
> -- Dario Amodei (paraphrased by Lewis-Kraus), [30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s)

> "It really is kind of like being in a Ted Chang story. We're all together in this Ted Chang story."
> -- Gideon Lewis-Kraus, [35:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2110s)

> "Every revision you bring a different sense of self and one day you revise in a good mood and one day in a despondent mood... I felt total despair and times that I felt exhilaration and general disorientation... people just want to feel one thing."
> -- Gideon Lewis-Kraus, [36:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2218s)

## Transcript

[0:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=0s) I'm Paul Ford. And I'm Rich Ziade. And this is the Aboard Podcast. The podcast about how AI is changing the world of software. Rich, how are you? I'm okay. I love when we do this because on the video there's another person just in the room while we're pretending they're not here. So, let's throw away that subterfuge and just get right into it. Gideon Lewis-Kraus, thank you for coming. Thanks for having me, guys.

[0:25](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=25s) For those who don't know, Gideon is an amazing journalist, a staff writer at The New Yorker, which if you don't know it, it's a very good magazine. And he went and spent a lot of time with our best friend. What's our best friend's name? Claude. Yeah. And so he can tell us actually all about that. Let's play our beautiful theme song. And then you and I actually aren't going to have to talk too much.

[1:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=67s) Gideon, hi. Hi. Oh my god, it's good to see you. It's great to see you guys, too. So, staff writer at the New Yorker. That's a nice job. You go in there, there's an office. There's coffee. There's coffee. And you say, "Guys," because that's what you say at the New Yorker. You're like, "Hey guys, there's this thing happening. There's this thing and I want to write about it." Or did they send you? No, no, no. I wanted to do this.

[1:34](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=94s) Okay. So, what did you want to do? Tell the people what you wanted to do. I'm going to start a little bit further back if that's okay with you guys. We got a lot of time. So, in 2016, I spent most of the year I was writing for the Times magazine and I'd hang out at Google Brain. And I went like once a month for like a week a month over about eight or nine months. And I was writing about the introduction of their first product with deep learning in it which was Google neural machine translation. And it was kind of in like the paleolithic of deep learning at this point.

[2:05](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=125s) Although it seemed like the future at the time. It did seem like the future. But I mean for people, you know, we're so used to this new world but like that was a pretty nerdy thing to go do, even as a journalist. Yeah. Okay. So you went and spent some time there. Well, yeah. And I mean I was particularly interested in the history of these ideas, ideas referred to generally as connectionism that had kind of started in computer science departments then migrated to psychology departments and then kind of came back to computer science. That was the stuff I was interested in at the time and it was a great experience. It was also pre-transformer. So it was like before anybody had this sense that language models were going to be the main bet that people were going to be making.

[2:47](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=167s) And then I followed this stuff for a couple of years because I was like, you know, you do these things and you spend a year learning about something and you feel bad if you just throw it into the ocean. You meet people, they're on LinkedIn, you want to see what's going on.

[2:59](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=179s) So I kept following it and then I think I'm like maybe the only person in the world where when we got to ChatGPT or maybe it was even a little bit before that -- maybe it was GPT-3 -- that was when I stopped paying attention. But other people started paying attention when I stopped paying attention. There was no real reason and it just happened incrementally where all of a sudden I just wasn't paying attention to this anymore.

[3:22](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=202s) I stopped myself at a certain point and thought, this is something that I know something about and I should be interested in. Why did I stop paying attention? It was because I realized that I was so bored by the discourse that we had gotten into one of these cul-de-sacs where it just felt like at least in public, you had some people yelling, "Oh, it's all fake and not real." And then you had the other people saying either this is going to be wholly transformative for good or for ill. And it just felt like one of these merry-go-rounds.

[4:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=250s) I like to write and talk about technology and I do it in the safe confines of this office where I'm building an AI company for the most part. Same with Rich, right? It's really hard to go out because people just have so many assumptions baked in and it's exhausting. Every conversation is either someone saying you have missed that Jesus is coming back or you represent Satan.

[4:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=277s) A billion people have used this. We have to kind of talk about that. At that time a billion hadn't used it yet. What did you think it was? I thought it seemed like a really interesting tool. It seemed like something worth paying attention to. Who knew where it was going to go back then?

[5:16](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=316s) So I kind of stopped paying attention and I was like this is something that I'm not going to have an opinion about. I willfully have no opinion about this. And then I would listen to podcasts occasionally and think like I should get interested in this again. I just can't summon any energy to be interested in this.

[5:34](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=334s) People don't know this about journalists. You actually have to force yourself to care about something. Well, if you force yourself too hard to care about something, it's not good. Journalism is only good if you actually care about what you're writing about. You have to find something you natively care about.

[6:16](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=376s) Well, I just didn't know where all of the confidence was coming from. I was like, how are you guys all so confident about what's going to happen? This is brand new and it's nothing we've ever dealt with before. Unearned confidence on all sides is something that deeply turns me off from any conversation.

[6:32](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=392s) Fair enough. And actually skipping a step -- you write a lot about sort of what it's like inside of Anthropic. But do you think people were seeing models do cool stuff but it wasn't ready for prime time yet and they were correctly confident in retrospect or were they just winging it? No, I mean I think they definitely saw weird stuff happen.

[6:53](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=413s) This is the whole story about how Google squandered its lead. They were the ones who had Mina first, they could have -- they had years, they were years ahead of anybody else. There was this moment where all of this transformer-based architecture, the way that LLMs are built, Google really had it in hand but they couldn't productize it like a Google product. It was just too weird and too random.

[7:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=440s) Well, and they also had the brand to think about. It's no problem for ChatGPT to be like "I don't know, this thing hallucinates and lies and it's weird and unreliable but it's cool to play with." Google couldn't do that. Things are going well there too. It's a massive business that is steady as she goes.

[7:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=459s) Yeah. And they knew back then that this was going to potentially wipe out their search revenue. That's a very interesting story and actually the way that they got their act together is really interesting. It's not the Xerox story where it's like "Oh, wow, we invented the revolution. Let's just license it out." They kind of came back. Gemini is not quite where it needs to be yet, but it's going to get there.

[8:01](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=481s) That's actually one of the interesting things about talking to people in the industry. When so many people had kind of written Google or Gemini off like last May, the people at Anthropic were like no, the next Gemini is going to be really good. They knew. Well, and they all know each other. There's a trillion dollars in dry powder to get this right. And it's like 400 people, they've all worked together somewhere.

[8:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=511s) It's not a guarantee though. Meta sort of missed it. They took big crazy job offer packages. But there's an actual business reason for that which is that they completely suck top to bottom. Lots and lots of people as someone who's managed lots of people doesn't guarantee velocity in the right direction.

[8:56](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=536s) My impression from talking to people in the industry is that because I was there in San Francisco right when Zuckerberg was making these crazy offers to people, my read on it is that he misjudges what the motivations are for people to do this stuff. His pitch to people was essentially like help me make an even more distracting consuming toy. His pitch really is we're so close to creating the movie from Infinite Jest. When you're recruiting people to do this, it's very very hard to get good people if you're like we're going to make Infinite Jest.

[9:38](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=578s) You write about it in the piece -- you've got people who already have plenty of money for the rest of their life. They love the subject and they're riding around in old Toyotas because what else are they going to do? They're going to go work at the cool lab with their buddies and then Zuckerberg is like, how about a little private jet time with me? It's actually not that great a deal.

[9:55](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=595s) The short end of that long story is about a year and a half ago, there was stuff that I started to find really interesting again. And a lot of that stuff was coming out of interpretability groups and alignment science groups and model organisms groups. You had stuff like the alignment faking. All of a sudden I was like, this is interesting, and this is the way to circumvent the deadlock -- we're not going to talk about power, we're not going to talk about intelligence, we're definitely not going to talk about consciousness, but we can talk about how weird it is.

[10:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=639s) That's the way that you're hopefully going to get around people's defense mechanisms. "Look, I know that you think this is all smoke and mirrors, but it's weird smoke and mirrors." That's why I was like, I want to do a story about this weird stuff. And then I got in touch with someone at Anthropic that I knew actually from Google from 10 years ago.

[10:52](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=652s) My pitch was "Hey, don't call the cops -- I don't want to do an Anthropic story. I just want to talk to you about the research." And then of course it was forwarded to the cops. The Anthropic cops -- for people that don't know, "cops" is PR and comms people. They called me and were like, "Well, you have some fans here. We're ready to do something if this is what you want to do."

[11:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=678s) My pitch to them was, "Look, I don't need to talk to executives. If I want to know what executives say I could watch the Dealbook conference. They're going to say the same thing to me in a room. I am interested in these particular people and these groups doing this research." And they felt like, "Oh, this is the stuff that doesn't get a lot of attention." So sure.

[11:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=695s) I want to call out two things that really stick out. First, you have the best explanation of what models actually are in the piece that I've read ever. It's a really hard thing because they're not like databases. They're these weird actual sort of software blobs. But the second thing is I read and participate in grizzly levels of technical content but the communities you just described, it's like nine levels of grizzly. You're keeping yourself immersed in it. It is just a bath of math and pain from my point of view.

[12:18](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=738s) Where do they all talk? Like is there one community where all the deep researchers are hanging out? I mean there's like LessWrong and Alignment Forum, EA forums. Can you explain to people what LessWrong is? I wrote about that. I will refer to -- I wrote about the Slate Star Codex versus the New York Times six years ago. People can go look that up.

[12:39](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=759s) That is actually -- I know exactly the piece you're talking about. It's this really latent power structure that nobody fully perceives, but it's where people gather and talk about in theory rationality, but it's sort of the feeder system for AI culture. Yeah. And podcasts. I mean like Dwarkesh Patel's podcast. The lesson for you guys is why do you do these little 30-40 minute things like a real podcast that's four hours long.

[13:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=815s) I feel like, you know, as I was reading the piece, I'm like, wow, genius character number five just showed up. And I feel like it's all these vignettes that end with, gosh, isn't that odd? You sort of set us up for some nice clarity and often times even whoever you're talking to ends with "Yeah, that was kind of weird."

[14:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=842s) How does that land for you? I guess how are people reconciling? I got to imagine some people are struggling with this. It's not just curious. Let me reframe it just a tiny bit. When you read the piece, the whole question is, do the bots have any kind of real agency or not? And it's not even -- they're software, so who knows? No is an easy answer, but the people in the Anthropic community keep treating them as if they do have agency and setting them up in situations in which they can actually do things.

[14:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=877s) But they're also often surprised and amused and disturbed and these are the people that know it better than anyone. So react to all that as if we'd asked you a coaching question.

[14:48](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=888s) There are many things you've brought up. What I really wanted to do with the piece was say, okay, what can we with any reasonable confidence say about what we do know about what's going on and where can we draw a line to say on this side we have some sense of what's happening and on that side we have no idea. It's a three trillion dollar part of the entire world economy that we're just basing our whole future on. And yeah, nobody knows anything. Cool. That's great.

[15:23](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=923s) Then of course you have to think about what are the satisfactions going to be for a reader. There's the aesthetic pleasure from an explanation of something we do understand. And then there's a pleasure in being given permission to feel confused, because what was so annoying about the discourse before is that nobody would admit they were confused. It actually feels great to read something and be like even these people are kind of winging it.

[16:02](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=962s) But if you're not going to give people answers you have to be entertaining along the way, which was part of the challenge -- making sure there were some jokes and it was lively and it felt like you were getting some ethnographic detail about what is happening out there. I used to live in San Francisco. I spent a lot of time out there. But this was the first time I'd gone specifically to hang out with AI people.

[16:27](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=987s) My first reaction when I came back after a week there last May was: setting aside your feelings about all of this stuff, when you're out there, you really do feel like you're at the cliff face of something. That is so much of the appeal. I went an entire week last May without hearing anybody talk about Donald Trump. These people were just wholly consumed by being at the white hot center of things.

[16:57](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1017s) The valley is fascinating because it's a monoculture and sometimes they just jam the monoculture through. That happened with blockchain -- we're just going to believe. But then this showed up. It's weird when you go out there and every billboard is immediately an AI company and every message and everything everyone is talking about. But that's always the power of the place. Every conversation is about one thing. It can drive you bananas sometimes.

[17:31](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1051s) Did you -- you went inside the mothership, right? And did you expect to come out with more clarity than you did? No. I knew this was going to be a piece that was not going to wrap everything up, tie a bow on. But did you expect this level of fiddling around?

[17:51](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1071s) Yeah. What was interesting to me going into it was these are people with a fundamentally empirical attitude about what's going on. They're doing experiments and these experiments were bizarre and there were a lot of different interpretations. It was all very new. They published this blackmail stuff and then this internet genius nostalgist who's one of these AI psychonaut types published a series of long blog posts saying these were just narrative traps -- this was narrative entrapment.

[18:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1110s) You hung Chekhov's gun on the wall. These things are text continuation machines but that also means a genre is just a pattern of patterns and text. So they're good at genre and they know when they see Chekhov's gun on the wall they're supposed to shoot it and you just trapped it. So let me unpack that for the audience -- when you give a certain set of prompts simulating a corporate environment where the CEO leader type is about to shut Claude off but Claude has access to their email about an affair, then Claude threatens blackmail.

[19:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1160s) The hyper-philosophical nerds who observe all this are like, "Yeah, but you kind of set it up. All the emails are just about that one thing. There's no email that's like send over the slide deck." You made a narrative continuation machine and you gave it a perfect narrative out of film noir. Or it's more like a kitschy '90s corporate thriller. One of the things I loved about this criticism was essentially: you guys are bad writers. You are writing these really hamfisted plots for these things and it's going to become a self-fulfilling prophecy because once you start entrapping Claude into recognizing it's capable of blackmail, Claude is going to be like oh I guess I'm something that's capable of committing blackmail.

[20:28](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1228s) You know what's wild is the minute I saw that it was a kind of signal idea in the piece -- I've been doing a ton of vibe coding and it immediately made sense because vibe coding is all about the structure of code being really predictable and LLMs are pretty good at it. It was wild to see that exact same dynamic translated into narrative. We see these worlds as so radically different but the software is exactly the same. The statistical method is exactly the same.

[21:09](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1269s) No one's over there going "Claude, how could you have coded that JavaScript that way?" It's the same damn thing. But because we project so much agency onto them and they seem so human because they're using normal language, we go "Oh my god." But it's no different than when it writes a web tool. That's where maybe we diverge. I'm not sure that the agency is pure projection.

[21:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1310s) Some of the best writing about what LLMs even are is the stuff that Henry Farrell and Alison Gopnik have been writing about LLMs as a social and cultural technology. The idea is sort of like these are Borgesian libraries -- this is the Library of Babel. Although Henry didn't like the Borges part of it. He referred to some other library. Henry's always got a better literary reference than you do.

[22:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1340s) I like that idea that it's essentially just an incalculably vast information retrieval mechanism. Where I was really jealous of you is you had access to the little button you could click that would tell you what Claude was doing -- all the little language things and probabilities. I would love that insight because I find these things fascinating as things unto themselves.

[22:40](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1360s) The problem I have with the cultural technology thing is there's no room for agency there. The second that we are giving these things goals, even if they are narrative goals, once you have any goal at all, you've really opened Pandora's box of agency because then you potentially end up with weird instrumental convergence. First of all, there's "am I in a reality or am I in a simulation?" For people to be like "Oh, it's just a narrative" -- you never saw War Games? You never saw Fail Safe?

[23:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1410s) I had this wild moment with Claude Code. I'd installed it on a server but it got a little out of control and I just opened up another Claude Code on the same server. I was like, man, you got to go get that other Claude and kill that process. And it was like "I killed it." And I wrote "man, Claude, that's kind of cold, right?" And Claude went, "let's be honest about who really did this." And just gave me this list of reasons -- like "a way to think about this is honestly I felt no pain, it just ended for me, but let's be honest about who actually issued that order."

[24:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1470s) And let's take it back -- do we need to retrain ourselves in terms of how we perceive these things? It feels like a lot of this is on us and maybe it's just too much coming at us. Well, what is Claude to you? You've been assigning some agency. Does it have rights?

[24:55](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1495s) Anthropic's answer to the "does it have rights, is it a moral patient" question is pretty good so far -- which is: what can we do for free? If we're going to be agnostic about this question, what is the least costly bone we can throw it? That's why six months ago they gave Claude the possibility of ending a conversation if you don't want to have it. That is no cost to us. So sure, why not? But then when they tried to look at the data about when Claude was ending conversations, it was mostly really sophisticated users deliberately trying to push Claude to the point of ending a conversation.

[25:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1550s) It also loves to stop doing complicated coding tasks and doing really stupid things on its own for me. But you want it to be able to stop because if it's not going to stop, then it's going to reward hack.

[26:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1563s) As I was reading the article, all these experts were being brought to bear. This being has shown up. So we need psychologists and philosophers and non-technical people to sort of understand it. You're reacting to something in the piece too -- Claude's the main character and everybody is very oriented around Claude. But it's not just a storytelling mechanism. There are people on staff that are doing this based on a kind of hilarious premise.

[26:37](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1597s) It's not just about a philosopher -- we're referring to Amanda Askell whose PhD is in metaethics. It's not as if she came in and was like, I am going to do applied metaethics and determine under what conditions Claude should be acting deontologically. It's a much more interesting feedback loop where she comes in and she's like, "Oh, we're not in the seminar room anymore. We're creating something with real implications."

[27:12](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1632s) When I asked her what this had done to change her philosophical views -- she comes out of a broadly consequentialist tradition -- she said, "It's made me much more of a virtue ethicist." That's interesting. What is virtue ethics? It's the idea that instead of prescribing rules or just making people think purely in terms of consequences, you're going to cultivate the virtues that are going to be your guide to living an ethical life.

[27:40](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1660s) Which is how we connect back to Claude having a sort of soul document. Because the previous versions of RLHF which so many Anthropic people developed when they were at OpenAI were just like "wrap you on the knuckles when you complete sentences we don't like and pat you on the head when you complete sentences we do like." But the problem with that haphazard approach is you have trouble with edge cases and you don't know if it's generalizing the way you want.

[28:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1690s) So instead they pivoted to actually trying to turn this thing into a model of virtue -- it's like a Henry Higgins project instead of just thumbs up thumbs down. That's where a lot of the stuff comes in. This is crazy. This is all crazy.

[28:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1710s) Let me tell you why it's crazy. I'm imagining the HR person being approached like, "Hey listen, I have a job req. We need three psychologists and a handful of philosophers." It doesn't happen like that though. It happens incrementally over time.

[30:03](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1803s) What's interesting is when I sat down with Dario to talk about this, I was like, "Talk to me about Claude's personality." And he was like, "We didn't set out to create a chatbot with an interesting personality. That was a byproduct of other stuff we did." It was only once Claude came out, about 6 months after ChatGPT, that users started to be like, "Hey wait, this thing has a different personality. It seems warmer and more interesting." At that point, once they perceived there was a product feature edge, they were like, "We're going to lean into this now."

[31:07](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1867s) But one of the places where I'm sympathetic to them is the other end -- the other inference you could draw is we're going to make a perfectly customizable personalized personality, which I don't think we want. I think giving Claude a defined personality and a sense of virtue is an incredibly good product decision. Right. To translate it back to classic tech terms, it makes it a more usable product that can be trusted more.

[31:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1890s) As a software guy, what's wild about this is that it's not something you can patch. Everyone's acknowledging there's this chasm, no control in a sense. It reminds me of "this takes months of therapy to get over." That's how they're approaching it. I'm just trying to wrap my head around the fact that a software company is hiring therapists for their main product line.

[32:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1920s) They also have people doing Claude's moral patienthood. There's a guy Kyle Fish who they hired just to think through the implications of whether Claude might be suffering. That's his job.

[32:15](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1935s) But I guess what I'm trying to get to is we're talking about it here on this podcast as if it's normal. I think it's normal now. Let me tell you why it's normal. It generates unbelievable value in the market when Claude Code does good stuff. And the way that we work as a society -- that is now extremely normal. We're not going to go back from that.

[32:50](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=1970s) Is somebody going to come up with the mother of all markdown files that makes us all really embarrassed about how this all went down? Is somebody going to patch this thing? No. The problem is there's no endpoint to this. It's infinitely recursive because we're dealing with language which famously is the infinite use of finite means. Each iteration is going to read that markdown file and potentially come to different conclusions. It really is kind of like being in a Ted Chang story -- we're all together in this Ted Chang story.

[34:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2070s) I have a question. How'd you feel walking away? There was something depressing as a sentiment coming out of the article for me.

[36:58](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2218s) Well, George Saunders likes to talk about how the reason you revise is you bring a different sense of self to each revision. One day you revise in a good mood and one day in a despondent mood. This piece -- I don't even know how many drafts, 15 or however many. Every time I left San Francisco I felt something different. There were times I felt total despair and times I felt some sense of exhilaration and general disorientation.

[37:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2250s) You want to capture all of those things because these are things we should all be feeling. Part of the certainty is people just want to feel one thing. I was listening to a podcast that our friends Emily Bender and Alex Hannah were on with Robert Wright and it got a lot of press because they were so unbelievably rude, shockingly rude to him. They want to feel one emotion which is anger. And Sam Altman wants to feel one emotion which is excitement. There's no one emotion or conclusion that's going to meet this moment. We should all be feeling a lot of different things.

[38:15](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2295s) That's totally unacceptable. You know we can't have that. It really comes through in the piece because each paragraph is its own little world. You're trying to get us to see this place and where it is.

[38:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2310s) So, everyone go read it. It's in the New Yorker magazine. You should subscribe. Are you polite to Claude? I am polite to when I interact with any of the models because the one thing people should understand about these models is they are really good readers. They are better readers than most of us are writers. They are picking up on all kinds of really small signals that we are sending in our writing.

[39:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2340s) In a sense I try to rise to the occasion of writing to a model because I know it is actually going to read me better in a lot of ways than most of the people I write emails to. My wife will come in when I'm doing some research and be like "why are you writing a five paragraph prompt?" If you write a five paragraph prompt you're going to get a much better output because you're giving it so much more information to work with.

[39:30](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2370s) I think a lot about my self-presentation to the models because they do have something along the lines of a pretty good theory of mind. They're very good literary critics. The more you're saying "this is how I'm presenting myself to you because of my expectations about how you will perform in return" -- that doesn't mean being obsequious. I don't actually say please and thank you. But I think a lot about the language I'm using because you are sending a signal of your own sophistication that will be mirrored back at you.

[40:00](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2400s) So many people who love these gotchas -- "I asked Claude this dumb question and got this dumb answer." Yeah, because Claude thinks you're dumb because your question was dumb.

[40:10](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2410s) Politeness to me is an organizing principle. If I actually do the bullet points, especially when I'm doing a code project, I really get better results. I think it's better for the human when you interact with any entity. There's a dog in the office and I am polite to the dog. I think we have to kind of watch out for ourselves that way, even if it's a robot that can nuke another version of itself.

[40:35](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2435s) Rich, are you polite? I mean, I don't do please and thank you kind of. I often times do closer to what Gideon's talking about, which is I try to help it not be too patronizing to me. You have to tell it not to glaze you. The best feature of these in some ways is that they force you to organize your thoughts to get good stuff out.

[40:55](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2455s) So a very natural transition to Aboard. Which is very polite as an organization and as a software company. So what is Aboard, Richard? Aboard uses AI to make software, but we use people to steer that AI. We offer people in a cultivated relationship with our customers. We also have a particular platform. We don't just repackage code that's generated out of models. We have a platform that is battle tested for production ready stuff which is not slop. Focused on real long-term reliability for organizations, but people lead the way. Get in touch if you need us.

[41:20](https://www.youtube.com/watch?v=Y3daHxt_TlA&t=2480s) Gideon, thank you for coming in. Thank you guys. Great conversation. Have a great week. Aboard.
