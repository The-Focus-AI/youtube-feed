---
video_id: 8-boBsWcr5A
title: "Satya Nadella â€“ How Microsoft thinks about AGI"
channel: Dwarkesh Patel
duration: 5321
duration_formatted: "1:28:41"
view_count: 309306
upload_date: 2025-11-12
url: https://www.youtube.com/watch?v=8-boBsWcr5A
thumbnail: https://i.ytimg.com/vi_webp/8-boBsWcr5A/maxresdefault.webp
tags:
  - Microsoft
  - Azure
  - AI
  - AGI
  - Satya-Nadella
  - OpenAI
  - data-centers
  - GPU
  - hyperscale
  - Copilot
  - GitHub
  - Nvidia
  - infrastructure
  - sovereignty
  - geopolitics
  - business-models
---

# Satya Nadella -- How Microsoft thinks about AGI

## Summary

In this interview conducted inside Microsoft's new Fairwater 2 data center in Wisconsin, Satya Nadella sits down with Dwarkesh Patel and Dylan Patel (SemiAnalysis) for an unusually frank conversation about how Microsoft is positioning itself for the AI era. Nadella walks through the data center -- which he says has training capacity almost equivalent to all of Azure across all regions -- and discusses the coupling between model architectures and hardware generations, the challenge of pacing infrastructure buildout when GPU generations change every 12-18 months, and why Jensen Huang's advice to him was "move fast between generations."

The conversation covers the full strategic landscape: Microsoft's new MAI model effort (led by Mustafa Suleyman and Karen Simonyan), the seven-year IP access agreement with OpenAI, GitHub Copilot's growth trajectory, and Nadella's theory that the coding AI market has 10x'd from $500 million to $5 billion in one year -- with Microsoft's market share dropping from near 100% to sub-25% as Cursor and Claude Code emerged. Rather than being alarmed, Nadella frames this as evidence that the market is expanding massively, analogous to the server-to-cloud transition that expanded Microsoft's addressable market despite lower per-unit margins.

The most revealing portion covers Microsoft's infrastructure strategy. Nadella explains why Microsoft paused some data center buildouts in mid-2025, arguing that blind expansion with a single GPU generation was riskier than pacing investment across generations. He makes the case that hyperscale is fundamentally a long-tail business, not a business of doing five big contracts with five customers, and that the real value comes from workload diversity, geographic distribution, and software systems knowledge -- not just raw GPU capacity. The interview concludes with Nadella's views on sovereignty, geopolitics, and why "trust in tech" may be the most important competitive feature in the bipolar world ahead.

## Highlights

### "We are now a capital-intensive knowledge business"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4272s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*71:12-72:15" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-71m12s.mp4"
```
</details>

> "I describe it as we are now a capital-intensive knowledge business. We have to use our knowledge to differentiate capital. The hardware guys have done a phenomenal job. But the software improvements of really throughput optimization we've been able to get quarter-over-quarter -- it's 5x, 10x, maybe 40x in some cases. That's knowledge intensity coming in."
> -- Satya Nadella, [1:11:12](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4272s)

### "Cursor is going to kill you -- it's not Borland"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=877s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*14:37-15:20" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-14m37s.mp4"
```
</details>

> "When you say, who's the competition now? Cursor is going to kill you -- it's not Borland. This is the right direction. This is it. The fact that we went from $500 million to $5 billion in the coding AI market in one year -- this is like the cloud-like stuff. Coding and AI is probably going to be the software factory category."
> -- Satya Nadella, [14:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=877s)

### "Now I see where my money is going"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=198s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*3:18-4:05" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-3m18s.mp4"
```
</details>

> "My God, it's kind of loud. 'Now I see where my money is going.' Welcome to the software company."
> -- Satya Nadella, [3:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=198s)

### "Office 365 becomes the agent substrate"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1879s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*31:19-32:20" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-31m19s.mp4"
```
</details>

> "In fact, I kind of look at Office 365, which today is an end-user tools business, becoming a business in support of agents doing work. In fact, all the stuff we built underneath -- you need some place to store it, some place to compute it, some place to manage all of these activities. The pricing is not just per user, it's per agent."
> -- Satya Nadella, [31:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1879s)

### "There's no birthright here"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1153s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*19:13-20:10" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-19m13s.mp4"
```
</details>

> "But you've gone from near 100% market share to sub-25% market share in coding AI in just one year. What makes you confident that Microsoft will keep winning? My response to that -- there's no birthright here, other than to say we should go innovate. What I'm optimistic about is that this category is going to be a lot bigger."
> -- Satya Nadella, [19:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1153s)

### "Trust in tech is probably the most important feature"

[![Clip](https://img.youtube.com/vi/8-boBsWcr5A/hqdefault.jpg)](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5257s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*87:37-88:10" "https://www.youtube.com/watch?v=8-boBsWcr5A" --force-keyframes-at-cuts --merge-output-format mp4 -o "8-boBsWcr5A-87m37s.mp4"
```
</details>

> "Trust in tech is probably the most important feature. It is, 'Can I trust you, the company, and can I trust your institutions to be a long-term supplier?' That's a good note to end on."
> -- Satya Nadella, [1:27:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5257s)

## Key Points

- **Fairwater 2 data center scale** ([1:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=79s)) - Microsoft's new data center in Wisconsin has training capacity approaching all of Azure across all regions combined, with five million network connections and the ability to run training jobs across two linked regional sites.

- **GPU generation coupling risk** ([3:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=210s)) - There is tight coupling between model architecture and GPU generation. Vera Rubin Ultra will have different power and cooling requirements. You don't want to over-build for one generation when the next is radically different.

- **AI as cognitive capability, not revolution** ([5:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=319s)) - Nadella cites Raj Reddy (Turing Award winner at CMU) who said AI should be viewed as a guardian angel or cognitive capability tool. Nadella sees it this way but acknowledges others see it as more than a tool.

- **Productivity takes 70 years to diffuse** ([7:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=474s)) - The Industrial Revolution created massive economic growth, but only after 70 years of diffusion. AI may compress this to 20-25 years, but workflows, work artifacts, and organizational structures all need to change.

- **SaaS business model disruption** ([9:23](https://www.youtube.com/watch?v=8-boBsWcr5A&t=563s)) - The high COGS of AI inference breaks the traditional SaaS model of near-zero marginal cost per user. Microsoft will need tiered pricing with subscriptions and consumption rights rather than flat-rate licensing.

- **Coding AI market 10x in one year** ([13:24](https://www.youtube.com/watch?v=8-boBsWcr5A&t=804s)) - The coding AI market has grown from roughly $500 million to $5 billion run rate in one year, across GitHub Copilot, Cursor, Claude Code, Windsurf, Replit, and OpenAI Codex. This 10x growth mirrors cloud expansion patterns.

- **GitHub market share dropped to sub-25%** ([19:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1153s)) - Microsoft went from near 100% to sub-25% market share in coding AI in one year. Nadella is not alarmed because the category expanded massively -- similar to how cloud expanded despite lower share than on-premise.

- **Excel Agent and the middle-tier model strategy** ([25:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1556s)) - Microsoft built an Excel Agent using a middle-tier GPT model with deep RL fine-tuning specific to Office tools. It knows formulas, can fix reasoning mistakes, and understands Excel semantics -- not just a wrapper on a frontier model.

- **Office as agent infrastructure** ([31:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1879s)) - Nadella sees Office 365 transitioning from an end-user tools business to infrastructure for AI agents. Pricing shifts from per-user to per-agent, with agents needing compute, storage, identity, and observability.

- **MAI model strategy** ([38:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2298s)) - Microsoft's own model (MAI) started with a small image model that ranked well on LMArena. Next step is an omni-model combining image, text, and other modalities. The goal is not to duplicate GPT but to complement it where OpenAI's family has gaps.

- **Talent acquisition from Google** ([41:09](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2469s)) - Microsoft hired the Blueshift reasoning team from Google, the post-training team from Gemini 2.5, and key researchers from DeepMind. Mustafa Suleyman and Karen Simonyan are leading the new lab.

- **The infrastructure pause explained** ([48:42](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2922s)) - Microsoft paused some data center buildouts in mid-2025 after realizing that building massive capacity for one GPU generation was risky. Jensen Huang's advice: move fast between generations. The pause was about rebalancing, not retreating.

- **Hyperscale is a long-tail business** ([53:17](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3197s)) - Nadella argues Azure should be a long-tail business for diverse AI workloads, not a business of doing five big contracts with five customers. The real value is in workload diversity, geographic distribution, and full-stack services.

- **Neocloud partnerships and Oracle** ([1:02:52](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3772s)) - Microsoft is buying capacity from neoclouds, taking build-to-suit deals, and even buying Oracle capacity. They want neoclouds to join their marketplace, feeding capacity to Azure customers.

- **Custom silicon strategy (Maia)** ([1:04:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3866s)) - Microsoft's Maia 200 custom accelerator looks promising but faces the challenge that Nvidia's next generation is always the benchmark. The strategy is to co-design silicon with their own models (MAI), similar to how Google uses TPUs.

- **OpenAI agreement details** ([1:07:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4046s)) - Microsoft has a new agreement with OpenAI giving them IP rights, including access to weights. OpenAI's SaaS business (ChatGPT) can run anywhere, but their PaaS business (API) must run on Azure with few exceptions.

- **Sovereign cloud and geopolitics** ([1:17:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4650s)) - Each country wants concentration risk mitigation -- access to multiple models, including open source. Microsoft is building sovereign services with confidential computing and EU Data Boundaries. The US is 4% of the world's population producing 50% of tech value.

- **Chinese competition and trust** ([1:27:20](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5220s)) - When asked about competing with ByteDance, Alibaba, DeepSeek, and Moonshot, Nadella says trust in tech is the most important competitive feature. "Can I trust you, the company, and your institutions to be a long-term supplier?"

## Mentions

### Companies
- **Microsoft** ([0:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=54s)) - Subject of the interview; discussed across all topics
- **OpenAI** ([37:40](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2260s)) - Microsoft's AI partner; discussed IP rights, new agreement, and Azure hosting requirements
- **Nvidia** ([3:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=210s)) - GPU supplier; GB200, GB300, Vera Rubin Ultra discussed; Jensen Huang's advice on generation pacing
- **Google** ([41:09](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2469s)) - Competitor; source of talent hires (Blueshift reasoning team, Gemini 2.5 post-training team)
- **Anthropic** ([42:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2576s)) - Competitor; Claude used in GitHub Copilot alongside OpenAI models
- **Cursor** ([14:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=877s)) - Competitor to GitHub Copilot; cited as example of new AI coding competitors
- **Oracle** ([56:57](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3417s)) - Microsoft buys Oracle capacity; competes on hyperscale
- **Meta** ([1:11:01](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4261s)) - Competitor taking corporate loans for infrastructure
- **Amazon** ([1:04:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3853s)) - Competitor trying to make custom chips (Trainium)
- **Salesforce** ([1:09:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4139s)) - Example of enterprise partner that would use Azure for OpenAI integration
- **SemiAnalysis** ([8:39](https://www.youtube.com/watch?v=8-boBsWcr5A&t=519s)) - Dylan Patel's research firm; Nadella says he couldn't run SemiAnalysis without AI
- **Replit** ([18:45](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1125s)) - Listed among coding AI competitors
- **Cognition** ([17:08](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1028s)) - AI agent company whose tools can be orchestrated through GitHub
- **EMC** ([13:03](https://www.youtube.com/watch?v=8-boBsWcr5A&t=783s)) - Historical reference; storage business that collapsed in the cloud transition
- **DeepSeek** ([1:27:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5224s)) - Chinese AI competitor mentioned in geopolitics discussion
- **ByteDance** ([1:27:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5224s)) - Chinese tech company mentioned as competitor
- **Alibaba** ([1:27:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5224s)) - Chinese tech company mentioned as competitor

### Products & Technologies
- **Fairwater 2** ([1:14](https://www.youtube.com/watch?v=8-boBsWcr5A&t=74s)) - Microsoft's new data center in Wisconsin, toured during the interview
- **GB200** ([3:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=210s)) - Nvidia GPU generation currently deployed in Fairwater 2
- **GB300** ([52:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3124s)) - Next Nvidia GPU generation coming soon
- **Vera Rubin Ultra** ([3:49](https://www.youtube.com/watch?v=8-boBsWcr5A&t=229s)) - Future Nvidia GPU with different power and cooling requirements
- **NVLink** ([3:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=210s)) - Nvidia's high-speed interconnect used in data center
- **GitHub Copilot** ([13:24](https://www.youtube.com/watch?v=8-boBsWcr5A&t=804s)) - Microsoft's AI coding assistant; discussed market share and growth
- **Azure** ([1:32](https://www.youtube.com/watch?v=8-boBsWcr5A&t=92s)) - Microsoft's cloud platform; core infrastructure discussion
- **Office 365** ([9:31](https://www.youtube.com/watch?v=8-boBsWcr5A&t=571s)) - Microsoft's productivity suite transitioning to agent infrastructure
- **Excel Agent** ([25:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1556s)) - Microsoft's AI agent for Excel built on middle-tier GPT model
- **MAI** ([38:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2298s)) - Microsoft's own AI model family; started with image model, moving to omni-model
- **Maia 200** ([1:04:44](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3884s)) - Microsoft's custom AI accelerator chip
- **Cobalt** ([1:04:55](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3895s)) - Microsoft's custom ARM CPU for Azure
- **Cosmos DB** ([55:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3318s)) - Microsoft's database service needed close to AI workloads for session data
- **Agent HQ** ([16:45](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1005s)) - GitHub's new concept for orchestrating multiple AI coding agents
- **Sovereign Services** ([1:20:10](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4810s)) - Microsoft's offering for countries wanting data sovereignty with confidential computing
- **LMArena** ([39:25](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2365s)) - Model evaluation leaderboard where Microsoft's MAI image model ranked well

### People
- **Satya Nadella** ([0:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=54s)) - CEO of Microsoft, interviewee
- **Dwarkesh Patel** ([0:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=54s)) - Interviewer, host of Dwarkesh Podcast
- **Dylan Patel** ([0:57](https://www.youtube.com/watch?v=8-boBsWcr5A&t=57s)) - Co-interviewer, founder of SemiAnalysis
- **Scott Guthrie** ([1:09](https://www.youtube.com/watch?v=8-boBsWcr5A&t=69s)) - Microsoft's EVP of Cloud + AI, gave data center tour
- **Jensen Huang** ([1:01:41](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3701s)) - Nvidia CEO; advised Nadella to move fast between GPU generations
- **Raj Reddy** ([6:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=364s)) - Turing Award winner at CMU who described AI as guardian angel or cognitive capability
- **Mustafa Suleyman** ([42:01](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2521s)) - Leading Microsoft's new AI lab
- **Karen Simonyan** ([42:01](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2521s)) - Leading Microsoft's AI research alongside Mustafa
- **Sam Altman** ([1:13:20](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4400s)) - OpenAI CEO; referenced regarding AGI and ASI timelines
- **Amy Hood** ([1:11:06](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4266s)) - Microsoft CFO; referenced regarding capital discipline

## Surprising Quotes

> "Now I see where my money is going. Welcome to the software company."
> -- Satya Nadella, touring the Fairwater 2 data center, [3:22](https://www.youtube.com/watch?v=8-boBsWcr5A&t=202s)

> "In fact, we are buyers of Oracle capacity. I think the industrial logic for what we are doing makes sense."
> -- Satya Nadella on acquiring compute from competitors, [56:57](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3417s)

> "The United States is just 4% of the world's population, and that 50% of tech value happens because quite frankly the US has built institutions -- its capital markets, its stewardship of what matters."
> -- Satya Nadella on US tech dominance and sovereignty, [1:18:01](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4681s)

> "If there's going to be one model that dominates by a massive distance, yes, that's a winner-take-all. But if there are multiple models, just like hyperscale competition, there is room here to go build value on top of models."
> -- Satya Nadella on model competition, [27:53](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1673s)

> "Jensen's advice to me was two things. Move fast between generations. That's why the execution -- I mean, it's like 90 days between when we receive GPUs and when they're running. That's real speed-of-light deployment."
> -- Satya Nadella on infrastructure strategy, [1:01:41](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3701s)

## Transcript

[0:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=54s) Today we are interviewing Satya Nadella. "We" being me and Dylan Patel, who is the founder of SemiAnalysis. Thank you. It's great. Thanks for giving us the opportunity. It's been really cool to see.

[1:09](https://www.youtube.com/watch?v=8-boBsWcr5A&t=69s) Satya and Scott Guthrie, Microsoft's EVP of Cloud and AI, gave us a tour of their brand new Fairwater 2 data center. We've tried to 10x the training capacity. So this would effectively be a 10x over the previous generation.

[1:28](https://www.youtube.com/watch?v=8-boBsWcr5A&t=88s) So to put it in perspective, the number of flops in this one data center is almost as much as all of Azure across all regions. It's got like five million network connections, linking sites in a region and between the two regions. In the future, you anticipate that a single training run might require two whole different regions to train?

[1:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=116s) Yes, you need enormous flops for a large training job and then you need more for inference. The reality is you'll use it for training and inference in all sorts of ways. It won't be dedicated only for one workload forever. The next data center, which you can see under construction nearby, will also be on a high-speed link connecting the two at a very high rate.

[2:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=146s) We have multiple sites in Milwaukee where literally you can see the model training flowing across them. It's kind of built for, essentially, the largest training runs. And then with the WAN, you can distribute inference. You literally run a training job across multiple buildings.

[2:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=174s) What we're seeing right here is a cell of racks. How many racks are in a cell? That's the reason I ask. I'll start counting. We'll let you start counting. That part also I can't tell you.

[3:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=198s) My God, it's kind of loud. "Now I see where my money is going." Welcome to the software company.

[3:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=210s) You've decided to use the GB200s and the NVLink? There is coupling from the model architecture to the GPU generation. And it's also scary in that sense, which is -- take Vera Rubin Ultra. That's going to have different power and cooling requirements. So you kind of don't want to over-build for one generation.

[4:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=244s) That goes back a little bit to the pacing question, which is that you want to be scaling in time as well as in space. When you look at all the past technological revolutions -- the Internet, or replaceable parts, industrialization -- each wave has gotten much faster in the time it takes to build out and achieve pervasiveness through the economy.

[4:35](https://www.youtube.com/watch?v=8-boBsWcr5A&t=275s) Many people, including those on this podcast, believe this is the final general-purpose technology and that this time is very, very different. We've already skyrocketed to hyperscalers building at a scale that's unmatched. The end state seems to be quite different from what I would call the traditional pattern. I'd like to understand that more.

[5:15](https://www.youtube.com/watch?v=8-boBsWcr5A&t=315s) I also feel for the idea that maybe after all this investment, AI could be the biggest thing. I start with that premise. But there's also the fact that this is still early innings. We're seeing some great properties in scaling, and I'm optimistic that they'll continue to work. There will be breakthroughs, but it's also a lot of engineering. That said, I also sort of take the view that the last 70 years of computing has also taught us a lot.

[6:04](https://www.youtube.com/watch?v=8-boBsWcr5A&t=364s) I like one of the things that Raj Reddy said. He's a Turing Award winner at CMU. He had a very simple framework for AI: it should either be a guardian angel or a cognitive capability. That's a simple way to think about what this is. It is going to be a cognitive capability tool. If I view it that way, I view it as a tool. Some people take exception to that and say this is more than a tool.

[6:45](https://www.youtube.com/watch?v=8-boBsWcr5A&t=405s) But so far, tools have automated things that humans did before. That's been the pattern with many technologies in the past. We had activities and then we had tools that did them. But one way to think about it is, maybe at some point, eventually a machine is producing tokens -- and Satya tokens are worth a lot. So are you creating this economic value by interviewing Satya? Whatever you want to call it, are the tokens going to be valuable?

[7:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=438s) Right now, if you have models that cost on the order of pennies per million tokens, there's just an enormous room for improvement, especially where a million tokens of Satya are worth a lot. At what margin is Microsoft involved in generating that value?

[7:41](https://www.youtube.com/watch?v=8-boBsWcr5A&t=461s) In some sense this goes back to the big picture: what's productivity going to really look like? The Industrial Revolution created massive productivity gains, but after 70 years of diffusion, that's the other thing to remember. This time around, for true economic growth to appear, the tools have to change, the work artifact has to change, and the workflow has to change. There's a lot of organizational management required for a corporation to truly change.

[8:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=506s) Going forward, do humans and the tokens they produce matter? Will there be Dwarkesh or Dylan tokens of the future? Would you be able to run SemiAnalysis without any humans? No chance, at the scale that you have it. But the question is, what's the scale? Will there be massive growth that comes through AI? Absolutely. The point is, what took 70 years with the Industrial Revolution may happen in 20 years, 25 years. And what happened in 200 years of the Industrial Revolution, that compression is real.

[9:23](https://www.youtube.com/watch?v=8-boBsWcr5A&t=563s) Microsoft historically has been the largest software-as-a-service company. You went from selling Windows licenses and Office servers to selling subscriptions to 365. But your business today also faces a challenge. Software-as-a-service has incredibly high margins. There's a lot of R&D, but low marginal cost. This is sort of why AI companies have been punished massively in the markets, because the COGS of AI breaks how these business models work.

[10:10](https://www.youtube.com/watch?v=8-boBsWcr5A&t=610s) How does the largest software-as-a-service company transition when the incremental cost per user is different? It's 20 bucks for Copilot. Does that make sense with the business models themselves? If you look at the menu of models starting from the cheapest to the most expensive, there will be some transaction margin for AI devices. Then there'll be enterprise consumption. To your point, what is a subscription? Enterprises love subscriptions because they can budget for them. So I think in some sense subscriptions will become consumption rights, and the margin structures will get tiered.

[11:36](https://www.youtube.com/watch?v=8-boBsWcr5A&t=696s) The good news for us is we are a portfolio company. At a portfolio level, we pretty much have exposure to all of the consumer levers as well. We need to figure out which models make sense in what categories.

[12:08](https://www.youtube.com/watch?v=8-boBsWcr5A&t=728s) Having a low ARPU is great, because it expands the market. During the transition from server to cloud, one fear was, "Oh my God, if all we did was basically take our Office licenses and move them to the cloud, this is going to not only shrink our margins but shrink revenue." Except what happened was the move to the cloud expanded the market massively. We sold a few servers in a few places, whereas in the cloud suddenly everyone could afford fractionally buying servers and IT cost dropped.

[13:03](https://www.youtube.com/watch?v=8-boBsWcr5A&t=783s) For example, the amount of money people were spending on storage -- EMC's biggest segment may have been storage. All that sort of dropped in the cloud. In fact, it was working capital efficiency that expanded the market massively.

[13:24](https://www.youtube.com/watch?v=8-boBsWcr5A&t=804s) If you take coding, what we built with GitHub Copilot -- the coding assistant market is that big in one year. The key thing is the market expands massively. The question is, will the parts of the revenue that Microsoft traditionally captured expand too? If you look earlier this year, according to various estimates, GitHub's run rate was like $500 million or something. Whereas now you have Claude Code, Cursor, Windsurf -- with around similar revenue, around a billion. So the question is, across all the surfaces, what share do Microsoft's equivalents of Copilot have?

[14:22](https://www.youtube.com/watch?v=8-boBsWcr5A&t=862s) I love this chart for so many reasons. Second is all these companies that are listed -- most of them didn't exist four or five years ago. You have new competitors. When you say, who's the competition now? Cursor is going to kill you -- it's not Borland. This is the right direction. This is it. The fact that we went from $500 million to $5 billion -- this is like the cloud-like stuff. Coding and AI is probably going to be a $50 billion or $100 billion category. It is the software factory category. I want to keep myself open-minded about it.

[15:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=913s) That's your point, which is a great one. We need to take what we had and compete. Just finished our quarterly announcement, and I feel good about our subscriber growth and retention. But the more interesting thing that has happened is where do the guys who are generating lots and lots of code go? GitHub is seeing massive growth in repo creation, PRs, everything. Interestingly enough, we are getting code from people who aren't even using our Copilot workflow, just because there are structural reasons to be on GitHub.

[16:16](https://www.youtube.com/watch?v=8-boBsWcr5A&t=976s) We also have coding code review agents, which are becoming important. We'll have many, many structural shots at this. What we want to do is what we did with Git -- from repos to issues to actions, these are powerful primitives. We want to extend that. Last week we announced Agent HQ as the conceptual thing. This is where, for example, you go to Mission Control, and now I can fire off multiple AI agents because I'll have essentially an orchestration layer -- Cognition stuff, anyone's agents -- so I get one package and then I can literally manage agents working in their independent branches.

[17:27](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1047s) I think one of the biggest places of innovation right now is how to manage AI agents writing code. I want to be able to digest what they're doing, steer and triage what the coding agents are doing. GitHub, and all of these new primitives we'll build, plus observability -- just think about everyone running AI agents. It will require a whole host of observability tools for the code base. I feel that's the opportunity. At the same time, we better be competitive and innovate.

[18:15](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1095s) But I like the chart. The key point here is that the total market is growing regardless of whose coding agent wins. It's growing at 15, 20%, which is way above GDP. It's a great market. It's grown from $500 million run rate at the end of last year to now where the current run rate across GitHub Copilot, Cursor, Claude Code, Windsurf, Replit, OpenAI Codex -- that's run rating at something like $5 billion. That's 10x. When you look at the TAM of software development, is it just for the existing 30 million developers, or is it something beyond that?

[19:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1153s) But you've gone from near 100% market share to sub-25% market share in just one year. What makes you confident that Microsoft will keep winning? My response to that -- there's no birthright here, other than to say we should go innovate. What I'm optimistic about is that this category is going to be a lot bigger. Let me say it that way. We had high share in VS Code, we had high share in the initial Copilot market. But the point is that even having a decent share of a much larger category is better. You could say we had a high share of on-premise compute. We have much lower share than that in hyperscale. But the category is an order of magnitude larger.

[20:29](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1229s) It all means you have to get competitive. I'm optimistic about what we're going to do with GitHub as the platform where all these agents come. Multiple shots on goal. Others can succeed along with us, so it doesn't need to be zero-sum.

[20:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1256s) I guess the reason to focus on this is not just about coding, but fundamentally about Office and all of Microsoft's products. One vision you could have about how AI develops is that models will always be somewhat hobbled and you'll need this scaffolding layer. Another vision is that over time these models become incredibly capable -- in the future, they'll be doing all the actual work. Then the model companies are charging thousands of dollars per month for what is basically a coworker which could use any UI to communicate.

[21:42](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1302s) If we're getting closer to that, why isn't it the case that the model companies are just getting more and more profitable? Why is the place where the scaffolding happens, as the model becomes more capable, going to be that important? In a world of very capable AI coworkers doing knowledge work, why doesn't all the value migrate just to the model?

[22:13](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1333s) It's a fair question about the scaffolding and the model. But my fundamental point also is that -- let's take information work, or take even coding. What we've built in GitHub Copilot is called model routing. In fact, I buy a subscription and the system auto-selects the right model for what I am asking it to do. It could arbitrage the tokens available from any model. If you take that argument, the scaffolding layer has real value, especially with open source models.

[23:22](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1402s) Today the scaffolding is dealing with the jaggedness of these intelligence problems. Over time, if models get much better, then you will vertically integrate yourself more tightly with the model. But there are going to be open source models available. As long as you have something that you can use effectively, there's room for builders. I can make the argument that if you're a model company, you may have done all the hard work, but it's one copy away from that being commoditized. Context engineering, and the liquidity of data -- that's where real value lies. So I think the argument can be made both ways.

[24:31](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1471s) There's two views of the world. One is that there are many competitive models out there, open source exists, and scaffolding and context engineering will drive some level of who wins and who doesn't. The other view is that models will consolidate. And everyone's in a tight race and there's not that much differentiation. You can see this in the revenue charts -- the model companies' gross margins on inference went from well below 50% to expanding, despite increased competition. OpenAI is competitive, Google is competitive, Anthropic is competitive. All these companies are now competitive, and yet margins are expanding at the model layer significantly.

[25:38](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1538s) It's a great question. Perhaps a few years ago people thought you just need a model and you can build a successful company. But the interesting thing is, when I look at what our team built called Excel Agent -- it's interesting. It's actually a model that is in the middle tier. We're taking a GPT family model and using RL fine-tuning with deep integration into the Office system to teach it what it means to be an Excel expert.

[26:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1590s) It's not just, "Hey, I just have a language model." I have a full understanding of what Excel formulas mean, what the semantics are. Because if I'm an Excel agent, I need to even fix the reasoning mistakes I make. I need to be able to see, "Oh, I got that formula wrong." To some degree, that's all being done not with a frontier model, but with a middle tier model. I'm giving it essentially a markdown-to-Excel translation capability to be a sophisticated Excel user.

[27:10](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1630s) Let me step back a little bit to the AI stack. You're building business logic in its traditional sense, wrapping it with AI, using this model which knows how to use the tool. It's all bundled in with all the tools used. And these tools can be built by everybody. For the model companies, they'll have to compete. If I'm a builder of a tool like this, I can substitute models. So as long as there's competition -- if there's going to be one model that dominates by a massive distance, yes, that's a winner-take-all. But if there are multiple models, just like hyperscale competition, there is room here to go build value on top of models.

[28:14](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1694s) I think of ourselves as being in the hyperscale business with models. We will have access to OpenAI models for seven years with full IP rights. Essentially, I think of ourselves as having a portfolio of models to use and innovate on with full flexibility. So we will always have a model level. Whether it's in knowledge work, whether it's in coding, we'll have scaffolding which will be model-forward. It won't be a wrapper on a model, but deeply integrated.

[28:58](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1738s) I have so many questions about the model strategy. But before we move onto those topics, let me ask about AI capabilities. Right now, computer use takes a screenshot of your screen, but it can't really interact deeply. I think the better mental model here is not screenshot-based interaction but AI that's actually able to use a computer as well as a human. It can look into the formulas, can use alternative approaches, and can switch between different software if needed. That's kind of what I'm saying -- deep integration with Excel matters because the AI shouldn't just screenshot it.

[29:46](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1786s) After all, Excel was built as a tool for analysts. An analyst should have tools that they can use. Humans use a computer -- that's their tool. What I'm saying is that I'm building an AI agent for analysis which happens to come with a priori knowledge of Excel built in.

[30:16](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1816s) Just to make sure we're talking about the same thing -- there's a world where a human like me is using Excel, and in the future the company would have tools that are AI-augmented. In fact, in the future I'll even have AI helping me steer everything. But it's still me steering. The second world is the company just literally hires AI agents that work fully autonomously, essentially an embodied set of capabilities.

[31:08](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1868s) So this AI tool that comes in with Excel knowledge -- it's going to be more token-efficient because it has deep integration. In fact, I kind of look at Office 365, which today is an end-user tools business, becoming a business in support of agents doing work. In fact, all the stuff we built underneath -- you need some place to store it, some place to compute it, some place to manage all of these activities, even for autonomous agents.

[31:55](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1915s) To make sure I understand -- you're saying even if a model company has actual computer use, even if it's not partnered with Microsoft, it could technically use Office through screenshots. But you're saying, if you're working with Microsoft, you get to give them lower-level access that makes it more efficient than what you could have otherwise done anyways?

[32:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1946s) Think about it like this -- first there was bare metal, then there was virtualization. That's another way to think about this. What is the entire substrate that an AI agent needs to work? That entire substrate is the opportunity, because the AI agent needs a computer.

[32:49](https://www.youtube.com/watch?v=8-boBsWcr5A&t=1969s) We're seeing a significant amount of demand for provisioning Office artifacts and what have you for autonomous agents. They really want to be able to provision entire environments. That's why we're going to have essentially an agent tier for Office 365, which is going to just keep growing because it's driven by agent demand. The pricing is not just per user, it's per agent.

[33:31](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2011s) The key is what's the stuff that an agent needs? A computer, a set of security things, identity, observability. All those things are going to get baked into that.

[33:50](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2030s) Think about it -- model companies are training their models to use Excel, or to use Word. But at the same time, they're also trying to convert everything into raw data formats. Because that is probably the most immediately efficient path -- converting legacy systems to standard cloud systems, converting what is done in Word and Excel to something more efficient in a classical sense. It's just not cost-effective for most enterprises to do that immediately.

[34:44](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2084s) If the models can utilize the tools that humans built, yes, Microsoft has a leadership position in those tools. But in these other categories, the use of traditional tools will be significantly less, just like the use of mainframes declined. Now mainframes have grown for the last two decades, but they're not the center of computing anymore. They've still grown, though.

[35:16](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2116s) How does that flow? I think there is going to be a significant amount of time where agents are going to be using the tools that humans also use, and they have to communicate with each other. A human needs to see outputs and inputs. You can't just say, "Oh, I migrated off." You have to live in this hybrid world. And there's a question about what the primitives are that are needed. Does that storage system need to have e-discovery? Do you need to have an identity system for agents along with humans having one identity system?

[36:12](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2172s) The same requirements we have today for enterprise infrastructure -- that's what we will have in the future as well. I would love all of Excel to be replaced by something better. I would love for all that to happen immediately. But databases in fact will be a huge part of the future. If I think about all of the Office artifacts eventually becoming something more like databases -- joins between structured and unstructured data -- that's the underlying infrastructure business. And a lot of that is all being driven by agents.

[36:52](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2212s) It could also be that a model company builds competing software. The competition could be that we build tools, others provision them, and then there will be competition at every layer.

[37:12](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2232s) Speaking of model companies -- you say not only do you have access to OpenAI models, you'll have your own model. Your most recent model that was released two weeks ago -- you obviously have the IP rights to OpenAI. But it seems to be behind on some benchmarks. How do you think about the fact that you theoretically have the right to use all of OpenAI's technology, especially if it's a big part of your strategy?

[37:46](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2266s) First of all, we are absolutely going to use OpenAI's models to the maximum across all of our products. That's what we plan to do all the way for the next seven years. That's where the analyst and this Excel agent strategy comes in -- we'll do RL fine-tuning on top of a GPT family where we have full access.

[38:18](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2298s) With the MAI model, the way that I think about it -- the good news here with the new agreement is that OpenAI is going to continue to build a world-class superintelligence lab. But at the same time, we're also going to think about how to use both these things. The MAI side will be very product-focused. Because we have access to the GPT family, the last thing we want to do is something that is just duplicative and doesn't add much value. So we want to generate a GPT family and maximize its value.

[39:06](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2346s) Take the image model that we launched, which I think is really good. We're using it both for cost efficiency and quality -- it's in Bing, and we're going to use that across our products. It's got personality and what have you. Even on the LMArena, we started ranking well. By the way, it was done with a very small model. The core capability, the instruction following -- we wanted to make sure we could do that well. That shows us, given scaling laws, what we are capable of.

[39:53](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2393s) The next thing we will do is an omni-model where we combine what we have done in image with text and other modalities. That will be the next pit stop on the MAI side. We are going to build a first-class model capability. We are going to continue to produce models. They will either be used in our products for cost-efficiency, or for specific capabilities. And we will do real research in order to make breakthroughs, exploiting the advantage we have of having the world's largest AI infrastructure.

[40:39](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2439s) Say we roll forward seven years. What does Microsoft do to make sure they have a frontier model capability? Today, OpenAI has developed many of the breakthroughs. Or Google's developed all the key techniques. But it is also a big talent game. Google has spent on the order of $20 billion on talent.

[41:09](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2469s) We hired the Blueshift reasoning team from Google last year. We hired the post-training team from Gemini 2.5 more recently. Key people who did important work at DeepMind are now at Microsoft. In fact, later this week even, Mustafa is going to share more clarity on what our lab is going to go do.

[42:27](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2547s) The strategy is that we are going to build the infrastructure to support all the models the world needs -- from OpenAI and others. That's one thing. Second, we will absolutely use the OpenAI model in our products. And we may -- like in GitHub Copilot where Anthropic's Claude is already used -- have multiple models going to be wrapped into our products as well. At the end of the day, the eval of the product as it meets customers is what matters. We'll start back from there into the vertical integration.

[43:25](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2605s) There's a question going forward about this distinction between training and inference and the difference between the different models. Are you expecting something like human-level continual learning? If you think about your last 30 years -- it's the last 30 years of wisdom and experience. We will eventually have models, if they get good at this ability to continuously learn on the job.

[44:00](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2640s) That is ahead, at least in my view, because you need models deployed across the economy learning how to do every single job, feeding their learnings back to that model. That creates a learning exponential feedback loop. If that happens and Microsoft isn't the one with the most broadly deployed model, does that matter?

[44:25](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2665s) You're saying that well, one model could learn everything while others in the long tail don't. That the model that's most broadly deployed in the world and best at continual learning -- that's game, set, match and you stop shopping? But the world today, for all the dominance of any single player -- take coding, there are multiple models. There is not one model that rules everything. There are multiple models that are getting used. It's like the thing, "Can one database be the one that rules them all?" There are multiple types of databases.

[45:23](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2723s) I think that there are going to be some advantages in data liquidity that any one model has. Is it going to happen in all geos? I don't think so. In all segments? I don't think so. So therefore I feel like the design space is still wide. But your fundamental point is having a capability at the model level and at the scaffolding layer, and then being able to compete not just as a vertical stack, but to be able to compose horizontally too.

[46:05](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2765s) You can't build an infrastructure that only works with one model. If you do that, what if you fall behind? Everything you built will be a waste. You need infrastructure that's capable of supporting whatever may come. Otherwise the capital you put in, which is massive, could be wasted. You're one tweak away, some MoE-like breakthrough, and everything goes out of the window. That's a scary thing. So you want to support whatever may come in your own model and from others. Be open. If you're serious about the hyperscale business, be open to all models.

[48:42](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2922s) So last year Microsoft was on path to be the largest hyperscaler by a big margin. You were the earliest in 2023, going hard in terms of leasing data centers, starting construction. You guys were on pace to certainly beat everyone by 2028. Then in the middle of last year, Microsoft did this big pause, where you pulled back on some of the capacity you were going to take. We're sitting in one of the largest data centers in the world. You guys are expanding like crazy. So what's going on? Why did you do this?

[49:31](https://www.youtube.com/watch?v=8-boBsWcr5A&t=2971s) The question is, what is the hyperscale business all about? If we're going to build out Azure to support everything from training to mid-training to data generation to inference -- that entire thing caused us to basically step back and think about what we need with a particular set of generations.

[50:08](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3008s) What we realized is that having up to now 10x'd our training capacity for the various OpenAI models, we had enough for that. But the more important thing is to have a balance -- inference models deployed all around the world. The rate of monetization is what matters. And then the infrastructure was going to be there to support it. So once we said that's the case, we started to rationalize.

[50:47](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3047s) If I look at the path we're on, we are still ramping massively. We are also buying up as much managed capacity as we can, whether it's to lease, or even GPUs as a service. We need to balance the serving needs and our training needs.

[51:10](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3070s) You don't want to just have a massive book of GPU contracts with no monetization path. That's not a business, you should be more thoughtful. Given that OpenAI was going to be building their own first-party capacity -- which is fantastic, it makes sense -- and even their capacity is all going to be first-party eventually, they'll be a hyperscaler on their own. We need to figure out our own fleet and our own research compute.

[51:49](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3109s) By the way, the other thing is that I didn't want to be stuck with one GPU generation. We just saw the GB200s, the GB300s are coming. The data center of the future is going to look very different from this one. The cooling is going to be so different. The power requirements are going to be so different. I didn't want to just go build out a whole number of gigawatts and then be stuck.

[52:22](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3142s) So I think the pacing matters, the workload diversity matters, customer diversity matters. The other thing that we've learned is that real AI workloads require not only the AI accelerator, but CPU, storage, networking. In fact, a lot of the margin structure of Azure comes from all these other services. Therefore, we want to build out Azure as being a full-stack cloud, because that's the hyperscale business, while also offering bare-metal GPU as a service.

[53:02](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3182s) But that can't crowd out the rest of the Azure business. You don't want to become a business of just doing five contracts with five customers. That's not a great Microsoft business. That may be a business for others. What we have said is that we're in the business of being a long-tail business for AI workloads, from bare-metal-as-a-service capabilities all the way to full SaaS. And that, I think, is the balance you see.

[53:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3217s) There's this whole fungibility topic. You'd rather have data centers in a good population center than in some remote location. How much does that matter as the horizon of AI workloads expands? When you go from real-time inference to 30 minutes for a deep research query, or to hours and days for some tasks -- why does it matter if it's location A, B, or C?

[54:11](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3251s) That's one of the other reasons why we want to invest in the networking between Azure regions. As workloads evolve and the usage of these tokens evolves, you don't want to be out of position. There are data residency laws -- we literally had to create an EU Data Boundary. You need the ability to handle calls to wherever, even if it's asynchronous.

[54:51](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3291s) You're 100% right in bringing up the question of what shapes the hyperscale business. One, tokens per dollar per watt. What is the usage pattern? Synchronous, asynchronous. Because the latencies may matter for certain workloads. You may need a Cosmos DB close to this for session data. What will shape the hyperscale business is all of these factors together.

[55:36](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3336s) By 2028 you were going to be 12-13 gigawatts. But something that's even more relevant -- Google has gone from your size to bigger than you by the end of 2027. Their return on invested capital is outstanding. So the question is, maybe it's not Microsoft's place to be the biggest hyperscaler.

[56:16](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3376s) First of all, I don't want to take away from what Google is doing -- they're building their business and I wish them well. For us, it didn't make sense to just chase raw capacity with limited time horizon contracts. The thing you have to think through is not just what you do for the next 5 years, but what you do for the next 50.

[56:49](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3409s) I feel very good about our OpenAI partnership. We have a decent book of business. In fact, we are buyers of Oracle capacity. I think the industrial logic for what we are doing makes sense. I'm not about chasing -- first of all, I track every competitor's margins and growth against ours, which I think is super useful.

[57:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3457s) I take your point that it's a better business to have higher margin from a long tail of customers. But then there's a question of, if we believe we're on the path to smarter and smarter models, what is the long tail that enterprises need? Because you want to use Grok plus, say, OpenAI plus something else -- on one platform where you provision them, build your application. That is the hyperscale business. It's not just like, "I have a token factory."

[58:53](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3533s) There is a separate business called just selling raw GPU capacity. And that's a different question about how much of that Microsoft wants to be in and not be in. But that's kind of at least the way I look at it.

[59:38](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3578s) There are sort of two questions here. One is about OpenAI training and inference capacity, and the other is about actually just running Azure as a full cloud business. I could have just built all of it in Wisconsin, or I may want to build it in India, in different locations. Where we have the regulatory needs and the data sovereignty requirements -- first of all, stateside capacity is super important. But when I look out to 2030, I have a very clear picture of our book of business by first-party and third-party, and how much they want versus the inference needs and our own research compute needs.

[1:00:48](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3648s) You're rightfully pointing out the pause, but it wasn't "Oh my God, we don't want to build." It was that we needed to build slightly differently by both workload type and geography. We'll keep ramping up our gigawatts. And how do I ride Moore's law on it -- should I buy everything in 2027 or spread it across 2027-28 with better generations?

[1:01:25](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3685s) Even with Nvidia, their pace of new generations increased. I didn't want to go get stuck for four years with one generation. In fact, Jensen's advice to me was two things: move fast between generations. That's why the execution -- it's like 90 days between when we receive GPUs and when they're running. That's real speed-of-light deployment. I wanted to get good on that, not just good at building the biggest single generation.

[1:02:05](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3725s) That way you have something much more balanced. For a large-scale industrial operation like ours, you don't want a situation where you've built up a lot in one generation and then are stranded. Or in one location -- that might be great for training but not for inference because I can't serve European customers from Texas.

[1:02:32](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3752s) How do I rationalize this statement with the fact that you've announced deals with Iris and other neoclouds, and there's a few more coming? It's fine for us because when you have line of sight to demand, and people are building capacity, it's great. We will take build-to-suit, we'll even take existing capacity. We need capacity and someone else has that. In fact, we want neoclouds to be part of our marketplace. They feed their capacity into our marketplace, customers benefit -- it's a great win for everyone.

[1:03:38](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3818s) You mentioned how this is a depreciating asset -- GPUs depreciate over five years. And Jensen is taking a 75% margin on that. The natural response for every hyperscaler is to develop their own accelerator so that they can capture more of the value chain and increase their margins. Google's way ahead of everyone else. You look at Amazon and they're trying to make Trainium work. But when we look at what Microsoft is ordering from Nvidia, you don't seem to be displacing any. You've had a custom chip program for just as long.

[1:04:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3866s) It's a good question. A couple of things. The real competitor for any new accelerator is the next Nvidia GPU. In a fleet, what I'm going to deploy has to clear the bar. Even for our own Maia 200, which looks great, except that one generation later Nvidia will have something better. On the CPU compute side, we had a lot of Intel, then we switched to AMD, then built Cobalt. That's how we scaled it. We have good institutional knowledge of how to build your own silicon and then manage a fleet.

[1:05:08](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3908s) Because by the way, even Google's TPU strategy makes the most sense when you have your own models. It makes sense because Nvidia is innovating so fast -- all models run on it and customer demand is there. For custom silicon, you better have your own model which is either a first-party model or you have to generate your own demand. So therefore you want to make the co-design work.

[1:05:35](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3935s) The way we are going to do it is to have a close relationship between our MAI model development and our silicon development, because I feel like that's what gives you the real TCO advantage -- where you literally have designed the model and the silicon together, and then you keep pace with your own models.

[1:06:02](https://www.youtube.com/watch?v=8-boBsWcr5A&t=3962s) OpenAI has a custom chip program which we have access to. The only IP you don't get is the manufacturing secrets. That's it. By the way, we gave them a bunch of our IP. This is one of the reasons why they -- we built it for them and they evolved it. And now as they innovate, we first want to instantiate what they've built.

[1:06:50](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4010s) So if anything, the way I think about the custom silicon question -- it's about speed-of-light execution on Nvidia's latest, because quite frankly that fleet is life itself. It's not just about Nvidia margins, but the TCO has many dimensions. On top of that, I want to be able to co-design with our MAI lineage and the system design.

[1:07:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4046s) Speaking of rights, you had an interview a while back where you described the new agreement you made with OpenAI -- you have access to the weights, you have access to all API calls that OpenAI makes. Is there any state? So if there's any state whatsoever -- all these complicated workloads that run in ChatGPT need databases and storage and so forth. Is ChatGPT storing stuff on Azure sessions?

[1:07:56](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4076s) The agreement we made accommodates the flexibility that OpenAI needs. Essentially think of OpenAI having two businesses. Their SaaS business is ChatGPT. Their PaaS business is the API. For the SaaS business, they can run it anywhere. But for the PaaS business, where a partner wants to use a stateless API, then Azure is the platform.

[1:08:36](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4116s) It seems like there's a way for them to build SaaS without Azure? No, for even that they'll have to come to Azure. There are some few exceptions, but other than that, they'd have to come to Azure. That's what we value as part of our partnership.

[1:09:19](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4139s) We were good partners to OpenAI given the flexibility they needed. So for example, if Salesforce wants to integrate OpenAI models, they would work together, train a model together. Is that allowed or do they have to use Azure? They'd have to come to Azure to run it. There are some few exceptions, but other than that, they'd have to come to Azure.

[1:10:33](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4233s) Walking through the factory, one of the things you notice is that you can think of this as a software business, but there's all this capex. If you just look over the last two years of capital expenditure, maybe you extrapolate that forward, it actually looks like it might grow. Other hyperscalers are taking loans. Meta has done a corporate loan. It seems clear this is a capital-intensive transition.

[1:11:12](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4272s) I think the structural change is what matters. I describe it as we are now a capital-intensive knowledge business. In fact, we have to use our knowledge to differentiate capital. The hardware guys have done a phenomenal job, which I think is unbelievable and great. But if you even look at some of the stats -- the software improvements of really throughput optimization we've been able to get quarter-over-quarter, it's 5x, 10x, maybe 40x in some of these benchmarks. That's knowledge intensity coming back.

[1:12:11](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4331s) That, at some level, is what we have to master. What's the difference between a classic old-time hoster and a hyperscaler? It's not the fleet -- but as long as you have systems know-how, software know-how, you can differentiate. That's why when we say fungibility, it's not just about the fleet. It's about being able to finish one workload and then schedule another workload efficiently. That is the type of stuff that creates the moat.

[1:12:54](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4374s) So yes, I think we'll still remain a software company that owns capital and we're going to manage it. The cash flow that Microsoft has allows us to make these investments.

[1:13:12](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4392s) It seems like in the short term, Microsoft's bet is on things taking a while, being more jagged. But what if the people who talk about AGI and ASI are correct? Sam Altman and others -- the question about what makes sense for a hyperscaler investing in this thing which depreciates over five years when the thing that somebody like Sam anticipates may arrive sooner.

[1:13:49](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4429s) There's research compute -- that's the best way to think about the leading edge. We should think of it as just R&D expense. How do you want to scale it? That needs to grow at an order of magnitude scale in some period -- is it 16 months? What have you. That's one thing. The rest is all demand driven. You can build a little ahead of demand, but you better have a demand signal.

[1:14:33](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4473s) Do you buy the labs' projections? These labs are now projecting massive revenue growth. In the marketplace there's all kinds of uncertainty. What do you expect? They have to put some numbers out there to raise money so that they can pay their bills and do research. That's a good thing. Someone's going to take some risk. It's not like it's all risk without seeing revenue -- whether it's OpenAI, or whether it's Anthropic. And we have a massive book of business with these companies.

[1:15:24](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4524s) Ultimately, there's two simple things. You brought up talent. The talent for AI research -- you've got to spend there. You've got to spend on compute. Those ratios have to be high. To be a leading R&D company in this world, you have to have both talent and compute, and you have to have a balance sheet that allows you to invest wisely.

[1:16:01](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4561s) As we look across the world, America owns a disproportionate share of tech. The US owns Windows through Microsoft, that's the main operating system. Linux is open source, but Windows is deployed everywhere. You look at Word, it's deployed everywhere. Office is deployed everywhere. But companies have grown elsewhere -- in Europe and in India and in all these other places.

[1:16:35](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4595s) In all of these different places, you're building data centers. But the political aspect of technology has changed. Governments didn't care about the dot-com bubble. But every administration now cares about AI. The question is, we're sort of in a bipolar world. India and all these other countries are saying, "We want our own AI capability." How does Microsoft navigate the difference?

[1:17:07](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4627s) How does Microsoft navigate from a world where the US is dominant, American companies benefit massively, to a world where it is bipolar? There's no birthright to win all of Europe or India or Singapore. What is your thought process here?

[1:17:30](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4650s) It's a super critical piece. I think that the most important thing for American companies and the US government is to ensure that we maintain our lead, but that we also collectively build trust globally. Because the United States is just an extraordinary anomaly in history. It's 4% of the world's population, and I think you should think about why 50% of tech value comes from the US. That 50% happens because quite frankly the US has built extraordinary institutions -- its capital markets, its rule of law, its stewardship of what matters at any given moment.

[1:18:34](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4714s) If that is broken, then that's not good for anyone. We start with that, which I think President Trump and everyone really gets. What I would like is for the United States government and the American companies to put our own capital at risk collectively. I would like the USG to take credit for the success of American companies all over the world. That's the kind of marketing that the United States should be doing -- not just attracting investment into the United States, but making sure jobs and growth are created all over the world.

[1:19:27](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4767s) So you start there, and then you even build on countries' legitimate sovereignty concerns -- their continuity, for them to have real agency. In fact, our European business has been strong. We made a series of commitments to Europe on how we would operate such that the European Union and the member states feel comfortable.

[1:20:06](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4806s) We're also building sovereign cloud services. We have something called Sovereign Services on Azure -- services along with confidential computing. We've done great innovative work with Nvidia to build, both technically and through policy, trust in the platform.

[1:20:36](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4836s) How do you see this shaking out as you have sovereignty concerns at the hyperscaler level and things on the model level? Do you expect countries to say, "Look, it's clear one model or a couple models are the best, but we're going to have some laws around their use?" Or do you expect that there will be this push so that every country has their own model capability?

[1:21:03](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4863s) Maybe an analogy is semiconductors -- people would like to have their sovereign semiconductor production capability. Semiconductors are so important to national security. But ultimately, what matters is the use of AI in the economy. That's the diffusion theory, which is more about the ability to use the leading technology. So I think that will drive the market structure.

[1:21:42](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4902s) But that said, they will want continuity. I believe there's always going to be a concern: "What happens with runaway deployment?" There will always be a desire for multiple models. That'll be one way for countries to mitigate concentration risk. "I want an open source model too." Every country will feel like, "Okay, I don't want to be dependent on one provider," and broadly diffusing because I can always switch to another model, whether it's open source or a competitor. Concentration risk and sovereignty -- these forces will drive the market structure.

[1:22:45](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4965s) This doesn't exist for semiconductors in the same way. It didn't exist until now. But this is something new -- if you turn off the models, it's not like losing cars or refrigerators. It's losing a real fraction of your productive capacity. It's worthwhile having redundancy, it's worthwhile having real sovereignty. We're a global economy.

[1:23:17](https://www.youtube.com/watch?v=8-boBsWcr5A&t=4997s) At this point, we've not learned anything about supply chain resilience if we just ignore sovereignty concerns. Any nation state, including the United States, understands what it takes to be more self-sufficient on critical technology. So I, as a multinational company, have to respect that.

[1:23:46](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5026s) If I don't, then I'm not respecting what is in every country's legitimate interest. I'm not saying they won't make tradeoffs. Absolutely, globalization can't just be rewound. But at the same time, no company can go to Washington and say, "Hey, we're not going to listen to your concerns" -- you'd be kicked out of the United States. The same is true in every other country too.

[1:24:26](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5066s) We need to respect what the lessons learned are, whether from semiconductors or other supply chains. People are saying, "Yes, globalization helped supply chains be efficient. But there's such a thing called resilience." So therefore that feature will get built. And that is the point you are making. It's not like saying all the TSMC plants are now in Arizona. That's not going to be. But is there a plan? There is a plan. Absolutely. So I feel that that's the world.

[1:25:11](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5111s) Each country will want to work with companies that respect their view and build for their needs, as opposed to companies that don't. And Microsoft is especially privileged here because you've been operating in all of these countries for decades. You have expertise in setting up operations in different regulatory environments. Therefore Microsoft is uniquely fit for a world where sovereignty matters.

[1:25:42](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5142s) I don't want to sort of describe it as a unique advantage. I would just say I think of that as a business that we've built through decades of work, and we plan to continue investing in it. My approach -- whether it's in the United States, Europe, or India -- has always been: "We want you to allocate wafer starts here, or we want you to build data centers here." To me, respecting what are legitimate national interests while building for it as a software and infrastructure company -- that's the job.

[1:26:34](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5194s) As we go to the bipolar world -- US versus China -- or you versus Anthropic, or you versus Google. How does America rebuild the trust? To say, "Actually, no, American companies are trustworthy." And how do you think about competition with Chinese companies? Whether that be ByteDance and Alibaba or DeepSeek and Moonshot?

[1:27:10](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5230s) It's a great question. You were talking about how AI is becoming this industrial capability that will diffuse quickly across all supply chains. You just think about China -- this is their moment to get good at something that matters. They're not going to moonshot to ASI next year, but they're going to be very competitive. How do you deal with Chinese competition?

[1:27:37](https://www.youtube.com/watch?v=8-boBsWcr5A&t=5257s) It's a great question. In fact, trust in tech is probably the most important feature. It is, "Can I trust you, the company, and can I trust your institutions to be a long-term supplier?" That's a good note to end on. Thank you so much. Thank you. It's awesome. You two guys are quite the team.
