---
video_id: _zgnSbu5GqE
title: "What are we scaling?"
channel: Dwarkesh Patel
duration: 754
duration_formatted: "12:34"
view_count: 101834
upload_date: 2025-12-23
url: https://www.youtube.com/watch?v=_zgnSbu5GqE
thumbnail: https://i.ytimg.com/vi/_zgnSbu5GqE/maxresdefault.jpg
tags:
  - AI
  - scaling
  - reinforcement-learning
  - AGI
  - continual-learning
  - diffusion
  - intelligence-explosion
  - essay
  - on-the-job-learning
  - RLVR
---

# What are we scaling?

## Summary

In this narrated essay, Dwarkesh Patel lays out a provocative thesis about the current state of AI progress: people with short AGI timelines who are simultaneously bullish on scaling reinforcement learning from verifiable reward (RLVR) are holding contradictory positions. If we were truly close to a human-like learner, the entire approach of pre-baking skills into models through expensive RL training environments would be unnecessary -- humans do not need to rehearse every piece of software before using it on the job. The fact that labs are investing billions in building specialized training loops reveals that the models fundamentally lack the on-the-job learning capabilities that make human workers valuable.

Patel argues that the "economic diffusion lag" explanation for why AI has not yet generated trillions in revenue is cope -- if models were truly at AGI level, they would diffuse into the economy faster than human workers, not slower. He contends that some amount of goalpost-shifting on AGI definitions is actually justified, since capabilities we previously thought sufficient for AGI have been achieved without producing the expected economic impact. The essay concludes with a forward-looking argument that continual learning -- not software singularity or hardware improvements -- will be the main driver of further AI progress, and that solving it will be a gradual process rather than a single breakthrough moment.

## Highlights

### "If we're close to a humanlike learner, RL scaling is doomed"

<iframe width="560" height="315" src="https://www.youtube.com/embed/_zgnSbu5GqE?start=0&end=50" frameborder="0" allowfullscreen></iframe>

> "I'm confused why some people have super short timelines yet at the same time are bullish on scaling up reinforcement learning atop LLMs. If we're actually close to a humanlike learner, then this whole approach of training on verifiable outcomes is doomed."
> -- Dwarkesh Patel, [0:00](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=0s)

### "Robotics proves the learning gap"

<iframe width="560" height="315" src="https://www.youtube.com/embed/_zgnSbu5GqE?start=70&end=130" frameborder="0" allowfullscreen></iframe>

> "In some fundamental sense, robotics is an algorithms problem, not a hardware or a data problem. With very little training, a human can learn how to operate current hardware to do useful work. So if you actually had a humanlike learner, robotics would be in large part a solved problem."
> -- Dwarkesh Patel, [1:10](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=70s)

### "Diffusion lag is cope"

<iframe width="560" height="315" src="https://www.youtube.com/embed/_zgnSbu5GqE?start=309&end=370" frameborder="0" allowfullscreen></iframe>

> "Sometimes people will say that the reason that AIs aren't more widely deployed right now is that technology takes a long time to diffuse. And I think this is cope. If these models actually were like humans on a server, they'd diffuse incredibly quickly."
> -- Dwarkesh Patel, [5:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=309s)

### "Some goalpost shifting is actually justified"

<iframe width="560" height="315" src="https://www.youtube.com/embed/_zgnSbu5GqE?start=399&end=475" frameborder="0" allowfullscreen></iframe>

> "If you showed me Gemini 3 in 2020, I would have been certain that it could automate half of knowledge work. And so we keep solving what we thought were the sufficient bottlenecks to AGI. We have models that have general understanding. They have few-shot learning. They have reasoning. And yet we still don't have AGI."
> -- Dwarkesh Patel, [6:39](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=399s)

### "Continual learning is the real singularity driver"

<iframe width="560" height="315" src="https://www.youtube.com/embed/_zgnSbu5GqE?start=567&end=630" frameborder="0" allowfullscreen></iframe>

> "All these scenarios neglect what I think will be the main driver of further improvements at top AGI: continual learning. Again, think about how humans become more capable than anything. It's mostly from experience in the relevant domain."
> -- Dwarkesh Patel, [9:27](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=567s)

## Key Points

- **RL scaling contradiction** ([0:00](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=0s)) - Short AGI timelines are contradicted by the need for massive RL training environments to teach models specific skills
- **Pre-baking skills reveals a gap** ([0:17](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=17s)) - Labs building RL environments to teach web browsing, Excel, etc. reveals models cannot learn on the job like humans
- **Robotics as the litmus test** ([1:10](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=70s)) - Robotics is fundamentally an algorithms problem; a humanlike learner would make it largely solved
- **The "automated Ilya" argument fails** ([1:35](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=95s)) - The idea that clunky RL will produce a superhuman AI researcher who then solves real learning is implausible
- **Context-specific skills matter** ([2:57](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=177s)) - People underrate how much company-specific and context-specific knowledge is required for most jobs
- **The macrophage anecdote** ([3:15](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=195s)) - A biologist's lab task of identifying macrophages illustrates why bespoke training loops for every micro-task are not economically viable
- **Actual AI will be bigger than expected** ([4:48](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=288s)) - People underestimate how transformative true brain-like AI will be because they only imagine incremental improvements to current models
- **Diffusion lag is cope** ([5:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=309s)) - AGI-level models would diffuse faster than human workers, not slower -- no lemons market, instant onboarding
- **Revenue as AGI test** ([6:17](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=377s)) - If capabilities were at AGI level, people would spend trillions per year on tokens; labs are orders of magnitude off this
- **Justified goalpost shifting** ([6:39](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=399s)) - Previous definitions of AGI were too narrow; Gemini 3 shown in 2020 would have seemed like it could automate half of knowledge work
- **RL scaling lacks public evidence** ([8:28](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=508s)) - People are laundering the prestige of pre-training scaling laws to justify bullish predictions about RLVR with no publicly known trend
- **Toby Ord's bearish analysis** ([9:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=549s)) - Analysis of O-series benchmarks suggests a million-fold scale-up in RL compute would be needed for significant gains
- **Continual learning as the main driver** ([9:27](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=567s)) - The intelligence explosion will be driven by continual learning, not software or hardware singularity
- **Hive mind distillation** ([9:58](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=598s)) - Future may look like specialized agents doing different jobs and bringing learnings back to a central model for batch distillation
- **Continual learning will be gradual** ([10:26](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=626s)) - Like in-context learning, continual learning will improve incrementally over 5-10 years rather than arriving as a single breakthrough
- **Competition neutralizes advantages** ([11:44](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=704s)) - Talent poaching, the SF rumor mill, and reverse engineering have neutralized any runaway advantage a single lab might have had

## Mentions

### Companies
- **OpenAI** ([1:45](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=105s)) - Referenced as pursuing automated AI researcher approach
- **Google/Gemini** ([7:10](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=430s)) - Gemini 3 referenced as a model that would have seemed like AGI in 2020

### Products & Technologies
- **Reinforcement Learning from Verifiable Reward (RLVR)** ([0:00](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=0s)) - Central topic; Patel argues its necessity reveals models lack humanlike learning
- **GPT-3** ([10:33](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=633s)) - Referenced as demonstrating in-context learning was powerful but not fully solved
- **O-series models** ([9:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=549s)) - Referenced via Toby Ord's analysis of their RL scaling benchmarks

### People
- **Beren Millidge** ([0:46](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=46s)) - Blog post on how most algorithmic progress is data progress, highlighting role of expert annotation
- **Ilya Sutskever** ([1:45](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=105s)) - Referenced as the hypothetical "automated Ilya" for solving AGI
- **Toby Ord** ([9:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=549s)) - Analysis connecting O-series benchmarks showing bearish RL scaling implications
- **Andrej Karpathy** ([10:19](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=619s)) - Referenced concept of "cognitive core" plus specialized knowledge for agents
- **Sam Altman** ([11:21](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=681s)) - Referenced as saying fully solved continual learning would be "game set match"

## Surprising Quotes

> "This just gives me the vibes of that old joke: we're losing money on every sale, but we'll make it up in volume. Somehow, this automated researcher is going to figure out the algorithm for AGI, which is a problem that humans have been banging their head against for the better half of a century, while not having the basic learning capabilities that children have."
> -- [1:52](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=112s)

> "It is not possible to automate even a single job by just baking in a predefined set of skills, let alone all the jobs. In fact, I think people are really underestimating how big a deal actual AI will be because they are just imagining more of this current regime."
> -- [4:40](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=280s)

> "Models keep getting more impressive at the rate that the short-timelines people predict, but more useful at the rate that the long-timelines people predict."
> -- [8:18](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=498s)

> "People are trying to launder the prestige that pre-training scaling has, which is almost as predictable as a physical law of the universe, to justify bullish predictions about reinforcement learning from verifiable reward, for which we have no publicly known trend."
> -- [8:46](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=526s)

> "I expect actual brain-like intelligences within the next decade or two, which is pretty wild."
> -- [5:01](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=301s)

## Transcript

[0:00](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=0s) I'm confused why some people have super short timelines yet at the same time are bullish on scaling up reinforcement learning atop LLMs. If we're actually close to a humanlike learner, then this whole approach of training on verifiable outcomes is doomed. Currently the labs are trying to bake in a bunch of skills into these models through mid-training. There's an entire supply chain of companies that are building RL environments which teach the model how to navigate a web browser or use Excel to build financial models. Now either these models will soon learn on the job in a self-directed way which will make all this pre-baking pointless, or they won't, which means that AGI is not imminent.

[0:40](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=40s) Humans don't have to go through the special training phase where they need to rehearse every single piece of software that they might ever need to use on the job. Beren Millidge made an interesting point about this in a recent blog post he wrote. He writes, quote, "When we see frontier models improving at various benchmarks, we should think not just about the increased scale and the clever ML research ideas, but the billions of dollars that are paid to PhDs, MDs, and other experts to write questions and provide example answers and reasoning targeting these precise capabilities."

[1:10](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=70s) You can see this tension most vividly in robotics. In some fundamental sense, robotics is an algorithms problem, not a hardware or a data problem. With very little training, a human can learn how to operate current hardware to do useful work. So if you actually had a humanlike learner, robotics would be in large part a solved problem. But the fact that we don't have such a learner makes it necessary to go out into a thousand different homes and practice a million times on how to pick up dishes or fold laundry.

[1:35](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=95s) Now, one counter-argument I've heard from the people who think we're going to have a takeoff within the next 5 years is that we have to do all this clunky RL in service of building a superhuman AI researcher. And then the million copies of this automated Ilya can go figure out how to solve robust and efficient learning from experience. This just gives me the vibes of that old joke: we're losing money on every sale, but we'll make it up in volume. Somehow, this automated researcher is going to figure out the algorithm for AGI, which is a problem that humans have been banging their head against for the better half of a century, while not having the basic learning capabilities that children have. I find it super implausible.

[2:12](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=132s) Besides, even if that's what you believe, it doesn't describe how the labs are approaching reinforcement learning from verifiable reward. You don't need to pre-bake in a consultant skill at crafting PowerPoint slides in order to automate Ilya. So clearly, the labs' actions hint at a worldview where these models will continue to fare poorly at generalization and on-the-job learning, thus making it necessary to build in the skills that we hope will be economically useful beforehand into these models.

[2:38](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=158s) Another counter-argument you can make is that even if the model could learn these skills on the job, it is just so much more efficient to build in these skills once during training rather than again for each user and each company. And look, it makes a ton of sense to just bake in fluency with common tools like browsers and terminals. And indeed, one of the key advantages that AGIs will have is this greater capacity to share knowledge across copies. But people are really underrating how much company and context-specific skills are required to do most jobs. And there just isn't currently a robust, efficient way for AIs to acquire those on the fly.

[3:15](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=195s) I was recently at a dinner with an AI researcher and a biologist. And it turned out the biologist had long timelines. And so we were asking about why she had these long timelines. And then she said, you know, one part of work recently in the lab has involved looking at slides and deciding if the dot in that slide is actually a macrophage or just looks like a macrophage. And the AI researcher, as you might anticipate, responded, look, image classification is a textbook deep learning problem. This is dead center in the kind of thing that we could train these models to do.

[3:48](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=228s) And I thought this is a very interesting exchange because it illustrated a key crux between me and the people who expect transformative economic impact within the next few years. Human workers are valuable precisely because we don't need to build in the training loops for every single small part of their job. It's not net productive to build a custom training pipeline to identify what macrophages look like given the specific way that this lab prepares slides and then another training loop for the next lab-specific micro-task and so on. What you actually need is an AI that can learn from semantic feedback or from self-directed experience and then generalize the way a human does.

[4:25](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=265s) Every day you have to do a hundred things that require judgment, situational awareness, and skills and context that are learned on the job. These tasks differ not just across different people but even from one day to the next for the same person. It is not possible to automate even a single job by just baking in a predefined set of skills, let alone all the jobs. In fact, I think people are really underestimating how big a deal actual AI will be because they are just imagining more of this current regime. They're not thinking about billions of humanlike intelligences on a server which can copy and merge all the learnings. And to be clear, I expect this, which is to say I expect actual brain-like intelligences within the next decade or two, which is pretty wild.

[5:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=309s) Sometimes people will say that the reason that AIs aren't more widely deployed right now across firms and already providing lots of value outside of coding is that technology takes a long time to diffuse. And I think this is cope. I think people are using this to gloss over the fact that these models just lack the capabilities that are necessary for broad economic value. If these models actually were like humans on a server, they'd diffuse incredibly quickly. In fact, they'd be so much easier to integrate and onboard than a normal human employee is. They could read your entire Slack and drive within minutes. And they could immediately distill all the skills that your other AI employees have.

[5:45](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=345s) Plus, the hiring market for humans is very much like a lemons market where it's hard to tell who the good people are beforehand. And then obviously hiring somebody who turns out to be bad is very costly. This is just not a dynamic that you would have to face or worry about if you're just spinning up another instance of a vetted AI model. So for these reasons, I expect it's going to be much easier to diffuse AI labor into firms than it is to hire a person. And companies hire people all the time. If the capabilities were actually at AGI level, people would be willing to spend trillions of dollars a year buying tokens that these models produce. Knowledge workers across the world cumulatively earn tens of trillions of dollars a year in wages. And the reason that labs are orders of magnitude off this figure right now is that the models are nowhere near as capable.

[6:39](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=399s) Now you might be like, look, how can the standard have suddenly become "labs have to earn tens of trillions of dollars of revenue a year"? Until recently people were saying, can these models reason? Do these models have common sense? Are they just doing pattern recognition? And obviously AI bulls are right to criticize AI bears for repeatedly moving these goalposts, and this is very often fair. It's easy to underestimate the progress that AI has made over the last decade. But some amount of goalpost shifting is actually justified. If you showed me Gemini 3 in 2020, I would have been certain that it could automate half of knowledge work.

[7:15](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=435s) And so we keep solving what we thought were the sufficient bottlenecks to AGI. We have models that have general understanding. They have few-shot learning. They have reasoning. And yet we still don't have AGI. So what is a rational response to observing this? I think it's totally reasonable to look at this and say, "Oh, actually there's much more to intelligence and labor than I previously realized." And while we're really close and in many ways have surpassed what I would have previously defined as AGI in the past, the fact that model companies are not making the trillions of dollars in revenue that would be implied by AGI clearly reveals that my previous definition of AGI was too narrow.

[7:55](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=475s) And I expect this to keep happening into the future. I expect that by 2030, the labs will have made significant progress on my hobby horse of continual learning and the models will be earning hundreds of billions of dollars in revenue a year, but they won't have automated all knowledge work. And I'll be like, look, we made a lot of progress, but we haven't hit AGI yet. We also need these other capabilities. We need X, Y, and Z capabilities in these models. Models keep getting more impressive at the rate that the short-timelines people predict, but more useful at the rate that the long-timelines people predict.

[8:28](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=508s) It's worth asking: what are we scaling with pre-training? We had this extremely clean and general trend in improvement in loss across multiple orders of magnitude in compute. Albeit this was on a power law, which is as weak as exponential growth is strong. But people are trying to launder the prestige that pre-training scaling has, which is almost as predictable as a physical law of the universe, to justify bullish predictions about reinforcement learning from verifiable reward, for which we have no publicly known trend. And when intrepid researchers do try to piece together the implications from scarce public data points, they get pretty bearish results. For example, Toby Ord has a great post where he cleverly connects the dots between the different O-series benchmarks, and this suggested to him that we need something like a million-fold scale-up in total RL compute to give a similar boost.

[9:27](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=567s) So people have spent a lot of time talking about the possibility of a software singularity where AI models will write the code that generates a smarter successor system, or a software-plus-hardware singularity where AIs also improve their successor's computing hardware. However, all these scenarios neglect what I think will be the main driver of further improvements at top AGI: continual learning. Again, think about how humans become more capable than anything. It's mostly from experience in the relevant domain.

[9:58](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=598s) Over conversation, Beren Millidge made this interesting suggestion that the future might look like continual learning agents who are all going out and they're doing different jobs and they're generating value and then they're bringing back all their learnings to the hive mind model which does some kind of batch distillation on all of these agents. The agents themselves could be quite specialized, containing what Karpathy called the cognitive core plus knowledge and skills relevant to the job they're being deployed to do.

[10:26](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=626s) Solving continual learning won't be a singular one-and-done achievement. Instead, it will feel like solving in-context learning. Now, GPT-3 already demonstrated in-context learning could be very powerful in 2020. Its in-context learning capabilities were so remarkable that the title of the GPT-3 paper was "Language Models are Few-Shot Learners." But of course, we didn't solve in-context learning when GPT-3 came out. And indeed, there's still plenty of progress that still has to be made from comprehension to context length. I expect a similar progression with continual learning. Labs will probably release something next year which they call continual learning and which will in fact count as progress towards continual learning. But human-level on-the-job learning may take another 5 to 10 years to iron out.

[11:09](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=669s) This is why I don't expect some kind of runaway gains from the first model that cracks continual learning. If you had fully solved continual learning drop out of nowhere, then sure, it might be game, set, match, as Sam Altman put it on the podcast when I asked him about this possibility. But that's probably not what's going to happen. Instead, some lab is going to figure out how to get some initial traction on this problem and then playing around with this feature will make it clear how it was implemented and then other labs will soon replicate the breakthrough and improve it slightly.

[11:41](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=701s) Besides, I just have some prior that the competition will stay pretty fierce between all these model companies. And this is informed by the observation that all these previous supposed flywheels, whether that's user engagement on chat or synthetic data or whatever, have done very little to diminish the greater and greater competition between model companies. Every month or so, the big three model companies will rotate around the podium, and the other competitors are not that far behind. There seems to be some force -- and this is potentially talent poaching, it's potentially the rumor mill in SF, or just normal reverse engineering -- which has so far neutralized any runaway advantage that a single lab might have had.

[12:16](https://www.youtube.com/watch?v=_zgnSbu5GqE&t=736s) This was a narration of an essay that I originally released on my blog at dwarkesh.com. I'm going to be publishing a lot more essays. I found it's actually quite helpful in ironing out my thoughts before interviews. If you want to stay up to date with those, you can subscribe at dwarkesh.com. Otherwise, I'll see you for the next episode.
