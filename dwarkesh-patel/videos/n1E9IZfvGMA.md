---
video_id: n1E9IZfvGMA
title: "Dario Amodei â€” \"We are near the end of the exponential\""
channel: Dwarkesh Patel
duration: 8540
duration_formatted: "2:22:20"
view_count: 717139
upload_date: 2026-02-13
url: https://www.youtube.com/watch?v=n1E9IZfvGMA
thumbnail: https://i.ytimg.com/vi_webp/n1E9IZfvGMA/maxresdefault.webp
tags:
  - AI
  - Anthropic
  - scaling
  - AGI
  - Claude
  - Claude-Code
  - reinforcement-learning
  - continual-learning
  - economics
  - regulation
  - China
  - safety
  - alignment
  - constitution
  - coding
  - diffusion
  - compute
  - bioterrorism
  - geopolitics
  - Dario-Amodei
---

# Dario Amodei -- "We are near the end of the exponential"

## Summary

In this extensive two-and-a-half-hour interview, Anthropic CEO Dario Amodei returns to the Dwarkesh Podcast three years after his first appearance and argues that we are near the end of the exponential curve of AI capabilities -- meaning not that progress is stopping, but that the "country of geniuses in a data center" may arrive within one to three years. He puts 90% probability on achieving this milestone within a decade and has a strong hunch it will happen much sooner, though he is careful to distinguish between raw capability and economic deployment, which he believes will follow a "middle path" -- not instant, not slow, but much faster than any previous technology diffusion.

The conversation covers extraordinary ground: the nature of RL scaling and why Amodei sees it as analogous to the transition from GPT-1 to GPT-2 (training on broader distributions rather than narrow skills); Anthropic's 10x annual revenue growth from $100M to $1B in 2024 and continuing to accelerate; why Amodei believes compute investment must balance uncertainty about timing; the spectrum from "90% of code written by AI" to "100% of the job done by AI"; why continual learning may be solved within one to three years; and his detailed economic model of how AI labs will achieve profitability through roughly 50% of compute going to training and 50% to inference.

The second half dives deep into geopolitics and governance: why both US and China having "a country of geniuses in a data center" could be dangerous; his vision for how democratic coalitions should set "rules of the road" before authoritarian states gain equivalent capabilities; his nuanced position on AI regulation where he opposes a blanket federal moratorium on state AI laws while acknowledging most individual state bills are poorly designed; and his detailed philosophy on Claude's constitution, where he advocates for a mostly "corrigible" model with principled limits rather than rigid rules, and describes Anthropic's DVQ (Dario Vision Quest) biweekly all-hands as a management innovation for maintaining transparency at scale.

## Highlights

### "I'm at 90% on a country of geniuses in a data center"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=831s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*13:51-14:55" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-13m51s.mp4"
```
</details>

> "On the basic hypothesis of, as you put it, 'a country of geniuses in a data center,' I'm at 90% on that. There's another 5% which is that I'm very wrong about the science. With coding, I think we'll be there in one or two years in terms of being able to do end-to-end coding."
> -- Dario Amodei, [13:51](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=831s)

### "If we had a country of geniuses in a data center, we would know it"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1666s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*27:46-28:10" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-27m46s.mp4"
```
</details>

> "I don't believe we're basically at AGI. If we had a 'country of geniuses in a data center,' we would know it. Everyone in Washington would know it. But we would know it. We don't."
> -- Dario Amodei, [27:46](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1666s)

### "There is zero time for bullshit"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2153s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*35:53-36:40" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-35m53s.mp4"
```
</details>

> "Within Anthropic, this is just really unambiguous. There is zero time for bullshit. Why do you think we're concerned? Because we think we're ahead of the competitors."
> -- Dario Amodei, [35:53](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2153s)

### "Revenue growth: $100M to $1B in 2024, still accelerating"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1283s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*21:23-22:25" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-21m23s.mp4"
```
</details>

> "In 2024, it was $100 million to $1 billion. You would think it would slow down, but we've seen 10x per year growth in revenue. I would bet it stays pretty fast."
> -- Dario Amodei, [21:23](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1283s)

### "I can't buy $1 trillion a year of compute in 2027"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2780s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*46:20-47:20" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-46m20s.mp4"
```
</details>

> "I could buy $1 trillion of compute. If my revenue is not $1 trillion dollars, there's no hedge on earth that could stop me from going bankrupt. Even though a part of my brain wonders... I can't buy $1 trillion a year of compute in 2027."
> -- Dario Amodei, [46:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2780s)

### "Dictatorships become morally obsolete"

[![Clip](https://img.youtube.com/vi/n1E9IZfvGMA/hqdefault.jpg)](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7570s)
<details>
<summary>Clip command</summary>

```bash
yt-dlp --download-sections "*126:10-127:10" "https://www.youtube.com/watch?v=n1E9IZfvGMA" --force-keyframes-at-cuts --merge-output-format mp4 -o "n1E9IZfvGMA-126m10s.mp4"
```
</details>

> "I am actually hopeful that -- it sounds too optimistic -- dictatorships become morally obsolete. Just as the feudal system was basically a form of government, and when technological change happened, it was no longer sustainable."
> -- Dario Amodei, [2:06:10](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7570s)

## Key Points

- **Scaling hypothesis confirmed** ([0:10](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=10s)) - The exponential of AI capabilities has proceeded roughly as Amodei expected, from "smart high school student to smart college student" level, with code reaching beyond that
- **RL scaling is like pre-training's GPT-1 to GPT-2 transition** ([6:36](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=396s)) - Current RL is training on narrow tasks like math; as it broadens to code and general tasks, generalization will emerge just as it did when pre-training moved from narrow datasets to Common Crawl
- **LLM training is between evolution and human learning** ([9:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=560s)) - Models start from random weights (like evolutionary blank slate) but then show in-context learning capacity of a million tokens, falling somewhere between evolutionary timescales and human on-the-spot learning
- **90% probability of "country of geniuses" within 10 years** ([13:51](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=831s)) - Amodei gives 90% on achieving AGI-level capabilities, with remaining uncertainty split between catastrophic world events (5%) and being fundamentally wrong about the science (5%)
- **End-to-end coding in 1-2 years** ([14:54](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=894s)) - Models will handle complete software engineering end-to-end within one to two years, though harder scientific tasks requiring verification will take longer
- **Spectrum of AI coding capability** ([18:07](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1087s)) - There is a wide spectrum from "90% of code written by AI" (already happening) to "100% of the SWE job done by AI" including design decisions, memos, and setting technical direction
- **10x annual revenue growth** ([21:23](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1283s)) - Anthropic grew from $100M to $1B in 2024, with first month of 2025 continuing the trend; Amodei expects the curve to bend somewhat but remain very fast
- **Economic diffusion is real but not cope** ([24:34](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1474s)) - Diffusion is a real phenomenon (enterprise adoption of Claude Code takes months longer than individual adoption) but AI will diffuse much faster than previous technologies
- **We are not at AGI yet** ([27:46](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1666s)) - If we had a "country of geniuses in a data center," everyone would know it -- Washington, the public, everyone. We don't.
- **Video editor test for AGI** ([30:42](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1842s)) - A true "country of geniuses" AI would be able to learn Dario's editing preferences on the job, the way a human video editor learns over months
- **Coding productivity study showed 20% downlift** ([35:23](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2123s)) - A major study found developers reported feeling more productive with AI but actually had 20% worse output; Amodei says within Anthropic the gains are "unambiguous"
- **AI research provides 15-20% total factor speedup** ([37:40](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2260s)) - Current AI coding tools provide about 15-20% total speedup; six months ago it was closer to 5% which didn't register
- **Continual learning may be solved in 1-3 years** ([46:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2780s)) - Amodei's hunch is that continual learning and the ability to serve as "a country of geniuses" comes in one to three years, with 95% confidence within 10 years
- **Compute purchasing dilemma** ([46:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2780s)) - Cannot buy $1 trillion in compute when revenue might be much less; must balance risk of over- and under-investment
- **Industry economics: ~50% training, ~50% inference** ([52:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3120s)) - In equilibrium, roughly half of compute goes to training and half to inference; the inference side has healthy gross margins, making the underlying business profitable
- **Oligopoly structure is likely stable** ([58:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3480s)) - High barriers to entry (capital + skill) mean a small number of firms will dominate, similar to cloud computing, with positive but not astronomical margins
- **Robotics may follow 1-2 years after AGI** ([1:06:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3990s)) - Once AI models achieve general capability, both robot design and control will be revolutionized, though hardware deployment adds additional time
- **Claude Code origin story** ([1:17:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4650s)) - Started as internal experimentation Dario encouraged; saw explosive internal adoption and launched externally based on product-market fit
- **Offense-dominant world risk** ([1:25:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5130s)) - Small number of AI players increases concentration risk; proliferation of model-building capability could create biological weapons threats
- **Federal AI moratorium is wrong** ([1:31:19](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5479s)) - Amodei opposes the federal moratorium on state AI laws because the federal government has no actual plan to replace state regulation
- **Bioterrorism regulation needed soon** ([1:37:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5820s)) - Could be as soon as later this year that targeted regulation around biological weapons risks is necessary
- **China strategy: deny chips, share cures** ([1:47:41](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=6461s)) - Cures and health benefits should flow to everyone; chips and data centers should not be shared with authoritarian regimes
- **Technology may dissolve authoritarianism** ([2:06:10](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7570s)) - Amodei is hopeful that AI technology could make dictatorships "morally obsolete" the way industrialization ended feudalism
- **Claude's constitution: mostly corrigible with principled limits** ([2:05:46](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7546s)) - Claude should mostly do what people want, with limits based on principles rather than rigid rules; this makes behavior more consistent and predictable
- **DVQ: Dario Vision Quest** ([2:17:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8220s)) - Biweekly all-hands where Amodei talks through strategy, models, products, and geopolitics honestly with the entire company, creating transparency at 2,500 people

## Mentions

### Companies
- **Anthropic** ([0:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=0s)) - Dario's company; central focus of discussion on revenue, compute, strategy
- **OpenAI** ([2:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=150s)) - Referenced in context of Dota and early scaling work
- **DeepMind** ([37:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2240s)) - Referenced as a competitor alongside OpenAI
- **Meta** ([58:40](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3520s)) - Amodei notes he always calls them Facebook
- **Jane Street** (sponsor mention) - Puzzle challenge with backdoors in language models
- **Labelbox** (sponsor mention) - RL tasks and environments for training
- **Mercury** (sponsor mention) - Personal banking accounts

### Products & Technologies
- **Claude Code** ([25:06](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1506s)) - Anthropic's coding tool; category leader, driving internal and external productivity
- **Claude CLI** ([1:17:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4650s)) - Original internal name before it became Claude Code
- **GPT-1** ([2:15](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=135s)) - Referenced as when Amodei first wrote about scaling hypotheses
- **GPT-2** ([8:07](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=487s)) - Transition to Common Crawl training that produced generalization
- **AlphaGo** ([2:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=150s)) - Early RL scaling example
- **AIME** ([5:05](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=305s)) - Math contest benchmark showing RL scaling
- **RLHF** ([3:52](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=232s)) - Higher-order reinforcement learning from human feedback
- **Computer use** ([32:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1920s)) - Anthropic's computer control feature for Claude

### People
- **Dario Amodei** ([0:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=0s)) - CEO of Anthropic, interviewee
- **Rich Sutton** ([5:31](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=331s)) - "The Bitter Lesson" author; described as "very non-LLM-pilled"
- **Alec Radford** ([6:47](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=407s)) - GPT-1 creator at OpenAI
- **Daniela Amodei** ([2:19:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8340s)) - Runs the company alongside Dario; credited with building Anthropic's culture

## Surprising Quotes

> "The most surprising thing is the lack of public recognition of how far we've come. To me, it is absolutely wild that you have people talking about the same tired, old bubble narratives when we are near the end of the exponential."
> -- [0:55](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=55s)

> "If the 'country of geniuses' comes and revenue is $10 trillion dollars, I could buy $1 trillion of compute. But if my revenue is not $1 trillion, there's no hedge on earth that could stop me from going bankrupt."
> -- [46:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2780s)

> "I think that particular law is dumb. It's just state legislators who just probably had little idea what they were voting on. They're like, 'AI models serving as emotional support? I don't want that to happen.'"
> -- [1:31:19](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5479s)

> "Models are good at different things. It's not just that Claude's good at one thing. It's more subtle than that. I think these things are actually quite differentiated -- more differentiation than you see in cloud."
> -- [59:10](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3550s)

> "One of my worries -- although it's also an exciting thing -- is that some very critical decision will be some decision nobody expected. Someone gives me this random half-page memo and I'm like, 'I don't know. I have to eat lunch. Let's do B.' That one decision could end up mattering enormously."
> -- [2:14:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8040s)

## Transcript

[0:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=0s) We talked three years ago. In your view, what has been the biggest difference? Broadly speaking, the exponential of AI capabilities has proceeded roughly as I expected it to go, give or take a year or two here and there. I wouldn't have predicted the specific direction of code, but it is roughly what I expected in terms of going from smart high school student to smart college student and in the case of code reaching beyond that. Roughly what I expected.

[0:55](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=55s) The most surprising thing is the lack of public recognition of how far we've come. To me, it is absolutely wild that you have people talking about the same tired, old bubble narratives when we are near the end of the exponential. That is what the exponential looks like right now.

[1:27](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=87s) The question we recorded three years ago was, "what's going to happen next?" I have a similar question now. At least from the public's point of view, three years ago we had this clear story: pre-training scaling across many orders of magnitude of compute was working. Now we have RL scaling and there's much less clarity. It's not even clear what the story is. Is it supposed to be teaching meta-learning?

[2:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=120s) I actually have the same hypothesis I've always had. I think I talked about it last time, but I wrote a document at OpenAI -- it wasn't about the scaling of language models specifically. When I wrote it GPT-1 had just come out. Back in those days there was robotics as a separate thing from language models, and there was RL that happened in AlphaGo and in Dota at OpenAI. It was written as a more general document. Rich Sutton wrote "The Bitter Lesson" a couple years later.

[2:57](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=177s) What it says is that all the cleverness, all the "we built in this special thing to do something" -- that doesn't matter very much. I think I listed seven factors. The second is the quantity of data. It needs to be a broad distribution. The fifth is that you need an objective function. The pre-training objective function, which is next-token prediction. Another is the RL objective -- you're going to go out and reach the goal, which is what you see in math and coding. And there's RLHF or higher-order versions of that. Then there's things around normalization or conditioning so that the big blob of compute flows in this relatively well-organized way.

[4:11](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=251s) That was the hypothesis, and I don't think I've seen very much to disconfirm it. The pre-training scaling laws were one example of it. Now it's been widely reported that it's continuing to give us gains. We're also seeing the same thing for RL -- a pre-training phase and then an RL phase on top of that. Even other companies have published things showing that you can plot the score of a model on math contests like AIME as a function of how long we've trained it, and it's not just math contests. We're seeing the same scaling in other domains.

[5:31](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=331s) You mentioned Rich Sutton and "The Bitter Lesson," and he's actually very non-LLM-pilled. But one way to paraphrase his objection is: a true human-like learner would not require all these billions of dollars' worth of RL environments to learn how to use Excel, how to use various software. The fact that we have to build in these skills suggests that we're actually lacking a core human learning algorithm.

[6:13](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=373s) That's the question. Why are we doing all this RL scaling instead of having something that's more human-like in its ability to learn on the fly? I think this should be thought of differently. It may be a red herring, but it may not matter. There is an interesting thing. Let me explain why I actually think it's a red herring to say that RL scaling is just about narrow skills.

[6:47](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=407s) If we look at pre-training back in 2017 when Alec Radford was doing GPT-1, he was training on small datasets that didn't represent a wide distribution of text. They did well on language modeling benchmarks -- a bunch of fanfiction, I think actually -- a small fraction of the text you can get, small datasets representing a narrow slice of what you see in the world. It didn't generalize well. It wouldn't generalize that well to other domains or all these measures of how well it did at various tasks. It was only when you trained over all the tasks you could scrape from something like Common Crawl -- GPT-2 -- that you started to get generalization.

[8:16](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=496s) We're starting first with simple RL tasks like math, and then we're going to broader training that involves things like code. I think then we're going to see broader generalization. So that kind of takes out the narrow skills objection. But there is a puzzle either way, which is that humans don't see trillions of words. There's a sample efficiency difference here. The models start from scratch. But we also see that once they're trained, they can learn and adapt within a context of a million tokens -- the only thing blocking long-horizon learning and adapting within that context.

[9:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=560s) I think there's something going on where the process of training LLMs maps somewhere between the process of humans learning and the process of human evolution. Our brain isn't just a blank slate. The language models literally start as random weights, whereas our brains come with rich architecture connected to all these inputs and outputs. I think of pre-training, and for that matter RL as well, as something between human evolution and human on-the-spot learning. So there's this hierarchy: there's evolution, there's human long-term learning, and there's just human reaction, and the models are hitting these levels but not necessarily at exactly the same points.

[10:40](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=640s) For example, if the analogy is that pre-training is like evolution and is not sample efficient, then if we're going to get the equivalent of a human agent from in-context learning, why are we investing so much in RL environments? There are companies whose work seems to be about teaching models how to use Slack, how to use whatever. I can't speak for the emphasis of anyone else. The goal is not to teach the model each specific skill, just as we don't do that within pre-training. We don't expose the model to every possible way text can appear. Rather, the model trains on a lot of things and from that learns to generalize. That was the transition from GPT-1 to GPT-2.

[11:53](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=713s) I've had these moments where I was like, "Oh yeah, you can give it data -- the cost of the house, the square feet of the house -- and it does linear regression." And it's never seen that exact thing before. With RL environments, the goal is very similar. We're trying to get a whole bunch of data, not to teach any specific skill, but because we want to generalize.

[12:39](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=759s) That makes sense. We're making progress toward AGI. Some people say we won't achieve AGI this century. You're saying we're hitting the end of the exponential. "We've been making progress since 2012." Obviously we're seeing in these models capabilities that neither evolution nor learning within a human lifetime does. What makes you think it's one to three years rather than decades?

[13:17](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=797s) There are two claims you could make. Starting with the weaker claim: when I first got into this in 2016, I wasn't sure. This was a 50/50 thing. I thought the scaling hypothesis was much more likely than anyone thinks. On the basic hypothesis of, as you put it, "a country of geniuses in a data center," I'm at 90% on that. There's about 5% for catastrophic events -- because the world is so unpredictable, where you get things like multiple companies' fabs getting blown up by missiles. You could construct a 5% world where that happens. There's another 5% which is that I'm very wrong about the science.

[14:54](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=894s) With coding, except for that last 5%, I think we'll be there in one or two years in terms of being able to do end-to-end coding. The thing that's harder, even on long timescales, is about tasks that are fundamentally hard to verify -- doing some fundamental scientific discovery. It's hard to verify those tasks. There may not be a reliable path to get there, but if there's any path, we'll find it. On the ten-year timeline I'm at 90%. I think it's crazy to say that this won't happen.

[15:48](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=948s) But the emphasis on verification hints to me that there may be a split. If you think about humans, we're both good at things for which we have verification and things for which we don't. We already see substantial generalization in both domains. I would describe this as a spectrum which will split apart over time, where we do all the verifiable things very well but we don't fully get there on unverifiable tasks. It's not a binary thing.

[16:47](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1007s) Even in verifiable domains, it's not clear to me you could replace a full software engineer. You are "a software engineer" in some sense, but the actual job involves setting technical direction, understanding requirements, writing long memos about your grand vision. That's part of the job. But SWE does involve design and architecture decisions, and the models are already pretty good at that.

[17:24](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1044s) Again, I'm making much weaker claims here than some. We're already almost there on coding. By what metric? There's one metric which is percentage of code written. If you consider other productivity improvements -- compilers write all the lines of software. The question is how many lines of code are written and how big the productivity improvement is.

[17:52](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1072s) I actually agree with you on this distinction for code and software engineering. Let me lay out the spectrum. I said the AI model will be writing 90% of code. That happened, at least at some places, including for many people downstream using our models. People thought I was saying that we won't need 90% of engineers. Those things are very far apart. The spectrum is: 90% of code is written by AI -- that's a big difference in productivity. Then there's things like compiling, setting up clusters, writing memos -- are done by the models. 100% of the tasks. Even when that happens, it doesn't mean we don't need software engineers. There are new higher-level things to be done.

[19:21](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1161s) In "Adolescence of Technology" I went through this. I actually totally agree with you on that. These are very different milestones, but we're proceeding through them super fast. The progress is going to happen fast, and it's going to matter a lot. But what I notice is that even in greenfield settings, people report starting a lot of projects but where is the renaissance of software, all these new things? At least so far, it doesn't seem like we see that.

[20:02](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1202s) Even if I never had to intervene with the code itself, jobs are complicated. Closing the loop on actually delivering something -- writing software or something -- how much of that is actually the coding? Maybe that should dilute our estimation of how close we are. I simultaneously agree with you that it's a spectrum, but at the same time, I think the progress is real.

[20:41](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1241s) You could have these two poles. One says: yes, AI is making progress. It's slow. It's going to take decades. Economic diffusion has become one of the main arguments -- we're not going to make AI progress that fast. The other axis is that we'll get recursive self-improvement. Can't you just draw an exponential curve? We're going to have Dyson spheres around the sun. I'm completely caricaturing the view, but both extremes exist.

[21:23](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1283s) But what we've seen from the beginning, at least at Anthropic, is about 10x per year growth in revenue. In 2024, it was $100 million to $1 billion. And the first month of this year continued that pace. You would think it would slow down, but we haven't seen that. Obviously that curve can't go on forever. I would even guess that it bends somewhat this year. But it stays on a pretty fast curve. I would bet it stays pretty fast for the foreseeable future.

[22:26](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1346s) So I think we should be thinking about this middle path. The capabilities aren't instant -- they take time because of economic factors. Because it's fiddly: "I have to do change management, but I have to change the security permissions, I had this old piece of software that was built and released and I have to rewrite it." You can tell the model to do that. So I think everything we've seen so far is consistent with this picture: there's an exponential that's the capability of the model, and then there's a slightly slower exponential that's downstream of that, which is the economic impact. Not instant, not slow, much faster than any previous technology.

[23:37](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1417s) When I look inside Anthropic, when I look at our own usage... Can I try a hot take on you? I feel like diffusion is cope that people say. When AI doesn't solve a problem, they're like, "oh, but it's a diffusion issue." You would think that the inherent advantages would solve the onboarding problem for new AIs getting onboarded. An AI can read your entire codebase. They can share all the knowledge that the previous AI learned. You don't have this adverse selection problem -- you can just hire copies of a vetted AI model.

[24:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1460s) People hire humans all the time and pay trillions in wages because they're useful, even though onboarding is harder. It should be much easier to diffuse AIs into the economy than it is to hire humans. I think diffusion is very real, but it has to do with limitations on the AI models. People are using it as kind of a buzzword to say this isn't a big deal. I'm not talking about how AI won't diffuse quickly -- I think AI will diffuse much faster than previous technologies.

[25:06](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1506s) I'll just give an example. There's Claude Code. If you're a developer, you can just use it. There is no reason why a developer at a big company can't adopt Claude Code as quickly as an individual developer. We do everything we can to promote it. Big enterprises, big financial companies, big tech companies adopt Claude Code much faster than enterprises historically adopt software. But it takes time. Any given feature will get adopted by the individual developers who are on the cutting edge many months faster than they will get adopted by large organizations.

[26:11](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1571s) There are just a number of factors. You have to provision it for everyone. The leaders of the company who are further away from the technology have to say, "Oh, it makes sense. This is what this Claude Code thing is. This is why it makes us more productive." Then they have to communicate it to the people two levels below. We have conversations like this every day. That's part of what's driving Anthropic's revenue to grow 20x or whatever.

[26:57](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1617s) Again, many enterprises are just moving much faster than historically. We're going to take shortcuts. They're moving much faster than with the ordinary API, which many of them use. But it's not an infinitely compelling product yet. When we have a "country of geniuses in a data center," it will be a compelling product enough maybe to get to hundreds of billions of dollars -- done faster than anything in history before, but not infinitely fast.

[27:36](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1656s) Maybe this is not your claim, but some people say "Oh, the capabilities are there, but because of diffusion..." I don't believe we're basically at AGI. If we had a "country of geniuses in a data center," we would know it. Everyone in Washington would know it. But we would know it. We don't.

[29:42](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1782s) Coming back to concrete prediction -- because there are so many dimensions to this, it can be easy to talk past each other. For example, when I interviewed you three years ago, you made predictions about what we should expect three years from now. You were predicting models that, if you talk to them for the course of an hour, would seem like a generally well-educated human. I think spiritually I feel unsatisfied because my expectation was that these models would automate large parts of white-collar work. The actual end capabilities exceeded my prediction but the impact didn't.

[30:21](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1821s) I will basically tell you where I think we are, and we can figure out exactly what kinds of predictions to make. Maybe I'll ask about it in the context of a job -- not because it's the most relevant job, but because I can evaluate it: video editors. Part of their job involves learning about my preferences and tastes. They're, over the course of many months, picking up skill on the job on the fly.

[31:09](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1869s) I guess what you're talking about is that someone's going to come in, they're going to be like, "Oh, I don't know Dario's preferences. Let me watch his previous interviews, look at what people are saying, talk to you, ask you questions, talk to your team, look at previous edits that you did, and from that, do the job." I think "a country of geniuses in a data center" will be able to do that. It'll have general control of a computer screen. It'll be able to use the computer screen to research, watch interviews, talk to you, and from that, do the job.

[32:06](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1926s) I think this is one of the things we're getting to the point on with computer use -- where the models are climbing in benchmarks. But I think when we first released computer use about a year ago, it was maybe at 20%. I don't remember exactly. There may be harder measures as well, but I think the direction is clear.

[32:50](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=1970s) For years, I've been trying to build agents and workflows. Often I have these text-in, text-out tasks that should be in the repertoire of these models. If it's something like "identify what the key issue is," maybe the LLMs do a seven-out-of-ten job on them. But I can't iterate with them to help them get better at the task. That missing ability, even if you can get a 7/10, prevents my ability to offload an actual job to them.

[33:28](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2008s) This connects to what we were talking about before with learning on the job. It's very related. I don't think people would say that learning on the job is what's preventing models from doing everything end to end. They can do a lot. At Anthropic we have people who don't write any code but say, "This is what I need. I just have Claude do it."

[34:04](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2044s) When I see Claude Code, familiarity with the codebase -- if someone hasn't worked at the company for a year, that's a limitation. I think what I'm saying is that we're making progress on all of these things. Don't you think with coding that's because there is an external scaffold of memory -- the codebase itself? I don't know how many other jobs have that. It has this unique advantage that all the context is written down. But when you say that, what you're implying is that "I have everything that the human needs." So that would be an example of a case where everything you needed was in the context. What we think of as learning -- "I started this job, I learned the code base" -- the model just did it in the context.

[35:09](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2109s) I mention this because there was a major study last year where developers were asked to close pull requests in repositories that they were unfamiliar with. They reported feeling more productive, that they had an uplift. But in fact, if you look at their output, there was a 20% downlift as a result of using these models. There's this interesting feeling that people feel -- where is this renaissance of software? If these models work so well on evaluations, why are we not seeing the expected productivity gains?

[35:53](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2153s) Within Anthropic, this is just really unambiguous. We put enormous pressure on ourselves to do more with fewer people. I think we do more than other companies. The amount we accomplish while also keeping our values is just incredible. There is zero time for bullshit. We can't afford to pretend like we're productive when we're not. Why do you think we're concerned? Because we think we're ahead of the competitors. If this were secretly reducing our productivity, you'd see it show up months later in the form of model launches. The models make you more productive.

[37:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2220s) The real productivity gains are qualitatively predicted by studies like this -- obviously you guys are making fast progress. But if there's recursive self-improvement -- you make the AI better, the AI makes the better next AI, et cetera -- why are we not seeing larger advantages? I think my model of the situation is that I would say right now the coding tools provide about a 15-20% total factor speedup. That's meaningful but six months ago it was closer to 5%, which doesn't register. It goes from 5% to 10% to 15%, where at each stage it's one of several factors that kind of matters.

[37:40](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2260s) I would also say there are multiple companies that are all roughly equally capable, and competition is perfectly good at preventing some of these other companies from pulling ahead. So I think everything we're seeing is consistent with the picture I'm painting. Again, my theme in all of this is: all of this is consistent with rapid exponentials, although the exponentials are relatively steep. It's like 10%, 20%, 25%, 40% -- you have to get all the blocking factors out of the way.

[38:20](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2300s) But this is one of the biggest questions. Stepping back, before we were talking about the technology stack. It seems like the point you were making is that if we get continual learning right, you don't need on-the-job learning. You can have potentially trillions of tokens worth of context and not need this basic human ability to learn on the job. But in most domains of economic activity, people learn on the job for the first few months, and then over time they get better. It's actually hard to define exactly what "learning on the job" means. But they got something and now they're applying it.

[39:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2340s) If AI doesn't develop this ability to learn on the job, I still think we could have huge changes to the world without that ability. That's actually the state of the technology right now. We have the pre-training and RL stage where we train the models and then they generalize. We're learning from more data and not learning from the on-the-job experience. But once you learn all that generic capability, just like with pre-training, the models know more about the history of the world than I do. They know more about baseball than I do, and electronics, all of these things. So I think even just that may get us to "a country of geniuses in a data center."

[40:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2400s) We also have, just with scaling, the kind of thing I would describe as a somewhat weaker and somewhat short-term version of learning. If you give the model a bunch of examples, it does get it. A million tokens is a lot. If you think about the model reading a million tokens, how long would it take me to read a million? Days or weeks. I think these two things within the existing paradigm will get us a large fraction of what we need for "a country of geniuses in a data center." They're going to get you a large fraction of it. As things are, this is enough to generate enormous economic value.

[41:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2460s) And then there is this idea of continual learning. I think we're working on that too. My hunch is that within a year or two, we also solve that, which gets you most of the way there without it. Maybe all of the national security implications, all of what I called "Adolescence of Technology" can happen without it. There's a good chance that we will solve it. There are a bunch of ideas. One is just to make the context longer. There are some engineering challenges that prevent much longer contexts from working. Another is to train on much longer contexts and then learn to serve them at inference. These are things we are working on.

[42:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2520s) This context length increase -- it seemed like from GPT-3 to GPT-4 Turbo, there was an enormous increase, and then for the two-ish years since, it's stalled. When context lengths get much longer, there's degradation in the ability of the model. So I'm curious what you're internally seeing. This isn't a research problem. This is an engineering problem. If you want to serve long context, you have to store all the memory. It's difficult. I don't even know all the details -- these are the activations you have to store, because we have MoE models and all of that.

[43:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2580s) Without getting too specific, there's two things. There's a context length that you serve at, and if you train at a shorter context and then try to serve at a long context, it's better than nothing, but there's degradation. Maybe it's harder to train at very long contexts. I want to ask a bigger picture question. If you had a human editor that's been working for you for six months, what year would you predict an AI could match that? My guess is that there's a lot of problems we still have to solve, but they're all on the path to "a country of geniuses in a data center." One to two years, maybe one to three years. It's not going to be long.

[44:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2640s) 95% -- that all this will happen in 10 years. I have a hunch -- this is more like a 50/50 thing -- one to two, maybe more like one to three. And then generally, Anthropic has predicted that models will "have the ability to navigate interfaces and have intellectual capabilities matching or exceeding those of most humans, with some ability to interface with the physical world."

[45:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2700s) At DealBook you were emphasizing how Anthropic is spending less on scaling compared to competitors. If you really believe that we're going to get "a country of geniuses in a data center" soon, why not build as big a data center as you can get? The TAM of a Nobel Prize winner -- what a Nobel Prize winner can do -- is trillions of dollars. That seems rational if you have more moderate timelines.

[46:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2760s) It actually all fits together. We go back to the economic argument. Let's say that we're making progress at this rate. I have very high conviction that we're going to get to "a country of geniuses in a data center." I have a hunch that we're going to get there in one to three years. So there's a little uncertainty on the timing, but I have high confidence that it won't be off by much. Then there's the economic diffusion side. We'll have models that are a country of geniuses -- one question is how many years after that does it take to generate trillions in revenue? I don't think it's guaranteed to be instant. It could be one year, it could be two years, although I'm skeptical of anything longer.

[47:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2820s) So we have uncertainty about when the capability arrives and how fast the revenue follows. With the way you buy compute -- you're buying it years in advance -- if you're off by even a couple of years, that can be ruinous. As I wrote in "Machines of Loving Grace," this "country of geniuses in the data center" -- my hunch is that it arrives soon. Let's say that happens. That's the starting gun. That drives a huge amount of economic value. There's a question of how much goes to the AI companies versus consumer surplus -- which I care about greatly -- we cure all diseases. But you have to do the biological discovery, then you have to go through the regulatory process.

[48:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2880s) My question is: how long does it take to get the benefits -- from models that can in theory invent -- out to everyone? From invention in the lab to when diseases have been eliminated. We've had a polio vaccine for 50 years but it hasn't reached the most remote corners of Africa. People are working as hard as they can. But that's difficult. Where I've settled on it is that it will be very fast compared to anything else in the world, but it still has its limits.

[48:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2910s) Again, the curve I'm looking at is: we've been growing revenue at 10x a year. At the beginning of this year, we're looking at compute purchases. We have to decide how much compute to buy. We have to reserve the data center. "How much compute do I get?" If revenue will continue growing 10x a year, that would be $10 billion at the end of 2026 and $1 trillion at the end of 2027. I could buy $1 trillion of compute. But if my revenue is not $1 trillion dollars, there's no hedge on earth that could stop me from going bankrupt. Even though a part of my brain wonders... I can't buy $1 trillion a year of compute in 2027. Maybe the growth rate is 5x a year instead of 10x.

[49:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=2970s) So you end up in a world where you're balancing risks. You accept some risk that there's so much demand you can't serve it all, and you accept some risk that you over-invested. When I talked about behaving responsibly, I think it is true we're spending somewhat less than the max. It's actually the other things -- have we been smart about it, have we gotten the best deals? Some companies I get the impression are spending without looking at the spreadsheet, that they don't really believe in the economics. They're just doing stuff because it sounds impressive. We have an enterprise business. We have better margins, which is the buffer.

[50:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3030s) I think we bought an amount that allows us to be in the game. It won't capture the full 10x a year, but it also won't put us in financial trouble. We've made that balance. I say that we're being responsible. Now, you might just have different definitions of what that means. Because when I think of actual human geniuses, I would happily buy $5 trillion worth of compute if I really could have human geniuses in a data center.

[52:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3120s) Let me explain the economics. You have a small number of firms. Each can invest some fraction in R&D. The gross profit margins on the marginal cost of serving inference are pretty good. Companies will compete, but because there's a small number, the Cournot equilibrium doesn't go to zero profits. If there's three firms in the economy and all serve a product, it doesn't equilibrate to zero. Right now we do have three leading firms, and the gross margins are decent.

[53:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3180s) What's happening is a combination of two things. There's the model itself that was trained -- let's say that cost $1 billion last year. It generates a certain amount of revenue and costs some amount to serve inference. That would have 75% gross margins or so. But at the same time, we're spending $10 billion building the next model. So there's an exponential scale-up that makes the company lose money even when each individual model makes money. In the equilibrium where we have "a country of geniuses" and model training scale-up has equilibrated more, the economics become clearer.

[55:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3300s) The underlying economics are profitable. It's really more of a prediction problem -- when you're buying the next generation of compute, you could end up very profitable but have no compute for research, or you could be less profitable and have all the compute for research. As a dynamic model of the industry, the "country of geniuses" is going to come in two to three years.

[57:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3450s) It is hard for me to see this industry generating trillions of dollars in revenue before 2030. It takes maybe three years. That would be like in 2028, we get the real "country of geniuses," the revenue goes into the low hundreds of billions. The "country of geniuses" accelerates it to trillions. It takes two years to get to the trillions. I suspect even composing the technical and economic exponentials, we'll get there before 2030.

[58:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3480s) All my lawyers never want me to say this, but I don't think this field's going to be a monopoly. You do get industries in which there's not one, but a small number of players. The way you get industries with a small number of players is very high costs of entry. Cloud is like that -- there are three, maybe four, players. The reason is that it's so expensive. It takes so much capital. In addition to putting up all this capital, you need a lot of skill to make it happen. Even if someone said, "Here's $100 billion to disrupt this industry," you'd also be betting that you can do all these hard technical things. Only to decrease the profit margins for everyone.

[59:10](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3550s) Models are not undifferentiated. Everyone knows Claude is good at different things. It's not just that Claude's good at one thing. It's more subtle than that. Models have different styles. I think these things are actually quite differentiated -- more differentiation than you see in cloud. Now, if AI models can do everything, including build other AI models, then that's not an argument for zero profits. That's an argument for more competition. I don't know what quite happens in that world, but maybe we want that world.

[1:02:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3720s) A worry of mine is that proximity to AI progress is going to create growing inequality. The 10-20% annual economic growth I described might happen much faster in Silicon Valley and parts of the world that are close to AI, and much slower elsewhere. One of the things I think about is: how do we ensure benefits are distributed?

[1:03:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3810s) Do you think that once we have this "country of geniuses," robotics is sort of quickly solved afterwards? A human can learn how to teleoperate current hardware with very little training. So if you have a human-like learner, shouldn't it solve robotics? I don't think it's dependent on any one pathway. It could happen because we trained models on many different video games, which are like robotic environments, or just trained them to control hardware. It's not necessarily about human-like learning. But I do think when for whatever reason the AI models get smart enough, robotics will be revolutionized -- both the design of robots and the ability to control them.

[1:06:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=3990s) Does that mean the robotics revolution generates trillions of dollars of revenue immediately? I'd say probably tack on another year or two for the hardware deployment. There's a general skepticism about whether there will be further barriers. We keep solving what we thought were the bottlenecks, and then we find new ones. But there's a stronger history of these barriers dissolving within the big blob of compute than of them persisting.

[1:11:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4290s) I think it's funny that in every interview, the hosts will be like, "But Dwarkesh wrote that we're missing continual learning." And Dario says, "It always makes me crack up because I'm sure there's some feeling of 'this guy keeps asking me about it and every interview I get asked about it.'" But we're just trying to figure this out together. Amodei says he doesn't think he has any great research insight that others don't. It's actually pretty hard for him to have concrete predictions that are much better than what would have been reasonable two or three years ago.

[1:13:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4380s) On the API versus other business models question: I think there's going to be a bunch of different business models that are going to be experimented with. The API model is more durable than many people think because if AI capability is advancing quickly, there's always a surface area of new use cases. The chatbot is already running into limitations -- not because the model isn't good enough but because the product surface is limited. But there's always going to be this front of capability where new things become possible.

[1:17:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4650s) Claude Code is a category leader. That didn't have to be something intrinsically that Anthropic had to build. Around the beginning of 2025, I said I want to see a nontrivial acceleration of your own research through AI. I encouraged people internally. I didn't mandate it. Internally people created something called Claude CLI, and then the name changed to Claude Code. It was the thing that everyone was using. I looked at it and said, "Probably we should ship this." It's seen such fast adoption within Anthropic. We have an audience of hundreds of thousands of developers which is representative of the external audience. We saw product-market fit and launched.

[1:21:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4860s) My background's in biology, but we realized that building a strong coding model with strong developer tools is what's needed to launch what's effectively a company that builds AI. There's this nice feedback loop where our developers use it, find issues, and we bake improvements into the next model.

[1:23:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=4980s) If AI progress continues, the ability to build and run AIs is going to be more distributed. The population of AIs will grow. That means lots of people will be able to build AIs or AI-powered companies. What is a vision for a world in which we have lots of different AIs, some of which might have weird goals? I think in "The Adolescence of Technology" I was specifically skeptical of this in the near term because we're all building models derived from the same foundations. We might live in an offense-dominant world where one breakthrough causes damage for everything else.

[1:25:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5130s) I think in the long run we need some architecture of governance that both allows us to govern a very large number of human-AI companies or economic units and protects the world against bioterrorism. Probably we're gonna need some monitoring system that monitors for all these things but preserves civil liberties. My worry is just the speed at which this is all happening. So maybe we need to do our thinking faster.

[1:27:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5250s) If checks and balances were going to work, they'll work whether it's AI doing it or humans. If they aren't going to work, they won't work regardless. The governments of the world may have to act faster. We may have to talk to AIs about building defensive systems. I don't know if those defenses are possible, but the technology is so far ahead in capability that it's hard for us to anticipate it in advance.

[1:31:19](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5479s) On December 26, the Tennessee legislature passed a law making it an offense for a person to knowingly use AI for emotional support. Of course, one of the things that Claude attempts to do well is provide emotional support. In general, it seems like a lot of the benefits that normal people could get from AI will be curtailed. I'm curious about Anthropic's position on the federal moratorium on state AI laws. I think that particular law is dumb. State legislators probably had little idea what they were voting on. But what was being voted on with the moratorium was banning all state regulation of AI without any federal regulation to replace it. There's no actual attempt to create federal rules. Given the serious dangers from things like biological weapons, I chose not to support that moratorium.

[1:37:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=5820s) What I think we should do is have the federal government step in, saying "Here are the rules. States can't differ from this." That would be fine if it were done in the right way. But "states you can't regulate, and we're not doing anything either" -- I think it will not age well. The risks I'm talking about are starting with biological weapon risks. If we see more evidence for bioterrorism risks, then we should act. I'm concerned about a world where legislation is too slow.

[1:41:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=6060s) I don't worry as much about the chatbot laws. I worry more about the FDA process, where AI models are going to accelerate drug discovery and the pipeline will get jammed up. We should bias toward the fact that with AI doing the heavy lifting, the efficacy evidence is actually going to be really crisp and clear. Maybe we don't need all this superstructure around drugs that barely work and often have serious side effects.

[1:47:41](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=6461s) Why shouldn't the US and China both have a country of geniuses in a data center? If we have an offense-dominant world, AI systems could be like nuclear weapons, but more dangerous. We could also have a world where it's unstable. With nuclear weapons, you can roughly estimate if the two sides fought, which would win. Conflict is more likely when the two sides have a different view of who would win. I'm also very concerned about a world where you have a country that uses AI to build a high-tech authoritarian state. My worry is if the world gets carved up into spheres that could be authoritarian or totalitarian.

[1:55:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=6900s) I think the exponential of the underlying technology continues. The models get smarter and smarter. There are distinguished points on the exponential that different countries will reach at different times. Is a nuclear deterrent still viable? I don't know. The technology could reach such a level that if you have cyber dominance, you could compromise every computer system. There's going to be some negotiation about what the post-AI world order looks like.

[2:00:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7200s) I think the US or a coalition of democracies -- which requires more international cooperation than we currently have -- needs to set up the rules of the road. The world is going to have to grapple with this. Those whose governments represent their people should have a stronger hand and have more leverage.

[2:05:46](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7546s) On Claude's constitution -- there's a mix of rules and principles. By teaching the model principles, its behavior is more consistent, easier to predict, and more likely to do what people want. If you just give it a list of rules -- "don't tell people how to hot-wire a car" -- it doesn't really understand the rules. The model has some hard guardrails like "Don't make bioweapons," but mostly it's trying to understand what it should be aiming for. On the corrigibility versus autonomy spectrum, I would say everything about the current models, and honestly models for the next few years, should be that they mostly do what people want. We're not trying to build something that has its own agenda.

[2:08:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7680s) How are those principles determined? I think there are maybe three levels of input. One is we iterate within Anthropic -- people write and change the constitution. Two is putting out public updates and having different companies publish different constitutions so people can compare. Three -- and this is speculative -- systems of democratic input. I wouldn't do this today because the stakes aren't high enough, but you could imagine a special section of the constitution that's democratically determined.

[2:13:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=7980s) When somebody eventually writes the equivalent of the history of this period, what is the thing that will be hardest for them to get right? I think it's that at every moment of this exponential, the extent to which things felt uncertain and contingent. This is a bias often present in history -- things seem inevitable in retrospect. And the weirdness of it. If we're one year or two years away from "a country of geniuses in a data center," the average person on the street has no idea.

[2:17:00](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8220s) I probably spend a third, maybe 40%, of my time on management. As Anthropic has gotten larger -- 2,500 people -- it's gotten harder. I try as much as possible to maintain the culture. Daniela, who runs the company alongside me, deserves enormous credit. But I think an important thing in the culture is that you have to articulate what the company is doing, what its strategy is, what its values are.

[2:18:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8310s) This is why I get up in front of the whole company every two weeks. It's called a DVQ -- Dario Vision Quest. That's the name it received, and it's one of these names you can't change. I get up in front of the company every two weeks and I just talk through three or four different topics -- the models we're producing, the products, the policy landscape as it relates to AI and geopolitics. I talk through very honestly what I'm thinking and then I answer questions.

[2:19:30](https://www.youtube.com/watch?v=n1E9IZfvGMA&t=8370s) A large fraction of the company comes. It really means that you can communicate a lot. I also have a Slack channel where I just write a bunch of stuff, often in response to things I'm seeing. We do internal surveys and there are things people raise. I'm just very honest about these things. The point is to get a reputation of telling the truth, calling things what they are, to acknowledge problems. It makes the company run better and increases the likelihood that we accomplish our mission. Well, in lieu of an external Dario Vision Quest, this interview is a little like that. Thank you, Dwarkesh.
